<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Crusher Quick-Start Guide &mdash; OLCF User Documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=19f00094" />
      <link rel="stylesheet" type="text/css" href="../_static/graphviz.css?v=eafc0fe6" />
      <link rel="stylesheet" type="text/css" href="../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
      <link rel="stylesheet" type="text/css" href="../_static/sphinx-data-viewer/jsonview.bundle.css?v=f6ef2277" />
      <link rel="stylesheet" type="text/css" href="../_static/sphinx-needs/libs/html/datatables.min.css?v=4b4fd840" />
      <link rel="stylesheet" type="text/css" href="../_static/sphinx-needs/common_css/need_style.css?v=678fb11e" />
      <link rel="stylesheet" type="text/css" href="../_static/sphinx-needs/common_css/need_core.css?v=f5b60a78" />
      <link rel="stylesheet" type="text/css" href="../_static/sphinx-needs/common_css/needstable.css?v=5e1b6797" />
      <link rel="stylesheet" type="text/css" href="../_static/sphinx-needs/common_css/need_links.css?v=2150a916" />
      <link rel="stylesheet" type="text/css" href="../_static/sphinx-needs/common_css/need_toggle.css?v=5c6620df" />
      <link rel="stylesheet" type="text/css" href="../_static/sphinx-needs/modern.css?v=803738c0" />
      <link rel="stylesheet" type="text/css" href="https://cdn.datatables.net/1.13.4/css/jquery.dataTables.min.css" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme_overrides.css?v=b034643a" />

  
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="canonical" href="https://docs.olcf.ornl.govsystems/crusher_quick_start_guide.html"/>
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../_static/jquery.js?v=5d32c60e"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="../_static/documentation_options.js?v=5929fcd5"></script>
        <script src="../_static/doctools.js?v=888ff710"></script>
        <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
        <script src="../_static/design-tabs.js?v=36754332"></script>
        <script src="../_static/sphinx-data-viewer/jsonview.bundle.js?v=18cd53c5"></script>
        <script src="../_static/sphinx-data-viewer/jsonview_loader.js?v=f7ff7e7d"></script>
        <script src="../_static/sphinx-needs/libs/html/datatables.min.js?v=8a4aee21"></script>
        <script src="../_static/sphinx-needs/libs/html/datatables_loader.js?v=a2cae175"></script>
        <script src="../_static/sphinx-needs/libs/html/sphinx_needs_collapse.js?v=dca66431"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <script src="https://cdn.datatables.net/1.13.4/js/jquery.dataTables.min.js"></script>
        <script src="../_static/activate_datatables.js?v=e38ccb97"></script>
        <script src="../_static/js/custom.js?v=f5206ae7"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Services and Applications" href="../services_and_applications/index.html" />
    <link rel="prev" title="Spock Quick-Start Guide" href="spock_quick_start_guide.html" />

</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: #efefef" >

          
          
          <a href="../index.html">
            
              <img src="../_static/olcf_logo.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../support/index.html">Contact &amp; Support</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../support/index.html#olcf-user-assistance-center">OLCF User Assistance Center</a></li>
<li class="toctree-l2"><a class="reference internal" href="../support/index.html#authentication-support">Authentication Support</a></li>
<li class="toctree-l2"><a class="reference internal" href="../support/index.html#olcf-office-hours">OLCF Office Hours</a></li>
<li class="toctree-l2"><a class="reference internal" href="../support/index.html#communication-to-users">Communication to Users</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../accounts/index.html">Accounts and Projects</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../accounts/accounts_and_projects.html">Request a New Allocation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../accounts/accounts_and_projects.html#what-are-the-differences-between-project-types">What are the differences between project types?</a></li>
<li class="toctree-l3"><a class="reference internal" href="../accounts/accounts_and_projects.html#what-happens-after-a-project-request-is-approved">What happens after a project request is approved?</a></li>
<li class="toctree-l3"><a class="reference internal" href="../accounts/accounts_and_projects.html#guidance-on-frontier-allocation-requests">Guidance on Frontier Allocation Requests</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../accounts/accounts_and_projects.html#applying-for-a-user-account">Applying for a user account</a></li>
<li class="toctree-l2"><a class="reference internal" href="../accounts/accounts_and_projects.html#checking-the-status-of-your-application">Checking the status of your application</a></li>
<li class="toctree-l2"><a class="reference internal" href="../accounts/accounts_and_projects.html#get-access-to-additional-projects">Get access to additional projects</a></li>
<li class="toctree-l2"><a class="reference internal" href="../accounts/frequently_asked_questions.html">Frequently Asked Questions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../accounts/frequently_asked_questions.html#how-do-i-apply-for-an-account">How do I apply for an account?</a></li>
<li class="toctree-l3"><a class="reference internal" href="../accounts/frequently_asked_questions.html#what-is-the-status-of-my-application">What is the status of my application?</a></li>
<li class="toctree-l3"><a class="reference internal" href="../accounts/frequently_asked_questions.html#how-should-i-acknowledge-the-olcf-in-my-publications-and-presentations">How should I acknowledge the OLCF in my publications and presentations?</a></li>
<li class="toctree-l3"><a class="reference internal" href="../accounts/frequently_asked_questions.html#what-is-a-subproject">What is a subproject?</a></li>
<li class="toctree-l3"><a class="reference internal" href="../accounts/frequently_asked_questions.html#i-no-longer-need-my-account-who-should-i-inform-and-what-should-i-do-with-my-olcf-issued-rsa-securid-token">I no longer need my account. Who should I inform and what should I do with my OLCF issued RSA SecurID token?</a></li>
<li class="toctree-l3"><a class="reference internal" href="../accounts/frequently_asked_questions.html#my-securid-token-is-broken-expired-what-should-i-do">My SecurID token is broken/expired. What should I do?</a></li>
<li class="toctree-l3"><a class="reference internal" href="../accounts/frequently_asked_questions.html#getting-help">Getting Help</a></li>
<li class="toctree-l3"><a class="reference internal" href="../accounts/frequently_asked_questions.html#additional-resources">Additional Resources</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../accounts/documents_and_forms.html">Documents and Forms</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../accounts/documents_and_forms.html#forms-for-requesting-a-project-allocation">Forms for Requesting a Project Allocation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../accounts/documents_and_forms.html#forms-for-requesting-an-account">Forms for Requesting an Account</a></li>
<li class="toctree-l3"><a class="reference internal" href="../accounts/documents_and_forms.html#forms-to-request-changes-to-computers-jobs-or-accounts">Forms to Request Changes to Computers, Jobs, or Accounts</a></li>
<li class="toctree-l3"><a class="reference internal" href="../accounts/documents_and_forms.html#report-templates">Report Templates</a></li>
<li class="toctree-l3"><a class="reference internal" href="../accounts/documents_and_forms.html#miscellaneous-forms">Miscellaneous Forms</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../accounts/olcf_policy_guide.html">OLCF Policy Guides</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../accounts/olcf_policy_guide.html#olcf-acknowledgement">OLCF Acknowledgement</a></li>
<li class="toctree-l3"><a class="reference internal" href="../accounts/olcf_policy_guide.html#software-requests">Software Requests</a></li>
<li class="toctree-l3"><a class="reference internal" href="../accounts/olcf_policy_guide.html#special-requests-and-policy-exemptions">Special Requests and Policy Exemptions</a></li>
<li class="toctree-l3"><a class="reference internal" href="../accounts/olcf_policy_guide.html#computing-policy">Computing Policy</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../accounts/olcf_policy_guide.html#computer-use">Computer Use</a></li>
<li class="toctree-l4"><a class="reference internal" href="../accounts/olcf_policy_guide.html#data-use">Data Use</a></li>
<li class="toctree-l4"><a class="reference internal" href="../accounts/olcf_policy_guide.html#software-use">Software Use</a></li>
<li class="toctree-l4"><a class="reference internal" href="../accounts/olcf_policy_guide.html#user-accountability">User Accountability</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../accounts/olcf_policy_guide.html#data-management-policy">Data Management Policy</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../accounts/olcf_policy_guide.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../accounts/olcf_policy_guide.html#data-retention-purge-quotas">Data Retention, Purge, &amp; Quotas</a></li>
<li class="toctree-l4"><a class="reference internal" href="../accounts/olcf_policy_guide.html#data-prohibitions-safeguards">Data Prohibitions &amp; Safeguards</a></li>
<li class="toctree-l4"><a class="reference internal" href="../accounts/olcf_policy_guide.html#software">Software</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../accounts/olcf_policy_guide.html#security-policy">Security Policy</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../accounts/olcf_policy_guide.html#scope">Scope</a></li>
<li class="toctree-l4"><a class="reference internal" href="../accounts/olcf_policy_guide.html#personal-use">Personal Use</a></li>
<li class="toctree-l4"><a class="reference internal" href="../accounts/olcf_policy_guide.html#accessing-olcf-computational-resources">Accessing OLCF Computational Resources</a></li>
<li class="toctree-l4"><a class="reference internal" href="../accounts/olcf_policy_guide.html#data-management">Data Management</a></li>
<li class="toctree-l4"><a class="reference internal" href="../accounts/olcf_policy_guide.html#sensitive-data">Sensitive Data</a></li>
<li class="toctree-l4"><a class="reference internal" href="../accounts/olcf_policy_guide.html#data-transfer">Data Transfer</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../accounts/olcf_policy_guide.html#incite-allocation-under-utilization-policy">INCITE Allocation Under-utilization Policy</a></li>
<li class="toctree-l3"><a class="reference internal" href="../accounts/olcf_policy_guide.html#project-reporting-policy">Project Reporting Policy</a></li>
<li class="toctree-l3"><a class="reference internal" href="../accounts/olcf_policy_guide.html#non-proprietary-institutional-user-agreement-policy">Non-proprietary Institutional User Agreement Policy</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../accounts/olcf_policy_guide.html#access">Access</a></li>
<li class="toctree-l4"><a class="reference internal" href="../accounts/olcf_policy_guide.html#rules-and-regulations">Rules and Regulations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../accounts/olcf_policy_guide.html#safety-and-health">Safety and Health</a></li>
<li class="toctree-l4"><a class="reference internal" href="../accounts/olcf_policy_guide.html#intent-to-publish">Intent to Publish</a></li>
<li class="toctree-l4"><a class="reference internal" href="../accounts/olcf_policy_guide.html#export-control">Export Control</a></li>
<li class="toctree-l4"><a class="reference internal" href="../accounts/olcf_policy_guide.html#intellectual-property">Intellectual Property</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../accounts/olcf_policy_guide.html#hipaa-itar-project-rules-of-behavior-policy">HIPAA/ITAR Project Rules of Behavior Policy</a></li>
<li class="toctree-l3"><a class="reference internal" href="../accounts/olcf_policy_guide.html#user-managed-software-ums-policy">User-Managed Software (UMS) Policy</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../accounts/olcf_policy_guide.html#purpose">Purpose</a></li>
<li class="toctree-l4"><a class="reference internal" href="../accounts/olcf_policy_guide.html#policies">Policies</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../accounts/glossary.html">Glossary</a></li>
<li class="toctree-l2"><a class="reference internal" href="../accounts/index.html#additional-resources">Additional Resources</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../connecting/index.html">Connecting</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../connecting/index.html#connecting-for-the-first-time">Connecting for the first time</a></li>
<li class="toctree-l2"><a class="reference internal" href="../connecting/index.html#activating-a-new-securid-fob">Activating a new SecurID fob</a></li>
<li class="toctree-l2"><a class="reference internal" href="../connecting/index.html#pins-passcodes-and-tokencodes">PINs, Passcodes, and Tokencodes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../connecting/index.html#x11-forwarding">X11 Forwarding</a></li>
<li class="toctree-l2"><a class="reference internal" href="../connecting/index.html#id2">Systems Available to All Projects</a></li>
<li class="toctree-l2"><a class="reference internal" href="../connecting/index.html#olcf-system-hostnames">OLCF System Hostnames</a></li>
<li class="toctree-l2"><a class="reference internal" href="../connecting/index.html#starting-a-tmux-session">Starting a Tmux Session</a></li>
<li class="toctree-l2"><a class="reference internal" href="../connecting/index.html#checking-system-availability">Checking System Availability</a></li>
</ul>
</li>
</ul>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Systems</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="2024_olcf_system_changes.html">2024 Notable System Changes</a><ul>
<li class="toctree-l3"><a class="reference internal" href="2024_olcf_system_changes.html#hpss-decommission-and-kronos-availability">HPSS Decommission and Kronos Availability</a><ul>
<li class="toctree-l4"><a class="reference internal" href="2024_olcf_system_changes.html#late-july-2024-kronos-available">Late July 2024 - Kronos available</a></li>
<li class="toctree-l4"><a class="reference internal" href="2024_olcf_system_changes.html#august-30-2024-hpss-becomes-read-only">August 30, 2024 - HPSS becomes read-only</a></li>
<li class="toctree-l4"><a class="reference internal" href="2024_olcf_system_changes.html#january-31-2025-hpss-decommissioned">January 31, 2025 - HPSS decommissioned</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="2024_olcf_system_changes.html#summit-and-alpine2-decommissions">Summit and Alpine2 Decommissions</a><ul>
<li class="toctree-l4"><a class="reference internal" href="2024_olcf_system_changes.html#november-15-2024-summit-decommissioned">November 15, 2024 - Summit decommissioned</a></li>
<li class="toctree-l4"><a class="reference internal" href="2024_olcf_system_changes.html#november-19-2024-alpine2-read-only">November 19, 2024 - Alpine2 read-only</a></li>
<li class="toctree-l4"><a class="reference internal" href="2024_olcf_system_changes.html#january-31-2025-alpine2-decommissioned">January 31, 2025 - Alpine2 decommissioned</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="frontier_user_guide.html">Frontier User Guide</a><ul>
<li class="toctree-l3"><a class="reference internal" href="frontier_user_guide.html#system-overview">System Overview</a><ul>
<li class="toctree-l4"><a class="reference internal" href="frontier_user_guide.html#frontier-compute-nodes">Frontier Compute Nodes</a></li>
<li class="toctree-l4"><a class="reference internal" href="frontier_user_guide.html#node-types">Node Types</a></li>
<li class="toctree-l4"><a class="reference internal" href="frontier_user_guide.html#system-interconnect">System Interconnect</a></li>
<li class="toctree-l4"><a class="reference internal" href="frontier_user_guide.html#file-systems">File Systems</a></li>
<li class="toctree-l4"><a class="reference internal" href="frontier_user_guide.html#operating-system">Operating System</a></li>
<li class="toctree-l4"><a class="reference internal" href="frontier_user_guide.html#gpus">GPUs</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="frontier_user_guide.html#connecting">Connecting</a></li>
<li class="toctree-l3"><a class="reference internal" href="frontier_user_guide.html#data-and-storage">Data and Storage</a><ul>
<li class="toctree-l4"><a class="reference internal" href="frontier_user_guide.html#transition-from-alpine-to-orion">Transition from Alpine to Orion</a></li>
<li class="toctree-l4"><a class="reference internal" href="frontier_user_guide.html#lfs-setstripe-wrapper">LFS setstripe wrapper</a></li>
<li class="toctree-l4"><a class="reference internal" href="frontier_user_guide.html#nfs-filesystem">NFS Filesystem</a></li>
<li class="toctree-l4"><a class="reference internal" href="frontier_user_guide.html#lustre-filesystem">Lustre Filesystem</a></li>
<li class="toctree-l4"><a class="reference internal" href="frontier_user_guide.html#kronos-archival-storage">Kronos Archival Storage</a></li>
<li class="toctree-l4"><a class="reference internal" href="frontier_user_guide.html#nvme">NVMe</a></li>
<li class="toctree-l4"><a class="reference internal" href="frontier_user_guide.html#nvme-usage">NVMe Usage</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="frontier_user_guide.html#using-globus-to-move-data-to-and-from-orion">Using Globus to Move Data to and from Orion</a></li>
<li class="toctree-l3"><a class="reference internal" href="frontier_user_guide.html#amd-gpus">AMD GPUs</a><ul>
<li class="toctree-l4"><a class="reference internal" href="frontier_user_guide.html#amd-vs-nvidia-terminology">AMD vs NVIDIA Terminology</a></li>
<li class="toctree-l4"><a class="reference internal" href="frontier_user_guide.html#blocks-workgroups-threads-work-items-grids-wavefronts">Blocks (workgroups), Threads (work items), Grids, Wavefronts</a></li>
<li class="toctree-l4"><a class="reference internal" href="frontier_user_guide.html#the-compute-unit">The Compute Unit</a></li>
<li class="toctree-l4"><a class="reference internal" href="frontier_user_guide.html#hip">HIP</a></li>
<li class="toctree-l4"><a class="reference internal" href="frontier_user_guide.html#things-to-remember-when-programming-for-amd-gpus">Things To Remember When Programming for AMD GPUs</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="frontier_user_guide.html#programming-environment">Programming Environment</a><ul>
<li class="toctree-l4"><a class="reference internal" href="frontier_user_guide.html#environment-modules-lmod">Environment Modules (Lmod)</a></li>
<li class="toctree-l4"><a class="reference internal" href="frontier_user_guide.html#compilers">Compilers</a></li>
<li class="toctree-l4"><a class="reference internal" href="frontier_user_guide.html#mpi">MPI</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="frontier_user_guide.html#compiling">Compiling</a><ul>
<li class="toctree-l4"><a class="reference internal" href="frontier_user_guide.html#id4">Compilers</a></li>
<li class="toctree-l4"><a class="reference internal" href="frontier_user_guide.html#id7">MPI</a></li>
<li class="toctree-l4"><a class="reference internal" href="frontier_user_guide.html#openmp">OpenMP</a></li>
<li class="toctree-l4"><a class="reference internal" href="frontier_user_guide.html#openmp-gpu-offload">OpenMP GPU Offload</a></li>
<li class="toctree-l4"><a class="reference internal" href="frontier_user_guide.html#openacc">OpenACC</a></li>
<li class="toctree-l4"><a class="reference internal" href="frontier_user_guide.html#id9">HIP</a></li>
<li class="toctree-l4"><a class="reference internal" href="frontier_user_guide.html#hip-openmp-cpu-threading">HIP + OpenMP CPU Threading</a></li>
<li class="toctree-l4"><a class="reference internal" href="frontier_user_guide.html#sycl">SYCL</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="frontier_user_guide.html#running-jobs">Running Jobs</a><ul>
<li class="toctree-l4"><a class="reference internal" href="frontier_user_guide.html#login-vs-compute-nodes">Login vs Compute Nodes</a></li>
<li class="toctree-l4"><a class="reference internal" href="frontier_user_guide.html#simplified-node-layout">Simplified Node Layout</a></li>
<li class="toctree-l4"><a class="reference internal" href="frontier_user_guide.html#slurm">Slurm</a></li>
<li class="toctree-l4"><a class="reference internal" href="frontier_user_guide.html#batch-scripts">Batch Scripts</a></li>
<li class="toctree-l4"><a class="reference internal" href="frontier_user_guide.html#interactive-jobs">Interactive Jobs</a></li>
<li class="toctree-l4"><a class="reference internal" href="frontier_user_guide.html#common-slurm-options">Common Slurm Options</a></li>
<li class="toctree-l4"><a class="reference internal" href="frontier_user_guide.html#slurm-environment-variables">Slurm Environment Variables</a></li>
<li class="toctree-l4"><a class="reference internal" href="frontier_user_guide.html#job-states">Job States</a></li>
<li class="toctree-l4"><a class="reference internal" href="frontier_user_guide.html#job-reason-codes">Job Reason Codes</a></li>
<li class="toctree-l4"><a class="reference internal" href="frontier_user_guide.html#scheduling-policy">Scheduling Policy</a></li>
<li class="toctree-l4"><a class="reference internal" href="frontier_user_guide.html#job-dependencies">Job Dependencies</a></li>
<li class="toctree-l4"><a class="reference internal" href="frontier_user_guide.html#monitoring-and-modifying-batch-jobs">Monitoring and Modifying Batch Jobs</a></li>
<li class="toctree-l4"><a class="reference internal" href="frontier_user_guide.html#srun">Srun</a></li>
<li class="toctree-l4"><a class="reference internal" href="frontier_user_guide.html#process-and-thread-mapping-examples">Process and Thread Mapping Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="frontier_user_guide.html#ensemble-jobs">Ensemble Jobs</a></li>
<li class="toctree-l4"><a class="reference internal" href="frontier_user_guide.html#tips-for-launching-at-scale">Tips for Launching at Scale</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="frontier_user_guide.html#software">Software</a></li>
<li class="toctree-l3"><a class="reference internal" href="frontier_user_guide.html#debugging">Debugging</a><ul>
<li class="toctree-l4"><a class="reference internal" href="frontier_user_guide.html#linaro-ddt">Linaro DDT</a></li>
<li class="toctree-l4"><a class="reference internal" href="frontier_user_guide.html#gdb">GDB</a></li>
<li class="toctree-l4"><a class="reference internal" href="frontier_user_guide.html#valgrind4hpc">Valgrind4hpc</a></li>
<li class="toctree-l4"><a class="reference internal" href="frontier_user_guide.html#omnitrace">Omnitrace</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="frontier_user_guide.html#profiling-applications">Profiling Applications</a><ul>
<li class="toctree-l4"><a class="reference internal" href="frontier_user_guide.html#getting-started-with-the-hpe-performance-analysis-tools-pat">Getting Started with the HPE Performance Analysis Tools (PAT)</a></li>
<li class="toctree-l4"><a class="reference internal" href="frontier_user_guide.html#getting-started-with-hpctoolkit">Getting Started with HPCToolkit</a></li>
<li class="toctree-l4"><a class="reference internal" href="frontier_user_guide.html#getting-started-with-the-rocm-profiler">Getting Started with the ROCm Profiler</a></li>
<li class="toctree-l4"><a class="reference internal" href="frontier_user_guide.html#roofline-profiling-with-the-rocm-profiler">Roofline Profiling with the ROCm Profiler</a></li>
<li class="toctree-l4"><a class="reference internal" href="frontier_user_guide.html#omniperf">Omniperf</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="frontier_user_guide.html#tips-and-tricks">Tips and Tricks</a><ul>
<li class="toctree-l4"><a class="reference internal" href="frontier_user_guide.html#using-reduced-precision-fp16-and-bf16-datatypes">Using reduced precision (FP16 and BF16 datatypes)</a></li>
<li class="toctree-l4"><a class="reference internal" href="frontier_user_guide.html#enabling-gpu-page-migration">Enabling GPU Page Migration</a></li>
<li class="toctree-l4"><a class="reference internal" href="frontier_user_guide.html#floating-point-fp-atomic-operations-and-coarse-fine-grained-memory-allocations">Floating-Point (FP) Atomic Operations and Coarse/Fine Grained Memory Allocations</a></li>
<li class="toctree-l4"><a class="reference internal" href="frontier_user_guide.html#performance-considerations-for-lds-fp-atomicadd">Performance considerations for LDS FP atomicAdd()</a></li>
<li class="toctree-l4"><a class="reference internal" href="frontier_user_guide.html#library-considerations-with-atomic-operations">Library considerations with atomic operations</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="frontier_user_guide.html#system-updates">System Updates</a><ul>
<li class="toctree-l4"><a class="reference internal" href="frontier_user_guide.html#id17">2024-09-03</a></li>
<li class="toctree-l4"><a class="reference internal" href="frontier_user_guide.html#id18">2024-08-20</a></li>
<li class="toctree-l4"><a class="reference internal" href="frontier_user_guide.html#id19">2024-07-16</a></li>
<li class="toctree-l4"><a class="reference internal" href="frontier_user_guide.html#id20">2024-04-17</a></li>
<li class="toctree-l4"><a class="reference internal" href="frontier_user_guide.html#id21">2024-03-19</a></li>
<li class="toctree-l4"><a class="reference internal" href="frontier_user_guide.html#id22">2024-01-23</a></li>
<li class="toctree-l4"><a class="reference internal" href="frontier_user_guide.html#id23">2023-12-05</a></li>
<li class="toctree-l4"><a class="reference internal" href="frontier_user_guide.html#id24">2023-10-03</a></li>
<li class="toctree-l4"><a class="reference internal" href="frontier_user_guide.html#id25">2023-09-19</a></li>
<li class="toctree-l4"><a class="reference internal" href="frontier_user_guide.html#id26">2023-07-18</a></li>
<li class="toctree-l4"><a class="reference internal" href="frontier_user_guide.html#id27">2023-05-09</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="frontier_user_guide.html#known-issues">Known Issues</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="summit_user_guide.html">Summit User Guide</a><ul>
<li class="toctree-l3"><a class="reference internal" href="summit_user_guide.html#summit-documentation-resources">Summit Documentation Resources</a></li>
<li class="toctree-l3"><a class="reference internal" href="summit_user_guide.html#system-overview">System Overview</a><ul>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#summit-nodes">Summit Nodes</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#node-types">Node Types</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#system-interconnect">System Interconnect</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#file-systems">File Systems</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#operating-system">Operating System</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#hardware-threads">Hardware Threads</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#gpus">GPUs</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="summit_user_guide.html#connecting">Connecting</a></li>
<li class="toctree-l3"><a class="reference internal" href="summit_user_guide.html#data-and-storage">Data and Storage</a></li>
<li class="toctree-l3"><a class="reference internal" href="summit_user_guide.html#software">Software</a></li>
<li class="toctree-l3"><a class="reference internal" href="summit_user_guide.html#shell-programming-environments">Shell &amp; Programming Environments</a><ul>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#default-shell">Default Shell</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#environment-management-with-lmod">Environment Management with Lmod</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="summit_user_guide.html#compiling">Compiling</a><ul>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#compilers">Compilers</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#linking-in-libraries">Linking in Libraries</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="summit_user_guide.html#running-jobs">Running Jobs</a><ul>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#login-launch-and-compute-nodes">Login, Launch, and Compute Nodes</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#batch-scripts">Batch Scripts</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#interactive-jobs">Interactive Jobs</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#common-bsub-options">Common bsub Options</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#batch-environment-variables">Batch Environment Variables</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#job-states">Job States</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#scheduling-policy">Scheduling Policy</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#job-dependencies">Job Dependencies</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#job-launcher-jsrun">Job Launcher (jsrun)</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#launching-a-job-with-jsrun">Launching a Job with jsrun</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#jsrun-examples">Jsrun Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#using-multithreading-in-a-job">Using Multithreading in a Job</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#launching-multiple-jsruns">Launching Multiple Jsruns</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#cuda-aware-mpi">CUDA-Aware MPI</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#monitoring-jobs">Monitoring Jobs</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#interacting-with-jobs">Interacting With Jobs</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#other-lsf-commands">Other LSF Commands</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#pbs-torque-moab-to-lsf-translation">PBS/Torque/MOAB-to-LSF Translation</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#easy-mode-vs-expert-mode">Easy Mode vs. Expert Mode</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#system-service-core-isolation">System Service Core Isolation</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#job-accounting-on-summit">Job Accounting on Summit</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#other-notes">Other Notes</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="summit_user_guide.html#debugging">Debugging</a><ul>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#linaro-ddt">Linaro DDT</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#gdb">GDB</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#valgrind">Valgrind</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="summit_user_guide.html#optimizing-and-profiling">Optimizing and Profiling</a><ul>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#profiling-gpu-code-with-nvidia-developer-tools">Profiling GPU Code with NVIDIA Developer Tools</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#score-p">Score-P</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#vampir">Vampir</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#hpctoolkit">HPCToolkit</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="summit_user_guide.html#nvidia-tesla-v100">NVIDIA V100 GPUs</a><ul>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#nvidia-v100-sm">NVIDIA V100 SM</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#hbm2">HBM2</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#nvidia-nvlink">NVIDIA NVLink</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#volta-multi-process-service">Volta Multi-Process Service</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#unified-memory">Unified Memory</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#independent-thread-scheduling">Independent Thread Scheduling</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#tensor-cores">Tensor Cores</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#tesla-v100-specifications">Tesla V100 Specifications</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#further-reading">Further Reading</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="summit_user_guide.html#burst-buffer">Burst Buffer</a><ul>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#nvme-xfs">NVMe (XFS)</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#current-nvme-usage">Current NVMe Usage</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#interactive-jobs-using-the-nvme">Interactive Jobs Using the NVMe</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#nvme-usage-example">NVMe Usage Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#spectral-library">Spectral Library</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="summit_user_guide.html#known-issues">Known Issues</a><ul>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#open-issues">Open Issues</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#resolved-issues">Resolved Issues</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#cuda-10-1-known-issues">CUDA 10.1 Known Issues</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="summit_user_guide.html#scalable-protected-infrastructure-spi">Scalable Protected Infrastructure (SPI)</a></li>
<li class="toctree-l3"><a class="reference internal" href="summit_user_guide.html#training-system-ascent">Training System (Ascent)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#id27">File Systems</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#obtaining-access-to-ascent">Obtaining Access to Ascent</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#logging-in-to-ascent">Logging In to Ascent</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="summit_user_guide.html#preparing-for-frontier">Preparing For Frontier</a><ul>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#hip">HIP</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#using-hip-on-summit">Using HIP on Summit</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#learning-to-program-with-hip">Learning to Program with HIP</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#previous-frontier-training-events">Previous Frontier Training Events</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="citadel_user_guide.html">Citadel User Guide</a><ul>
<li class="toctree-l3"><a class="reference internal" href="citadel_user_guide.html#what-is-citadel">What is Citadel</a><ul>
<li class="toctree-l4"><a class="reference internal" href="citadel_user_guide.html#citadel-spi-documentation">Citadel (SPI) Documentation</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="andes_user_guide.html">Andes User Guide</a><ul>
<li class="toctree-l3"><a class="reference internal" href="andes_user_guide.html#system-overview">System Overview</a><ul>
<li class="toctree-l4"><a class="reference internal" href="andes_user_guide.html#compute-nodes">Compute nodes</a></li>
<li class="toctree-l4"><a class="reference internal" href="andes_user_guide.html#login-nodes">Login nodes</a></li>
<li class="toctree-l4"><a class="reference internal" href="andes_user_guide.html#file-systems">File systems</a></li>
<li class="toctree-l4"><a class="reference internal" href="andes_user_guide.html#lfs-setstripe-wrapper">LFS setstripe wrapper</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="andes_user_guide.html#shell-and-programming-environments">Shell and programming environments</a><ul>
<li class="toctree-l4"><a class="reference internal" href="andes_user_guide.html#default-shell">Default shell</a></li>
<li class="toctree-l4"><a class="reference internal" href="andes_user_guide.html#environment-management-with-lmod">Environment management with lmod</a></li>
<li class="toctree-l4"><a class="reference internal" href="andes_user_guide.html#installed-software">Installed Software</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="andes_user_guide.html#compiling">Compiling</a><ul>
<li class="toctree-l4"><a class="reference internal" href="andes_user_guide.html#available-compilers">Available compilers</a></li>
<li class="toctree-l4"><a class="reference internal" href="andes_user_guide.html#changing-compilers">Changing compilers</a></li>
<li class="toctree-l4"><a class="reference internal" href="andes_user_guide.html#compiler-wrappers">Compiler wrappers</a></li>
<li class="toctree-l4"><a class="reference internal" href="andes_user_guide.html#compiling-threaded-codes">Compiling threaded codes</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="andes_user_guide.html#running-jobs">Running Jobs</a><ul>
<li class="toctree-l4"><a class="reference internal" href="andes_user_guide.html#login-vs-compute-nodes-on-commodity-clusters">Login vs Compute Nodes on Commodity Clusters</a></li>
<li class="toctree-l4"><a class="reference internal" href="andes_user_guide.html#slurm">Slurm</a></li>
<li class="toctree-l4"><a class="reference internal" href="andes_user_guide.html#interactive-batch-jobs-on-commodity-clusters">Interactive Batch Jobs on Commodity Clusters</a></li>
<li class="toctree-l4"><a class="reference internal" href="andes_user_guide.html#common-batch-options-to-slurm">Common Batch Options to Slurm</a></li>
<li class="toctree-l4"><a class="reference internal" href="andes_user_guide.html#batch-environment-variables">Batch Environment Variables</a></li>
<li class="toctree-l4"><a class="reference internal" href="andes_user_guide.html#modifying-batch-jobs">Modifying Batch Jobs</a></li>
<li class="toctree-l4"><a class="reference internal" href="andes_user_guide.html#monitoring-batch-jobs">Monitoring Batch Jobs</a></li>
<li class="toctree-l4"><a class="reference internal" href="andes_user_guide.html#job-execution">Job Execution</a></li>
<li class="toctree-l4"><a class="reference internal" href="andes_user_guide.html#batch-queues-on-andes">Batch Queues on Andes</a></li>
<li class="toctree-l4"><a class="reference internal" href="andes_user_guide.html#job-accounting-on-andes">Job Accounting on Andes</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="andes_user_guide.html#andes-debugging">Debugging</a><ul>
<li class="toctree-l4"><a class="reference internal" href="andes_user_guide.html#linaro-ddt">Linaro DDT</a></li>
<li class="toctree-l4"><a class="reference internal" href="andes_user_guide.html#gdb">GDB</a></li>
<li class="toctree-l4"><a class="reference internal" href="andes_user_guide.html#valgrind">Valgrind</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="andes_user_guide.html#visualization-tools">Visualization tools</a><ul>
<li class="toctree-l4"><a class="reference internal" href="andes_user_guide.html#paraview">ParaView</a></li>
<li class="toctree-l4"><a class="reference internal" href="andes_user_guide.html#visit">VisIt</a></li>
<li class="toctree-l4"><a class="reference internal" href="andes_user_guide.html#remote-visualization-using-vnc-non-gpu">Remote Visualization using VNC (non-GPU)</a></li>
<li class="toctree-l4"><a class="reference internal" href="andes_user_guide.html#remote-visualization-using-vnc-gpu-nodes">Remote Visualization using VNC (GPU nodes)</a></li>
<li class="toctree-l4"><a class="reference internal" href="andes_user_guide.html#remote-visualization-using-nice-dcv-gpu-nodes-only">Remote Visualization using Nice DCV (GPU nodes only)</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="home_user_guide.html">Home</a><ul>
<li class="toctree-l3"><a class="reference internal" href="home_user_guide.html#system-overview">System Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="home_user_guide.html#access-connecting">Access &amp; Connecting</a></li>
<li class="toctree-l3"><a class="reference internal" href="home_user_guide.html#usage">Usage</a><ul>
<li class="toctree-l4"><a class="reference internal" href="home_user_guide.html#acceptable-tasks">Acceptable Tasks</a></li>
<li class="toctree-l4"><a class="reference internal" href="home_user_guide.html#unacceptable-tasks">Unacceptable Tasks</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="dtn_user_guide.html">Data Transfer Nodes (DTNs)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="dtn_user_guide.html#system-overview">System Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="dtn_user_guide.html#interactive-access">Interactive Access</a></li>
<li class="toctree-l3"><a class="reference internal" href="dtn_user_guide.html#access-from-globus-online">Access From Globus Online</a></li>
<li class="toctree-l3"><a class="reference internal" href="dtn_user_guide.html#batch-queue-slurm">Batch Queue (Slurm)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="dtn_user_guide.html#queue-policy">Queue Policy</a></li>
<li class="toctree-l4"><a class="reference internal" href="dtn_user_guide.html#submitting-jobs-to-frontier">Submitting jobs to Frontier</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="hpss_user_guide.html">High Performance Storage System</a><ul>
<li class="toctree-l3"><a class="reference internal" href="hpss_user_guide.html#system-overview">System Overview</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="odo_user_guide.html">Odo</a><ul>
<li class="toctree-l3"><a class="reference internal" href="odo_user_guide.html#system-overview">System Overview</a><ul>
<li class="toctree-l4"><a class="reference internal" href="odo_user_guide.html#file-systems">File Systems</a></li>
<li class="toctree-l4"><a class="reference internal" href="odo_user_guide.html#obtaining-access-to-odo">Obtaining Access to Odo</a></li>
<li class="toctree-l4"><a class="reference internal" href="odo_user_guide.html#logging-in-to-odo">Logging In to Odo</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="ascent_user_guide.html">Ascent</a><ul>
<li class="toctree-l3"><a class="reference internal" href="ascent_user_guide.html#system-overview">System Overview</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="spock_quick_start_guide.html">Spock Quick-Start Guide</a><ul>
<li class="toctree-l3"><a class="reference internal" href="spock_quick_start_guide.html#system-overview">System Overview</a><ul>
<li class="toctree-l4"><a class="reference internal" href="spock_quick_start_guide.html#spock-compute-nodes">Spock Compute Nodes</a></li>
<li class="toctree-l4"><a class="reference internal" href="spock_quick_start_guide.html#system-interconnect">System Interconnect</a></li>
<li class="toctree-l4"><a class="reference internal" href="spock_quick_start_guide.html#file-systems">File Systems</a></li>
<li class="toctree-l4"><a class="reference internal" href="spock_quick_start_guide.html#gpus">GPUs</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="spock_quick_start_guide.html#connecting">Connecting</a></li>
<li class="toctree-l3"><a class="reference internal" href="spock_quick_start_guide.html#data-and-storage">Data and Storage</a><ul>
<li class="toctree-l4"><a class="reference internal" href="spock_quick_start_guide.html#nfs">NFS</a></li>
<li class="toctree-l4"><a class="reference internal" href="spock_quick_start_guide.html#gpfs">GPFS</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="spock_quick_start_guide.html#programming-environment">Programming Environment</a><ul>
<li class="toctree-l4"><a class="reference internal" href="spock_quick_start_guide.html#environment-modules-lmod">Environment Modules (Lmod)</a></li>
<li class="toctree-l4"><a class="reference internal" href="spock_quick_start_guide.html#compilers">Compilers</a></li>
<li class="toctree-l4"><a class="reference internal" href="spock_quick_start_guide.html#mpi">MPI</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="spock_quick_start_guide.html#compiling">Compiling</a><ul>
<li class="toctree-l4"><a class="reference internal" href="spock_quick_start_guide.html#id3">MPI</a></li>
<li class="toctree-l4"><a class="reference internal" href="spock_quick_start_guide.html#openmp">OpenMP</a></li>
<li class="toctree-l4"><a class="reference internal" href="spock_quick_start_guide.html#openmp-gpu-offload">OpenMP GPU Offload</a></li>
<li class="toctree-l4"><a class="reference internal" href="spock_quick_start_guide.html#hip">HIP</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="spock_quick_start_guide.html#running-jobs">Running Jobs</a><ul>
<li class="toctree-l4"><a class="reference internal" href="spock_quick_start_guide.html#slurm-workload-manager">Slurm Workload Manager</a></li>
<li class="toctree-l4"><a class="reference internal" href="spock_quick_start_guide.html#slurm-compute-node-partitions">Slurm Compute Node Partitions</a></li>
<li class="toctree-l4"><a class="reference internal" href="spock_quick_start_guide.html#process-and-thread-mapping">Process and Thread Mapping</a></li>
<li class="toctree-l4"><a class="reference internal" href="spock_quick_start_guide.html#nvme-usage">NVMe Usage</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="spock_quick_start_guide.html#getting-help">Getting Help</a></li>
</ul>
</li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Crusher Quick-Start Guide</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#system-overview">System Overview</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#crusher-compute-nodes">Crusher Compute Nodes</a></li>
<li class="toctree-l4"><a class="reference internal" href="#system-interconnect">System Interconnect</a></li>
<li class="toctree-l4"><a class="reference internal" href="#file-systems">File Systems</a></li>
<li class="toctree-l4"><a class="reference internal" href="#gpus">GPUs</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#connecting">Connecting</a></li>
<li class="toctree-l3"><a class="reference internal" href="#data-and-storage">Data and Storage</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#nfs-filesystem">NFS Filesystem</a></li>
<li class="toctree-l4"><a class="reference internal" href="#lustre-filesystem">Lustre Filesystem</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#programming-environment">Programming Environment</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#environment-modules-lmod">Environment Modules (Lmod)</a></li>
<li class="toctree-l4"><a class="reference internal" href="#compilers">Compilers</a></li>
<li class="toctree-l4"><a class="reference internal" href="#mpi">MPI</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#compiling">Compiling</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id3">MPI</a></li>
<li class="toctree-l4"><a class="reference internal" href="#openmp">OpenMP</a></li>
<li class="toctree-l4"><a class="reference internal" href="#openmp-gpu-offload">OpenMP GPU Offload</a></li>
<li class="toctree-l4"><a class="reference internal" href="#hip">HIP</a></li>
<li class="toctree-l4"><a class="reference internal" href="#hip-openmp-cpu-threading">HIP + OpenMP CPU Threading</a></li>
<li class="toctree-l4"><a class="reference internal" href="#sycl">SYCL</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#running-jobs">Running Jobs</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#slurm-workload-manager">Slurm Workload Manager</a></li>
<li class="toctree-l4"><a class="reference internal" href="#slurm-compute-node-partitions">Slurm Compute Node Partitions</a></li>
<li class="toctree-l4"><a class="reference internal" href="#process-and-thread-mapping">Process and Thread Mapping</a></li>
<li class="toctree-l4"><a class="reference internal" href="#nvme-usage">NVMe Usage</a></li>
<li class="toctree-l4"><a class="reference internal" href="#tips-for-launching-at-scale">Tips for Launching at Scale</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#profiling-applications">Profiling Applications</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#getting-started-with-the-hpe-performance-analysis-tools-pat">Getting Started with the HPE Performance Analysis Tools (PAT)</a></li>
<li class="toctree-l4"><a class="reference internal" href="#getting-started-with-hpctoolkit">Getting Started with HPCToolkit</a></li>
<li class="toctree-l4"><a class="reference internal" href="#getting-started-with-the-rocm-profiler">Getting Started with the ROCm Profiler</a></li>
<li class="toctree-l4"><a class="reference internal" href="#roofline-profiling-with-the-rocm-profiler">Roofline Profiling with the ROCm Profiler</a></li>
<li class="toctree-l4"><a class="reference internal" href="#omnitrace">Omnitrace</a></li>
<li class="toctree-l4"><a class="reference internal" href="#omniperf">Omniperf</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#notable-differences-between-summit-and-crusher">Notable Differences between Summit and Crusher</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#using-reduced-precision-fp16-and-bf16-datatypes">Using reduced precision (FP16 and BF16 datatypes)</a></li>
<li class="toctree-l4"><a class="reference internal" href="#enabling-gpu-page-migration">Enabling GPU Page Migration</a></li>
<li class="toctree-l4"><a class="reference internal" href="#floating-point-fp-atomic-operations-and-coarse-fine-grained-memory-allocations">Floating-Point (FP) Atomic Operations and Coarse/Fine Grained Memory Allocations</a></li>
<li class="toctree-l4"><a class="reference internal" href="#performance-considerations-for-lds-fp-atomicadd">Performance considerations for LDS FP atomicAdd()</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#system-updates">System Updates</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id6">2024-03-19</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id7">2024-01-23</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id8">2023-12-05</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id9">2023-10-03</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id10">2023-09-19</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id11">2023-07-18</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id12">2023-04-05</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id13">2022-12-29</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#getting-help">Getting Help</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../services_and_applications/index.html">Services and Applications</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../services_and_applications/slate/index.html">Slate</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../services_and_applications/slate/overview.html">Overview</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/overview.html#what-is-slate">What is Slate?</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/overview.html#what-is-kubernetes">What is Kubernetes?</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/overview.html#what-is-openshift">What is OpenShift?</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../services_and_applications/slate/getting_started.html">Getting Started</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/getting_started.html#requesting-a-slate-project-allocation">Requesting A Slate Project Allocation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/getting_started.html#logging-in">Logging in</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/getting_started.html#slate-namespaces">Slate Namespaces</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/getting_started.html#install-the-oc-tool">Install the OC tool</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/getting_started.html#test-login-with-oc-tool">Test login with OC Tool</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../services_and_applications/slate/guided_tutorial.html">Guided Tutorial</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/guided_tutorial.html#creating-your-project">Creating your project</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/guided_tutorial.html#guided-web-gui-tutorial">Guided Web GUI Tutorial</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../services_and_applications/slate/guided_tutorial_cli.html">Guided Tutorial: CLI</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/guided_tutorial_cli.html#adding-a-pod-to-your-project">Adding a Pod to your Project</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../services_and_applications/slate/image_building.html">Image Building</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/image_building.html#build-types">Build Types</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/image_building.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/image_building.html#logging-into-the-registry-externally">Logging into the registry externally</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../services_and_applications/slate/workloads/index.html">Workloads</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/workloads/pod.html">Pods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/workloads/deployment.html">Deployments</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../services_and_applications/slate/networking/index.html">Networking</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/networking/services.html">Services</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/networking/nodeport.html">NodePorts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/networking/route.html">Routes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/networking/networkpolicy.html">Network Policies</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/networking/port_forwarding.html">Quick Access from Outside Slate</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../services_and_applications/slate/storage.html">Persistent Storage</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/storage.html#creating-a-persistent-volume-claim">Creating A Persistent Volume Claim</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/storage.html#adding-pvc-to-pod">Adding PVC To Pod</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/storage.html#backups">Backups</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../services_and_applications/slate/workflows/index.html">Workflows</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/workflows/overview.html">Workflows Overview</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/workflows/openshift_gitops.html">OpenShift GitOps</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/workflows/openshift_pipelines.html">OpenShift Pipelines</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../services_and_applications/slate/use_cases/index.html">Application Deployment Examples</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/use_cases/simple_website.html">Build and Deploy Simple Website</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/use_cases/mongodb_service.html">Deploy MongoDB</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/use_cases/nginx_hello_world.html">Deploy NGINX with Hello World</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/use_cases/gitlab_runner.html">GitLab Runners</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/use_cases/helm_example.html">Deploy Packages with Helm</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/use_cases/helm_prerequisite.html">Helm Prerequisites</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/use_cases/minio.html">MinIO Object Store (On an NCCS Filesystem)</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../services_and_applications/slate/access_olcf_resources/index.html">Access OLCF Resources From Containers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/access_olcf_resources/job_submit.html">Batch Job Submission</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/access_olcf_resources/mount_fs.html">Mount OLCF Filesystems</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../services_and_applications/slate/other_resources.html">Schedule Other Slate Resources</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/other_resources.html#gpus">GPUs</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../services_and_applications/slate/olcf_provided_applications/index.html">OLCF-Provided Applications on Slate</a><ul class="simple">
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../services_and_applications/slate/troubleshooting/index.html">Troubleshooting</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/troubleshooting/fix-writable-directories.html">Fix Container Image Permissions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/troubleshooting/debugging.html">Debugging</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../services_and_applications/slate/examples.html">YAML Object Quick Reference</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/examples.html#cronjobs">CronJobs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/examples.html#deployments-and-stateful-sets">Deployments and Stateful Sets</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/examples.html#pods">Pods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/examples.html#roles-and-rolebindings">Roles and Rolebindings</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/examples.html#routes-services-and-nodeports">Routes, Services and Nodeports</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/examples.html#persistent-volume-claims">Persistent Volume Claims</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../services_and_applications/slate/glossary.html">Glossary</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../services_and_applications/myolcf/index.html">myOLCF</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../services_and_applications/myolcf/overview.html">Overview</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/myolcf/overview.html#what-is-myolcf">What is myOLCF?</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/myolcf/overview.html#what-can-it-do">What can it do?</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/myolcf/overview.html#can-i-suggest-a-feature">Can I suggest a feature?</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../services_and_applications/myolcf/authenticating.html">Authenticating</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/myolcf/authenticating.html#olcf-moderate-accounts">OLCF Moderate Accounts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/myolcf/authenticating.html#olcf-open-accounts">OLCF Open Accounts</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../services_and_applications/myolcf/project_pages/project_pages.html">Project Pages</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/myolcf/project_pages/project_pages.html#project-context">Project Context</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/myolcf/project_pages/project_pages.html#switching-project-contexts">Switching Project Contexts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/myolcf/project_pages/project_pages.html#available-pages">Available Pages</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../services_and_applications/myolcf/account_pages/account_pages.html">Account Pages</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/myolcf/account_pages/account_pages.html#account-context">Account Context</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/myolcf/account_pages/account_pages.html#available-pages">Available Pages</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../services_and_applications/myolcf/account_pages/processing_membership_requests.html">Processing Project Membership Requests</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../services_and_applications/jupyter/index.html">Jupyter</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../services_and_applications/jupyter/overview.html">Overview</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/jupyter/overview.html#jupyter-at-olcf">Jupyter at OLCF</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/jupyter/overview.html#access">Access</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/jupyter/overview.html#cpu-vs-gpu-jupyterlab-available-resources">CPU vs. GPU JupyterLab (Available Resources)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/jupyter/overview.html#working-within-lustre-and-nfs-launching-a-notebook">Working within Lustre and NFS (Launching a notebook)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/jupyter/overview.html#conda-environments-and-custom-notebooks">Conda environments and custom notebooks</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/jupyter/overview.html#manually-stopping-your-jupyterlab-session">Manually stopping your JupyterLab session</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/jupyter/overview.html#things-to-be-aware-of">Things to be aware of</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/jupyter/overview.html#example-jupyter-notebooks">Example Jupyter Notebooks</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../data/index.html">Data Storage and Transfers</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../data/index.html#summary-of-storage-areas">Summary of Storage Areas</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../data/index.html#notes-on-user-centric-data-storage">Notes on User-Centric Data Storage</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../data/index.html#user-home-directories-nfs">User Home Directories (NFS)</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../data/index.html#notes-on-project-centric-data-storage">Notes on Project-Centric Data Storage</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../data/index.html#project-home-directories-nfs">Project Home Directories (NFS)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../data/index.html#project-work-areas">Project Work Areas</a></li>
<li class="toctree-l4"><a class="reference internal" href="../data/index.html#project-archive-directories">Project Archive Directories</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../data/index.html#data-policies">Data Policies</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../data/index.html#information">Information</a></li>
<li class="toctree-l3"><a class="reference internal" href="../data/index.html#special-requests">Special Requests</a></li>
<li class="toctree-l3"><a class="reference internal" href="../data/index.html#data-retention">Data Retention</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../data/index.html#orion-lustre-hpe-clusterstor-filesystem">Orion Lustre HPE ClusterStor Filesystem</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../data/index.html#orion-performance-tiers-and-file-striping-policy">Orion Performance Tiers and File Striping Policy</a></li>
<li class="toctree-l3"><a class="reference internal" href="../data/index.html#i-o-patterns-that-benefit-from-file-striping">I/O Patterns that Benefit from File Striping</a></li>
<li class="toctree-l3"><a class="reference internal" href="../data/index.html#lfs-setstripe-wrapper">LFS setstripe wrapper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../data/index.html#lustre-file-locking-tips">Lustre File Locking Tips</a></li>
<li class="toctree-l3"><a class="reference internal" href="../data/index.html#darshan-runtime-and-i-o-profiling">Darshan-runtime and I/O Profiling</a></li>
<li class="toctree-l3"><a class="reference internal" href="../data/index.html#purge">Purge</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../data/index.html#alpine2-ibm-spectrum-scale-filesystem">Alpine2 IBM Spectrum Scale Filesystem</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../data/index.html#alpine2-performance-under-non-ideal-workloads">Alpine2 Performance under non-ideal workloads</a></li>
<li class="toctree-l3"><a class="reference internal" href="../data/index.html#tips">Tips</a></li>
<li class="toctree-l3"><a class="reference internal" href="../data/index.html#major-difference-between-lustre-hpe-clusterstor-and-ibm-spectrum-scale">Major difference between Lustre HPE ClusterStor and IBM Spectrum Scale</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../data/index.html#hpss-data-archival-system">HPSS Data Archival System</a></li>
<li class="toctree-l2"><a class="reference internal" href="../data/index.html#kronos-nearline-archival-storage-system">Kronos Nearline Archival Storage System</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../data/index.html#access-data-transfer">Access / Data Transfer</a></li>
<li class="toctree-l3"><a class="reference internal" href="../data/index.html#directory-structure">Directory Structure</a></li>
<li class="toctree-l3"><a class="reference internal" href="../data/index.html#project-quotas">Project Quotas</a></li>
<li class="toctree-l3"><a class="reference internal" href="../data/index.html#kronos-and-hpss-comparison">Kronos and HPSS Comparison</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../data/index.html#transferring-data">Transferring Data</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../data/index.html#data-transferring-data-globus">Globus</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../data/index.html#using-globus-to-move-data-between-collections">Using Globus to Move Data Between Collections</a></li>
<li class="toctree-l4"><a class="reference internal" href="../data/index.html#using-globus-from-your-local-workstation">Using Globus From Your Local Workstation</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../data/index.html#hsi">HSI</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../data/index.html#additional-hsi-documentation">Additional HSI Documentation</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../data/index.html#htar">HTAR</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../data/index.html#htar-limitations">HTAR Limitations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../data/index.html#additional-htar-documentation">Additional HTAR Documentation</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../data/index.html#command-line-terminal-tools">Command-Line/Terminal Tools</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../data/index.html#burst-buffer-and-spectral-library">Burst Buffer and Spectral Library</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../software/index.html">Software</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../software/SWList.html">SWList</a></li>
<li class="toctree-l2"><a class="reference internal" href="../software/SWList_flat_table.html">SWList_Flat_Table</a></li>
<li class="toctree-l2"><a class="reference internal" href="../software/SWList_less_wide.html">SWList_Less_Wide</a></li>
<li class="toctree-l2"><a class="reference internal" href="../software/SWList_condensed.html">SWList_condensed</a></li>
<li class="toctree-l2"><a class="reference internal" href="../software/SWList_expanding.html">SWList_expanding_copy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../software/software-news.html">Software News</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../software/software-news.html#frontier-updated-modules-for-cpe-23-12-october-16-2024">Frontier: Updated modules for cpe/23.12 (October 16 2024)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../software/software-news.html#frontier-core-module-october-15-2024">Frontier: Core module (October 15, 2024)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../software/software-news.html#frontier-system-software-update-july-16-2024">Frontier: System Software Update (July 16, 2024)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../software/software-news.html#frontier-user-environment-changes-july-9-2024">Frontier: User Environment Changes (July 9, 2024)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../software/analytics/index.html">ML/DL &amp; Data Analytics</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../software/analytics/ibm-wml-ce.html">IBM Watson Machine Learning CE -&gt; Open CE</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../software/analytics/ibm-wml-ce.html#getting-started">Getting Started</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/analytics/ibm-wml-ce.html#running-distributed-deep-learning-jobs">Running Distributed Deep Learning Jobs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/analytics/ibm-wml-ce.html#setting-up-custom-environments">Setting up Custom Environments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/analytics/ibm-wml-ce.html#best-distributed-deep-learning-performance">Best Distributed Deep Learning Performance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/analytics/ibm-wml-ce.html#troubleshooting-tips">Troubleshooting Tips</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../software/analytics/pbdR.html">R and pbdR on Summit</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../software/analytics/pbdR.html#loading-r">Loading R</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/analytics/pbdR.html#how-to-run-an-r-script">How to Run an R Script</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/analytics/pbdR.html#r-hello-world-example">R Hello World Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/analytics/pbdR.html#pbdr-hello-world-example">pbdR Hello World Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/analytics/pbdR.html#common-r-packages-for-parallelism">Common R Packages for Parallelism</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/analytics/pbdR.html#gpu-computing-with-r">GPU Computing with R</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/analytics/pbdR.html#more-information">More Information</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../software/analytics/nvidia-rapids.html">NVIDIA RAPIDS</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../software/analytics/nvidia-rapids.html#overview">Overview</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/analytics/nvidia-rapids.html#getting-started">Getting Started</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/analytics/nvidia-rapids.html#rapids-on-jupyter">RAPIDS on Jupyter</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/analytics/nvidia-rapids.html#rapids-on-summit">RAPIDS on Summit</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/analytics/nvidia-rapids.html#setting-up-custom-environments">Setting up Custom Environments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/analytics/nvidia-rapids.html#blazingsql-distributed-execution">BlazingSQL Distributed Execution</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../software/analytics/apache-spark.html">Apache Spark</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../software/analytics/apache-spark.html#overview">Overview</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/analytics/apache-spark.html#getting-started">Getting Started</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../software/python/index.html">Python on OLCF Systems</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../software/python/index.html#overview">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="../software/python/index.html#olcf-python-guides">OLCF Python Guides</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../software/python/conda_basics.html">Conda Basics</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/python/parallel_h5py.html">Installing mpi4py and h5py</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/python/cupy.html">Installing CuPy</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/python/sbcast_conda.html">Sbcast Conda Environments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/python/jupyter_envs.html">Jupyter Visibility</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/python/pytorch_summit.html">PyTorch on Summit</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/python/pytorch_frontier.html">PyTorch on Frontier</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../software/python/index.html#module-usage">Module Usage</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../software/python/index.html#base-environment">Base Environment</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/python/index.html#custom-environments">Custom Environments</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../software/python/index.html#how-to-run">How to Run</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../software/python/index.html#summit">Summit</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/python/index.html#frontier-andes">Frontier / Andes</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../software/python/index.html#best-practices">Best Practices</a></li>
<li class="toctree-l3"><a class="reference internal" href="../software/python/index.html#additional-resources">Additional Resources</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../software/profiling/index.html">Profiling Tools</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../software/profiling/Scorep.html">Score-P</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../software/profiling/Scorep.html#overview">Overview</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/profiling/Scorep.html#usage">Usage</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/profiling/Scorep.html#instrumentation">Instrumentation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/profiling/Scorep.html#measurement">Measurement</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/profiling/Scorep.html#profiling">Profiling</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/profiling/Scorep.html#tracing">Tracing</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/profiling/Scorep.html#manual-instrumentation">Manual Instrumentation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/profiling/Scorep.html#score-p-demo-video">Score-P Demo Video</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../software/profiling/TAU.html">Tuning and Analysis Utilities (TAU)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../software/profiling/TAU.html#run-time-environment-variables">Run-Time Environment Variables</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/profiling/TAU.html#compile-time-environment-variables">Compile-Time Environment Variables</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/profiling/TAU.html#miniweather-example-application">MiniWeather Example Application</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/profiling/TAU.html#cuda-profiling-tools-interface">CUDA Profiling Tools Interface</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/profiling/TAU.html#tracing">Tracing</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/profiling/TAU.html#selective-instrumentation">Selective Instrumentation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/profiling/TAU.html#dynamic-phase">Dynamic Phase</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/profiling/TAU.html#static-phase">Static Phase</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/profiling/TAU.html#openmp-offload">OpenMP Offload</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../software/profiling/Vampir.html">Vampir</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../software/profiling/Vampir.html#overview">Overview</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/profiling/Vampir.html#usage">Usage</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/profiling/Vampir.html#vampir-on-a-login-node">Vampir on a Login Node</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/profiling/Vampir.html#vampir-using-vampirserver">Vampir Using VampirServer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/profiling/Vampir.html#vampir-tunneling-to-vampirserver">Vampir Tunneling to VampirServer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/profiling/Vampir.html#vampir-gui-demo">Vampir GUI Demo</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../software/UMS/index.html">User-Managed Software</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../software/UMS/index.html#introduction">Introduction</a></li>
<li class="toctree-l3"><a class="reference internal" href="../software/UMS/index.html#currently-available-user-managed-software">Currently Available User-Managed Software</a></li>
<li class="toctree-l3"><a class="reference internal" href="../software/UMS/index.html#usage">Usage</a></li>
<li class="toctree-l3"><a class="reference internal" href="../software/UMS/index.html#policies">Policies</a></li>
<li class="toctree-l3"><a class="reference internal" href="../software/UMS/index.html#writing-ums-modulefiles">Writing UMS Modulefiles</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../software/workflows/index.html">Workflows</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../software/workflows/index.html#running-workflows-on-olcf-resources">Running Workflows on OLCF Resources</a></li>
<li class="toctree-l3"><a class="reference internal" href="../software/workflows/index.html#workflow-systems">Workflow Systems</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../software/workflows/entk.html">Ensemble Toolkit (EnTK)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/workflows/fireworks.html">FireWorks</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/workflows/mlflow.html">MLflow</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/workflows/parsl.html">Parsl</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/workflows/pmake.html">pmake</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/workflows/swift_t.html">Swift/T</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/workflows/libensemble.html">libEnsemble</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../software/e4s.html">E4S Software Stack</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../software/e4s.html#summit">Summit</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../software/e4s.html#access-via-modulefiles">Access via modulefiles</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/e4s.html#e4s-21-08-packages">E4S 21.08 Packages</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/e4s.html#e4s-21-05-packages">E4S 21.05 Packages</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../software/e4s.html#spock">Spock</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../software/e4s.html#id1">Access via modulefiles</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/e4s.html#id2">E4S 21.08 Packages</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/e4s.html#id3">E4S 21.05 Packages</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../software/spack_environments.html">Spack Environments</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../software/spack_environments.html#purpose">Purpose</a></li>
<li class="toctree-l3"><a class="reference internal" href="../software/spack_environments.html#getting-started">Getting Started</a></li>
<li class="toctree-l3"><a class="reference internal" href="../software/spack_environments.html#add-dependencies-to-the-environment">Add Dependencies to the environment</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../software/spack_environments.html#adding-olcf-modulefiles-as-external-packages">Adding OLCF Modulefiles as External Packages</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/spack_environments.html#adding-user-defined-dependencies-to-the-environment">Adding User-Defined Dependencies to the environment</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../software/spack_environments.html#installing-the-environment">Installing the Environment</a></li>
<li class="toctree-l3"><a class="reference internal" href="../software/spack_environments.html#more-details">More Details</a></li>
<li class="toctree-l3"><a class="reference internal" href="../software/spack_environments.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../software/viz_tools/index.html">Visualization Tools</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../software/viz_tools/visit.html">VisIt</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../software/viz_tools/visit.html#overview">Overview</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/viz_tools/visit.html#installing-and-setting-up-visit">Installing and Setting Up Visit</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/viz_tools/visit.html#remote-gui-usage">Remote GUI Usage</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/viz_tools/visit.html#command-line-example">Command Line Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/viz_tools/visit.html#troubleshooting">Troubleshooting</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/viz_tools/visit.html#additional-resources">Additional Resources</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../software/viz_tools/paraview.html">ParaView</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../software/viz_tools/paraview.html#overview">Overview</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/viz_tools/paraview.html#installing-and-setting-up-paraview">Installing and Setting Up ParaView</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/viz_tools/paraview.html#remote-gui-usage">Remote GUI Usage</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/viz_tools/paraview.html#command-line-example">Command Line Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/viz_tools/paraview.html#troubleshooting">Troubleshooting</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/viz_tools/paraview.html#additional-resources">Additional Resources</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../software/containers_on_summit.html">Containers on Summit</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../software/containers_on_summit.html#basic-information">Basic Information</a></li>
<li class="toctree-l3"><a class="reference internal" href="../software/containers_on_summit.html#setup-before-building">Setup before Building</a></li>
<li class="toctree-l3"><a class="reference internal" href="../software/containers_on_summit.html#build-and-run-workflow">Build and Run Workflow</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../software/containers_on_summit.html#building-a-simple-image">Building a Simple Image</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/containers_on_summit.html#using-a-container-registry-to-build-and-save-your-images">Using a Container Registry to Build and Save your Images</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/containers_on_summit.html#running-a-simple-container-in-a-batch-job">Running a Simple Container in a Batch Job</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/containers_on_summit.html#running-an-mpi-program-with-the-olcf-mpi-base-image">Running an MPI program with the OLCF MPI base image</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/containers_on_summit.html#running-a-single-node-gpu-program-with-the-olcf-mpi-base-image">Running a single node GPU program with the OLCF MPI base image</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/containers_on_summit.html#running-a-cuda-aware-mpi-program-with-the-olcf-mpi-base-image">Running a CUDA-Aware MPI program with the OLCF MPI base image</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../software/containers_on_summit.html#tips-tricks-and-things-to-watch-out-for">Tips, Tricks, and Things to Watch Out For</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../software/containers_on_frontier.html">Containers on Frontier</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../software/containers_on_frontier.html#examples-for-building-and-running-containers">Examples for Building and Running Containers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../software/containers_on_frontier.html#building-and-running-a-container-image-from-a-base-linux-distribution-for-mpi">Building and running a container image from a base Linux distribution for MPI</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/containers_on_frontier.html#pushing-your-apptainer-image-to-an-oci-registry-supporting-oras-e-g-dockerhub">Pushing your Apptainer image to an OCI Registry supporting ORAS (e.g. DockerHub)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/containers_on_frontier.html#building-an-image-on-top-of-an-existing-image-local-docker-image-oci-artifact">Building an image on top of an existing image (local, docker image, OCI artifact)</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../software/containers_on_frontier.html#olcf-base-images-apptainer-modules">OLCF Base Images &amp; Apptainer Modules</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../software/containers_on_frontier.html#base-images">Base Images</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/containers_on_frontier.html#apptainer-modules">Apptainer Modules</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/containers_on_frontier.html#example-workflow">Example Workflow</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../software/containers_on_frontier.html#some-restrictions-and-tips">Some Restrictions and Tips</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../software/debugging/index.html">Debugging</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../software/debugging/index.html#linaro-forge-ddt">Linaro Forge DDT</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../software/debugging/index.html#client-setup-and-usage">Client Setup and Usage</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/debugging/index.html#download">Download</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/debugging/index.html#installation">Installation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/debugging/index.html#configuration">Configuration</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../software/debugging/index.html#gnu-gdb">GNU GDB</a></li>
<li class="toctree-l3"><a class="reference internal" href="../software/debugging/index.html#valgrind">Valgrind</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../training/index.html">Training</a><ul>
<li class="toctree-l2"><a class="reference external" href="https://www.olcf.ornl.gov/for-users/training/training-calendar" target="_blank">OLCF Training Calendar</a></li>
<li class="toctree-l2"><a class="reference external" href="https://github.com/olcf-tutorials" target="_blank">OLCF Tutorials</a></li>
<li class="toctree-l2"><a class="reference internal" href="../training/training_archive.html">OLCF Training Archive</a></li>
<li class="toctree-l2"><a class="reference internal" href="../training/olcf_gpu_hackathons.html">OLCF GPU Hackathons</a></li>
<li class="toctree-l2"><a class="reference internal" href="../training/olcf_gpu_hackathons.html#facultyhack">FacultyHack</a></li>
<li class="toctree-l2"><a class="reference external" href="https://vimeo.com/channels/olcftraining" target="_blank">OLCF Vimeo Channel</a></li>
<li class="toctree-l2"><a class="reference internal" href="../training/index.html#new-user-quick-start">New User Quick Start</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quantum/index.html">Quantum</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../quantum/quantum_access.html">Quantum Computing User Program (QCUP) Access</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../quantum/quantum_access.html#qcup-priorities">QCUP Priorities</a></li>
<li class="toctree-l3"><a class="reference internal" href="../quantum/quantum_access.html#project-allocations">Project Allocations</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../quantum/quantum_access.html#what-happens-after-a-project-request-is-approved">What happens after a project request is approved?</a></li>
<li class="toctree-l4"><a class="reference internal" href="../quantum/quantum_access.html#project-renewals">Project Renewals</a></li>
<li class="toctree-l4"><a class="reference internal" href="../quantum/quantum_access.html#closeout-and-quarterly-reports">Closeout and Quarterly Reports</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../quantum/quantum_access.html#user-accounts">User Accounts</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../quantum/quantum_access.html#checking-the-status-of-your-application">Checking the status of your application</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../quantum/quantum_access.html#accessing-quantum-resources">Accessing Quantum Resources</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../quantum/quantum_access.html#ibm-quantum-computing">IBM Quantum Computing</a></li>
<li class="toctree-l4"><a class="reference internal" href="../quantum/quantum_access.html#quantinuum">Quantinuum</a></li>
<li class="toctree-l4"><a class="reference internal" href="../quantum/quantum_access.html#rigetti">Rigetti</a></li>
<li class="toctree-l4"><a class="reference internal" href="../quantum/quantum_access.html#ionq">IonQ</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../quantum/quantum_access.html#publication-citations">Publication Citations</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../quantum/quantum_systems/index.html">Quantum Systems</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../quantum/quantum_systems/ibm_quantum.html">IBM Quantum</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../quantum/quantum_systems/ibm_quantum.html#overview">Overview</a></li>
<li class="toctree-l4"><a class="reference internal" href="../quantum/quantum_systems/ibm_quantum.html#connecting">Connecting</a></li>
<li class="toctree-l4"><a class="reference internal" href="../quantum/quantum_systems/ibm_quantum.html#running-jobs-queue-policies">Running Jobs &amp; Queue Policies</a></li>
<li class="toctree-l4"><a class="reference internal" href="../quantum/quantum_systems/ibm_quantum.html#checking-system-availability-capability">Checking System Availability &amp; Capability</a></li>
<li class="toctree-l4"><a class="reference internal" href="../quantum/quantum_systems/ibm_quantum.html#software">Software</a></li>
<li class="toctree-l4"><a class="reference internal" href="../quantum/quantum_systems/ibm_quantum.html#additional-resources">Additional Resources</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../quantum/quantum_systems/rigetti.html">Rigetti</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../quantum/quantum_systems/rigetti.html#overview">Overview</a></li>
<li class="toctree-l4"><a class="reference internal" href="../quantum/quantum_systems/rigetti.html#connecting">Connecting</a></li>
<li class="toctree-l4"><a class="reference internal" href="../quantum/quantum_systems/rigetti.html#running-jobs">Running Jobs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../quantum/quantum_systems/rigetti.html#allocations-credit-usage">Allocations &amp; Credit Usage</a></li>
<li class="toctree-l4"><a class="reference internal" href="../quantum/quantum_systems/rigetti.html#data-storage-policies">Data Storage Policies</a></li>
<li class="toctree-l4"><a class="reference internal" href="../quantum/quantum_systems/rigetti.html#software">Software</a></li>
<li class="toctree-l4"><a class="reference internal" href="../quantum/quantum_systems/rigetti.html#additional-resources">Additional Resources</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../quantum/quantum_systems/quantinuum.html">Quantinuum</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../quantum/quantum_systems/quantinuum.html#overview">Overview</a></li>
<li class="toctree-l4"><a class="reference internal" href="../quantum/quantum_systems/quantinuum.html#connecting">Connecting</a></li>
<li class="toctree-l4"><a class="reference internal" href="../quantum/quantum_systems/quantinuum.html#running-jobs-queue-policies">Running Jobs &amp; Queue Policies</a></li>
<li class="toctree-l4"><a class="reference internal" href="../quantum/quantum_systems/quantinuum.html#allocations-credit-usage">Allocations &amp; Credit Usage</a></li>
<li class="toctree-l4"><a class="reference internal" href="../quantum/quantum_systems/quantinuum.html#software">Software</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../quantum/quantum_systems/ionq.html">IonQ</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../quantum/quantum_systems/ionq.html#overview">Overview</a></li>
<li class="toctree-l4"><a class="reference internal" href="../quantum/quantum_systems/ionq.html#ionq-systems">IonQ systems</a></li>
<li class="toctree-l4"><a class="reference internal" href="../quantum/quantum_systems/ionq.html#connecting">Connecting</a></li>
<li class="toctree-l4"><a class="reference internal" href="../quantum/quantum_systems/ionq.html#running-jobs-queue-policies">Running Jobs &amp; Queue Policies</a></li>
<li class="toctree-l4"><a class="reference internal" href="../quantum/quantum_systems/ionq.html#allocations-credit-usage">Allocations &amp; Credit Usage</a></li>
<li class="toctree-l4"><a class="reference internal" href="../quantum/quantum_systems/ionq.html#checking-system-availability-capability">Checking System Availability &amp; Capability</a></li>
<li class="toctree-l4"><a class="reference internal" href="../quantum/quantum_systems/ionq.html#additional-resources">Additional Resources</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../quantum/quantum_software/index.html">Quantum Software</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../quantum/quantum_software/hybrid_hpc.html">Quantum Software on HPC Systems</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../quantum/quantum_software/hybrid_hpc.html#overview">Overview</a></li>
<li class="toctree-l4"><a class="reference internal" href="../quantum/quantum_software/hybrid_hpc.html#qiskit">Qiskit</a></li>
<li class="toctree-l4"><a class="reference internal" href="../quantum/quantum_software/hybrid_hpc.html#pyquil-forest-sdk-rigetti">PyQuil/Forest SDK (Rigetti)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../quantum/quantum_software/hybrid_hpc.html#pennylane">PennyLane</a></li>
<li class="toctree-l4"><a class="reference internal" href="../quantum/quantum_software/hybrid_hpc.html#pytket">Pytket</a></li>
<li class="toctree-l4"><a class="reference internal" href="../quantum/quantum_software/hybrid_hpc.html#cuda-q">CUDA-Q</a></li>
<li class="toctree-l4"><a class="reference internal" href="../quantum/quantum_software/hybrid_hpc.html#batch-jobs">Batch Jobs</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../quantum/hello_qcup.html">Hello QCUP Scripts</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../quantum/hello_qcup.html#overview">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="../quantum/hello_qcup.html#ibm-quantum">IBM Quantum</a></li>
<li class="toctree-l3"><a class="reference internal" href="../quantum/hello_qcup.html#quantinuum">Quantinuum</a></li>
<li class="toctree-l3"><a class="reference internal" href="../quantum/hello_qcup.html#ionq">IonQ</a></li>
<li class="toctree-l3"><a class="reference internal" href="../quantum/hello_qcup.html#rigetti">Rigetti</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../quantum/quantum_faq.html">Frequently Asked Questions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../quantum/quantum_faq.html#how-do-quantum-computers-differ-from-classical-computers">How do quantum computers differ from classical computers?</a></li>
<li class="toctree-l3"><a class="reference internal" href="../quantum/quantum_faq.html#what-is-a-qubit">What is a qubit?</a></li>
<li class="toctree-l3"><a class="reference internal" href="../quantum/quantum_faq.html#how-do-i-access-the-olcf-quantum-computing-resources">How do I access the OLCF quantum computing resources?</a></li>
<li class="toctree-l3"><a class="reference internal" href="../quantum/quantum_faq.html#what-happens-after-i-apply-for-access-to-qcup">What happens after I apply for access to QCUP?</a></li>
<li class="toctree-l3"><a class="reference internal" href="../quantum/quantum_faq.html#i-formerly-had-access-to-quantum-resources-but-my-backends-lattices-etc-have-disappeared-what-do-i-do">I formerly had access to quantum resources, but my backends/lattices/etc. have disappeared, what do I do?</a></li>
<li class="toctree-l3"><a class="reference internal" href="../quantum/quantum_faq.html#i-applied-to-a-quantum-computing-resource-via-the-vendor-website-but-dont-have-access-what-do-i-do">I applied to a quantum computing resource via the vendor website, but dont have access; what do I do?</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../spi/index.html">Scalable Protected Infrastructure (SPI)</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../spi/index.html#what-is-spi">What is SPI</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../spi/index.html#what-is-citadel">What is Citadel</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../spi/index.html#new-user-quickstart">New User QuickStart</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../spi/index.html#notable-differences">Notable Differences</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../spi/index.html#allocations-and-user-accounts">Allocations and User Accounts</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../spi/index.html#allocations-projects">Allocations (Projects)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../spi/index.html#requesting-a-new-allocation-project">Requesting a New Allocation (Project)</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../spi/index.html#user-accounts">User Accounts</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../spi/index.html#requesting-a-new-user-account">Requesting a New User Account</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../spi/index.html#available-resources">Available Resources</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../spi/index.html#compute"><span class="xref std std-ref">Compute</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../spi/index.html#file-systems"><span class="xref std std-ref">File Systems</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../spi/index.html#data-transfer"><span class="xref std std-ref">Data Transfer</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../spi/index.html#ip-whitelisting">IP Whitelisting</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../spi/index.html#whitelisting-an-ip-or-range">Whitelisting an IP or range</a></li>
<li class="toctree-l3"><a class="reference internal" href="../spi/index.html#finding-your-ip">Finding your IP</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../spi/index.html#citadel">Citadel</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../spi/index.html#login-nodes">Login Nodes</a></li>
<li class="toctree-l3"><a class="reference internal" href="../spi/index.html#connecting">Connecting</a></li>
<li class="toctree-l3"><a class="reference internal" href="../spi/index.html#building-software">Building Software</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../spi/index.html#external-repositories">External Repositories</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../spi/index.html#running-batch-jobs">Running Batch Jobs</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../spi/index.html#spi-file-systems">File Systems</a></li>
<li class="toctree-l2"><a class="reference internal" href="../spi/index.html#spi-data-transfer">Data Transfer</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../ace_testbed/index.html">Advanced Computing Ecosystem Testbed (ACE)</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../ace_testbed/defiant_quick_start_guide.html">Defiant Quick-Start Guide</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../ace_testbed/defiant_quick_start_guide.html#system-overview">System Overview</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../ace_testbed/defiant_quick_start_guide.html#defiant-compute-nodes">Defiant Compute Nodes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../ace_testbed/defiant_quick_start_guide.html#system-interconnect">System Interconnect</a></li>
<li class="toctree-l4"><a class="reference internal" href="../ace_testbed/defiant_quick_start_guide.html#file-systems">File Systems</a></li>
<li class="toctree-l4"><a class="reference internal" href="../ace_testbed/defiant_quick_start_guide.html#gpus">GPUs</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../ace_testbed/defiant_quick_start_guide.html#connecting">Connecting</a></li>
<li class="toctree-l3"><a class="reference internal" href="../ace_testbed/defiant_quick_start_guide.html#data-and-storage">Data and Storage</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../ace_testbed/defiant_quick_start_guide.html#nfs-filesystem">NFS Filesystem</a></li>
<li class="toctree-l4"><a class="reference internal" href="../ace_testbed/defiant_quick_start_guide.html#lustre-filesystem-polis">Lustre Filesystem (Polis)</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../ace_testbed/defiant_quick_start_guide.html#programming-environment">Programming Environment</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../ace_testbed/defiant_quick_start_guide.html#environment-modules-lmod">Environment Modules (Lmod)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../ace_testbed/defiant_quick_start_guide.html#compilers">Compilers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../ace_testbed/defiant_quick_start_guide.html#mpi">MPI</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../ace_testbed/defiant_quick_start_guide.html#compiling">Compiling</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../ace_testbed/defiant_quick_start_guide.html#id3">MPI</a></li>
<li class="toctree-l4"><a class="reference internal" href="../ace_testbed/defiant_quick_start_guide.html#openmp">OpenMP</a></li>
<li class="toctree-l4"><a class="reference internal" href="../ace_testbed/defiant_quick_start_guide.html#openmp-gpu-offload">OpenMP GPU Offload</a></li>
<li class="toctree-l4"><a class="reference internal" href="../ace_testbed/defiant_quick_start_guide.html#hip">HIP</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../ace_testbed/defiant_quick_start_guide.html#running-jobs">Running Jobs</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../ace_testbed/defiant_quick_start_guide.html#slurm-workload-manager">Slurm Workload Manager</a></li>
<li class="toctree-l4"><a class="reference internal" href="../ace_testbed/defiant_quick_start_guide.html#slurm-compute-node-partitions">Slurm Compute Node Partitions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../ace_testbed/defiant_quick_start_guide.html#process-and-thread-mapping">Process and Thread Mapping</a></li>
<li class="toctree-l4"><a class="reference internal" href="../ace_testbed/defiant_quick_start_guide.html#nvme-usage">NVMe Usage</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../ace_testbed/defiant_quick_start_guide.html#container-usage">Container Usage</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../ace_testbed/defiant_quick_start_guide.html#setup-before-building">Setup before Building</a></li>
<li class="toctree-l4"><a class="reference internal" href="../ace_testbed/defiant_quick_start_guide.html#build-and-run-workflow">Build and Run Workflow</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../ace_testbed/defiant_quick_start_guide.html#getting-help">Getting Help</a></li>
<li class="toctree-l3"><a class="reference internal" href="../ace_testbed/defiant_quick_start_guide.html#known-issues">Known Issues</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../contributing/index.html">Contributing to these docs</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../contributing/index.html#submitting-suggestions">Submitting suggestions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../contributing/index.html#authoring-content">Authoring content</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../contributing/index.html#setup-authoring-environment">Setup authoring environment</a></li>
<li class="toctree-l3"><a class="reference internal" href="../contributing/index.html#edit-the-docs">Edit the docs</a></li>
<li class="toctree-l3"><a class="reference internal" href="../contributing/index.html#resources">Resources</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../contributing/index.html#github-guidelines">GitHub Guidelines</a></li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu"  style="background: #efefef" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">OLCF User Documentation</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content style-external-links">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="index.html">Systems</a></li>
      <li class="breadcrumb-item active">Crusher Quick-Start Guide</li>
      <li class="wy-breadcrumbs-aside">
              <a href="https://github.com/olcf/olcf-user-docs/blob/master/systems/crusher_quick_start_guide.rst" class="fa fa-github"> Edit on GitHub</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="crusher-quick-start-guide">
<span id="id1"></span><h1>Crusher Quick-Start Guide<a class="headerlink" href="#crusher-quick-start-guide" title="Link to this heading"></a></h1>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p><strong>The Crusher Test and Development System will be decommissioned on April 12th, 2024.</strong> The
file systems that were available on Crusher are still accessible from the Home
server and the Data Transfer Nodes (DTNs), so all your data will remain accessible.
If you do not have access to other OLCF systems, your project will move to data-only
for 30-days. If you have any questions, please contact <a class="reference external" href="mailto:help&#37;&#52;&#48;olcf&#46;ornl&#46;gov" target="_blank">help<span>&#64;</span>olcf<span>&#46;</span>ornl<span>&#46;</span>gov</a>.</p>
</div>
<section id="system-overview">
<span id="crusher-system-overview"></span><h2>System Overview<a class="headerlink" href="#system-overview" title="Link to this heading"></a></h2>
<p>Crusher is an National Center for Computational Sciences (NCCS) moderate-security system that contains identical hardware and similar software as the upcoming Frontier system. It is used as an early-access testbed for Center for Accelerated Application Readiness (CAAR) and Exascale Computing Project (ECP) teams as well as NCCS staff and our vendor partners. The system has 2 cabinets, the first with 128 compute nodes and the second with 64 compute nodes, for a total of 192 compute nodes.</p>
<section id="crusher-compute-nodes">
<span id="id2"></span><h3>Crusher Compute Nodes<a class="headerlink" href="#crusher-compute-nodes" title="Link to this heading"></a></h3>
<p>Each Crusher compute node consists of [1x] 64-core AMD EPYC 7A53 Optimized 3rd Gen EPYC CPU (with 2 hardware threads per physical core) with access to 512 GB of DDR4 memory. Each node also contains [4x] AMD MI250X, each with 2 Graphics Compute Dies (GCDs) for a total of 8 GCDs per node. The programmer can think of the 8 GCDs as 8 separate GPUs, each having 64 GB of high-bandwidth memory (HBM2E). The CPU is connected to each GCD via Infinity Fabric CPU-GPU, allowing a peak host-to-device (H2D) and device-to-host (D2H) bandwidth of 36+36 GB/s. The 2 GCDs on the same MI250X are connected with Infinity Fabric GPU-GPU with a peak bandwidth of 200 GB/s. The GCDs on different MI250X are connected with Infinity Fabric GPU-GPU in the arrangement shown in the Crusher Node Diagram below, where the peak bandwidth ranges from 50-100 GB/s based on the number of Infinity Fabric connections between individual GCDs.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>TERMINOLOGY:</strong></p>
<p>The 8 GCDs contained in the 4 MI250X will show as 8 separate GPUs according to Slurm, <code class="docutils literal notranslate"><span class="pre">ROCR_VISIBLE_DEVICES</span></code>, and the ROCr runtime, so from this point forward in the quick-start guide, we will simply refer to the GCDs as GPUs.</p>
</div>
<a class="reference internal image-reference" href="../_images/Frontier_Node_Diagram.jpg"><img alt="Crusher node architecture diagram" class="align-center" src="../_images/Frontier_Node_Diagram.jpg" style="width: 100%;" /></a>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>There are [4x] NUMA domains per node and [2x] L3 cache regions per NUMA for a total of [8x] L3 cache regions. The 8 GPUs are each associated with one of the L3 regions as follows:</p>
<p>NUMA 0:</p>
<ul class="simple">
<li><p>hardware threads 000-007, 064-071 | GPU 4</p></li>
<li><p>hardware threads 008-015, 072-079 | GPU 5</p></li>
</ul>
<p>NUMA 1:</p>
<ul class="simple">
<li><p>hardware threads 016-023, 080-087 | GPU 2</p></li>
<li><p>hardware threads 024-031, 088-095 | GPU 3</p></li>
</ul>
<p>NUMA 2:</p>
<ul class="simple">
<li><p>hardware threads 032-039, 096-103 | GPU 6</p></li>
<li><p>hardware threads 040-047, 104-111 | GPU 7</p></li>
</ul>
<p>NUMA 3:</p>
<ul class="simple">
<li><p>hardware threads 048-055, 112-119 | GPU 0</p></li>
<li><p>hardware threads 056-063, 120-127 | GPU 1</p></li>
</ul>
</div>
</section>
<section id="system-interconnect">
<h3>System Interconnect<a class="headerlink" href="#system-interconnect" title="Link to this heading"></a></h3>
<p>The Crusher nodes are connected with [4x] HPE Slingshot 200 Gbps (25 GB/s) NICs providing a node-injection bandwidth of 800 Gbps (100 GB/s).</p>
</section>
<section id="file-systems">
<h3>File Systems<a class="headerlink" href="#file-systems" title="Link to this heading"></a></h3>
<p>Crusher is connected to the center-wide IBM Spectrum Scale filesystem providing 250 PB of storage capacity with a peak write speed of 2.5 TB/s. Crusher also has access to the center-wide NFS-based filesystem (which provides user and project home areas). While Crusher does not have <em>direct</em> access to the centers Nearline archival storage system (Kronos) - for user and project archival storage - users can log in to the moderate DTNs to move data to/from Kronos or use the OLCF Kronos Globus collection. For more information on using Kronos, see the <a class="reference internal" href="../data/index.html#kronos"><span class="std std-ref">Kronos Nearline Archival Storage System</span></a> section.</p>
</section>
<section id="gpus">
<h3>GPUs<a class="headerlink" href="#gpus" title="Link to this heading"></a></h3>
<p>Crusher contains a total of 768 AMD MI250X. The AMD MI250X has a peak performance of 53 TFLOPS in double-precision for modeling and simulation. Each MI250X contains 2 GPUs, where each GPU has a peak performance of 26.5 TFLOPS (double-precision), 110 compute units, and 64 GB of high-bandwidth memory (HBM2) which can be accessed at a peak of 1.6 TB/s. The 2 GPUs on an MI250X are connected with Infinity Fabric with a bandwidth of 200 GB/s (in both directions simultaneously).</p>
</section>
</section>
<hr class="docutils" />
<section id="connecting">
<h2>Connecting<a class="headerlink" href="#connecting" title="Link to this heading"></a></h2>
<p>To connect to Crusher, <code class="docutils literal notranslate"><span class="pre">ssh</span></code> to <code class="docutils literal notranslate"><span class="pre">crusher.olcf.ornl.gov</span></code>. For example:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>ssh<span class="w"> </span>&lt;username&gt;@crusher.olcf.ornl.gov
</pre></div>
</div>
<p>For more information on connecting to OLCF resources, see <a class="reference internal" href="../connecting/index.html#connecting-to-olcf"><span class="std std-ref">Connecting for the first time</span></a>.</p>
</section>
<hr class="docutils" />
<section id="data-and-storage">
<h2>Data and Storage<a class="headerlink" href="#data-and-storage" title="Link to this heading"></a></h2>
<p>For more detailed information about center-wide file systems and data archiving available on Crusher, please refer to the pages on <a class="reference internal" href="../data/index.html#data-storage-and-transfers"><span class="std std-ref">Data Storage and Transfers</span></a>, but the two subsections below give a quick overview of NFS and Lustre storage spaces.</p>
<section id="nfs-filesystem">
<h3>NFS Filesystem<a class="headerlink" href="#nfs-filesystem" title="Link to this heading"></a></h3>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Area</p></th>
<th class="head"><p>Path</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Permissions</p></th>
<th class="head"><p>Quota</p></th>
<th class="head"><p>Backups</p></th>
<th class="head"><p>Purged</p></th>
<th class="head"><p>Retention</p></th>
<th class="head"><p>On Compute Nodes</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>User Home</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">/ccs/home/[userid]</span></code></p></td>
<td><p>NFS</p></td>
<td><p>User set</p></td>
<td><p>50 GB</p></td>
<td><p>Yes</p></td>
<td><p>No</p></td>
<td><p>90 days</p></td>
<td><p>Read-only</p></td>
</tr>
<tr class="row-odd"><td><p>Project Home</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">/ccs/proj/[projid]</span></code></p></td>
<td><p>NFS</p></td>
<td><p>770</p></td>
<td><p>50 GB</p></td>
<td><p>Yes</p></td>
<td><p>No</p></td>
<td><p>90 days</p></td>
<td><p>Read-only</p></td>
</tr>
</tbody>
</table>
</section>
<section id="lustre-filesystem">
<h3>Lustre Filesystem<a class="headerlink" href="#lustre-filesystem" title="Link to this heading"></a></h3>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Area</p></th>
<th class="head"><p>Path</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Permissions</p></th>
<th class="head"><p>Quota</p></th>
<th class="head"><p>Backups</p></th>
<th class="head"><p>Purged</p></th>
<th class="head"><p>Retention</p></th>
<th class="head"><p>On Compute Nodes</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Member Work</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">/lustre/orion/[projid]/scratch/[userid]</span></code></p></td>
<td><p>Lustre HPE ClusterStor</p></td>
<td><p>700</p></td>
<td><p>50 TB</p></td>
<td><p>No</p></td>
<td><p>90 days</p></td>
<td><p>N/A</p></td>
<td><p>Yes</p></td>
</tr>
<tr class="row-odd"><td><p>Project Work</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">/lustre/orion/[projid]/proj-shared</span></code></p></td>
<td><p>Lustre HPE ClusterStor</p></td>
<td><p>770</p></td>
<td><p>50 TB</p></td>
<td><p>No</p></td>
<td><p>90 days</p></td>
<td><p>N/A</p></td>
<td><p>Yes</p></td>
</tr>
<tr class="row-even"><td><p>World Work</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">/lustre/orion/[projid]/world-shared</span></code></p></td>
<td><p>Lustre HPE ClusterStor</p></td>
<td><p>775</p></td>
<td><p>50 TB</p></td>
<td><p>No</p></td>
<td><p>90 days</p></td>
<td><p>N/A</p></td>
<td><p>Yes</p></td>
</tr>
</tbody>
</table>
</section>
</section>
<hr class="docutils" />
<section id="programming-environment">
<h2>Programming Environment<a class="headerlink" href="#programming-environment" title="Link to this heading"></a></h2>
<p>Crusher users are provided with many pre-installed software packages and scientific libraries. To facilitate this, environment management tools are used to handle necessary changes to the shell.</p>
<section id="environment-modules-lmod">
<h3>Environment Modules (Lmod)<a class="headerlink" href="#environment-modules-lmod" title="Link to this heading"></a></h3>
<p>Environment modules are provided through <a class="reference external" href="https://lmod.readthedocs.io/en/latest/" target="_blank">Lmod</a>, a Lua-based module system for dynamically altering shell environments. By managing changes to the shells environment variables (such as <code class="docutils literal notranslate"><span class="pre">PATH</span></code>, <code class="docutils literal notranslate"><span class="pre">LD_LIBRARY_PATH</span></code>, and <code class="docutils literal notranslate"><span class="pre">PKG_CONFIG_PATH</span></code>), Lmod allows you to alter the software available in your shell environment without the risk of creating package and version combinations that cannot coexist in a single environment.</p>
<section id="general-usage">
<h4>General Usage<a class="headerlink" href="#general-usage" title="Link to this heading"></a></h4>
<p>The interface to Lmod is provided by the <code class="docutils literal notranslate"><span class="pre">module</span></code> command:</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Command</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">module</span> <span class="pre">-t</span> <span class="pre">list</span></code></p></td>
<td><p>Shows a terse list of the currently loaded modules</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">module</span> <span class="pre">avail</span></code></p></td>
<td><p>Shows a table of the currently available modules</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">module</span> <span class="pre">help</span> <span class="pre">&lt;modulename&gt;</span></code></p></td>
<td><p>Shows help information about <code class="docutils literal notranslate"><span class="pre">&lt;modulename&gt;</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">module</span> <span class="pre">show</span> <span class="pre">&lt;modulename&gt;</span></code></p></td>
<td><p>Shows the environment changes made by the <code class="docutils literal notranslate"><span class="pre">&lt;modulename&gt;</span></code> modulefile</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">module</span> <span class="pre">spider</span> <span class="pre">&lt;string&gt;</span></code></p></td>
<td><p>Searches all possible modules according to <code class="docutils literal notranslate"><span class="pre">&lt;string&gt;</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">module</span> <span class="pre">load</span> <span class="pre">&lt;modulename&gt;</span> <span class="pre">[...]</span></code></p></td>
<td><p>Loads the given <code class="docutils literal notranslate"><span class="pre">&lt;modulename&gt;</span></code>(s) into the current environment</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">module</span> <span class="pre">use</span> <span class="pre">&lt;path&gt;</span></code></p></td>
<td><p>Adds <code class="docutils literal notranslate"><span class="pre">&lt;path&gt;</span></code> to the modulefile search cache and <code class="docutils literal notranslate"><span class="pre">MODULESPATH</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">module</span> <span class="pre">unuse</span> <span class="pre">&lt;path&gt;</span></code></p></td>
<td><p>Removes <code class="docutils literal notranslate"><span class="pre">&lt;path&gt;</span></code> from the modulefile search cache and <code class="docutils literal notranslate"><span class="pre">MODULESPATH</span></code></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">module</span> <span class="pre">purge</span></code></p></td>
<td><p>Unloads all modules</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">module</span> <span class="pre">reset</span></code></p></td>
<td><p>Resets loaded modules to system defaults</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">module</span> <span class="pre">update</span></code></p></td>
<td><p>Reloads all currently loaded modules</p></td>
</tr>
</tbody>
</table>
</section>
<section id="searching-for-modules">
<h4>Searching for Modules<a class="headerlink" href="#searching-for-modules" title="Link to this heading"></a></h4>
<p>Modules with dependencies are only available when the underlying dependencies, such as compiler families, are loaded. Thus, module avail will only display modules that are compatible with the current state of the environment. To search the entire hierarchy across all possible dependencies, the <code class="docutils literal notranslate"><span class="pre">spider</span></code> sub-command can be used as summarized in the following table.</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Command</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">module</span> <span class="pre">spider</span></code></p></td>
<td><p>Shows the entire possible graph of modules</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">module</span> <span class="pre">spider</span> <span class="pre">&lt;modulename&gt;</span></code></p></td>
<td><p>Searches for modules named <code class="docutils literal notranslate"><span class="pre">&lt;modulename&gt;</span></code> in the graph of possible modules</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">module</span> <span class="pre">spider</span> <span class="pre">&lt;modulename&gt;/&lt;version&gt;</span></code></p></td>
<td><p>Searches for a specific version of <code class="docutils literal notranslate"><span class="pre">&lt;modulename&gt;</span></code> in the graph of possible modules</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">module</span> <span class="pre">spider</span> <span class="pre">&lt;string&gt;</span></code></p></td>
<td><p>Searches for modulefiles containing <code class="docutils literal notranslate"><span class="pre">&lt;string&gt;</span></code></p></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="compilers">
<h3>Compilers<a class="headerlink" href="#compilers" title="Link to this heading"></a></h3>
<p>Cray, AMD, and GCC compilers are provided through modules on Crusher. The Cray and AMD compilers are both based on LLVM/Clang. There is also a system/OS versions of GCC available in <code class="docutils literal notranslate"><span class="pre">/usr/bin</span></code>. The table below lists details about each of the module-provided compilers.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>It is highly recommended to use the Cray compiler wrappers (<code class="docutils literal notranslate"><span class="pre">cc</span></code>, <code class="docutils literal notranslate"><span class="pre">CC</span></code>, and <code class="docutils literal notranslate"><span class="pre">ftn</span></code>) whenever possible. See the next section for more details.</p>
</div>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Vendor</p></th>
<th class="head"><p>Programming Environment</p></th>
<th class="head"><p>Compiler Module</p></th>
<th class="head"><p>Language</p></th>
<th class="head"><p>Compiler Wrapper</p></th>
<th class="head"><p>Compiler</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td rowspan="3"><p>Cray</p></td>
<td rowspan="3"><p><code class="docutils literal notranslate"><span class="pre">PrgEnv-cray</span></code></p></td>
<td rowspan="3"><p><code class="docutils literal notranslate"><span class="pre">cce</span></code></p></td>
<td><p>C</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">cc</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">craycc</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>C++</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">CC</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">craycxx</span></code> or <code class="docutils literal notranslate"><span class="pre">crayCC</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Fortran</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">ftn</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">crayftn</span></code></p></td>
</tr>
<tr class="row-odd"><td rowspan="3"><p>AMD</p></td>
<td rowspan="3"><p><code class="docutils literal notranslate"><span class="pre">PrgEnv-amd</span></code></p></td>
<td rowspan="3"><p><code class="docutils literal notranslate"><span class="pre">rocm</span></code></p></td>
<td><p>C</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">cc</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">amdclang</span></code></p></td>
</tr>
<tr class="row-even"><td><p>C++</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">CC</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">amdclang++</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Fortran</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">ftn</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">amdflang</span></code></p></td>
</tr>
<tr class="row-even"><td rowspan="3"><p>GCC</p></td>
<td rowspan="3"><p><code class="docutils literal notranslate"><span class="pre">PrgEnv-gnu</span></code></p></td>
<td rowspan="3"><p><code class="docutils literal notranslate"><span class="pre">gcc</span></code></p></td>
<td><p>C</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">cc</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">${GCC_PATH}/bin/gcc</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>C++</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">CC</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">${GCC_PATH}/bin/g++</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Fortran</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">ftn</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">${GCC_PATH}/bin/gfortran</span></code></p></td>
</tr>
</tbody>
</table>
<section id="cray-programming-environment-and-compiler-wrappers">
<h4>Cray Programming Environment and Compiler Wrappers<a class="headerlink" href="#cray-programming-environment-and-compiler-wrappers" title="Link to this heading"></a></h4>
<p>Cray provides <code class="docutils literal notranslate"><span class="pre">PrgEnv-&lt;compiler&gt;</span></code> modules (e.g., <code class="docutils literal notranslate"><span class="pre">PrgEnv-cray</span></code>) that load compatible components of a specific compiler toolchain. The components include the specified compiler as well as MPI, LibSci, and other libraries. Loading the <code class="docutils literal notranslate"><span class="pre">PrgEnv-&lt;compiler&gt;</span></code> modules also defines a set of compiler wrappers for that compiler toolchain that automatically add include paths and link in libraries for Cray software. Compiler wrappers are provided for C (<code class="docutils literal notranslate"><span class="pre">cc</span></code>), C++ (<code class="docutils literal notranslate"><span class="pre">CC</span></code>), and Fortran (<code class="docutils literal notranslate"><span class="pre">ftn</span></code>).</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Use the <code class="docutils literal notranslate"><span class="pre">-craype-verbose</span></code> flag to display the full include and link information used by the Cray compiler wrappers. This must be called on a file to see the full output (e.g., <code class="docutils literal notranslate"><span class="pre">CC</span> <span class="pre">-craype-verbose</span> <span class="pre">test.cpp</span></code>).</p>
</div>
</section>
</section>
<section id="mpi">
<h3>MPI<a class="headerlink" href="#mpi" title="Link to this heading"></a></h3>
<p>The MPI implementation available on Crusher is Crays MPICH, which is GPU-aware so GPU buffers can be passed directly to MPI calls.</p>
</section>
</section>
<hr class="docutils" />
<section id="compiling">
<h2>Compiling<a class="headerlink" href="#compiling" title="Link to this heading"></a></h2>
<p>This section covers how to compile for different programming models using the different compilers covered in the previous section.</p>
<section id="id3">
<h3>MPI<a class="headerlink" href="#id3" title="Link to this heading"></a></h3>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Implementation</p></th>
<th class="head"><p>Module</p></th>
<th class="head"><p>Compiler</p></th>
<th class="head"><p>Header Files &amp; Linking</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td rowspan="2"><p>Cray MPICH</p></td>
<td rowspan="2"><p><code class="docutils literal notranslate"><span class="pre">cray-mpich</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">cc</span></code>, <code class="docutils literal notranslate"><span class="pre">CC</span></code>, <code class="docutils literal notranslate"><span class="pre">ftn</span></code> (Cray compiler wrappers)</p></td>
<td><p>MPI header files and linking is built into the Cray compiler wrappers</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">hipcc</span></code></p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">-L${MPICH_DIR}/lib</span> <span class="pre">-lmpi</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">${CRAY_XPMEM_POST_LINK_OPTS}</span> <span class="pre">-lxpmem</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">${PE_MPICH_GTL_DIR_amd_gfx90a}</span> <span class="pre">${PE_MPICH_GTL_LIBS_amd_gfx90a}</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">-I${MPICH_DIR}/include</span></code></div>
</div>
</td>
</tr>
</tbody>
</table>
<section id="gpu-aware-mpi">
<h4>GPU-Aware MPI<a class="headerlink" href="#gpu-aware-mpi" title="Link to this heading"></a></h4>
<p>To use GPU-aware Cray MPICH, users must set the following modules and environment variables:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>module<span class="w"> </span>load<span class="w"> </span>craype-accel-amd-gfx90a
module<span class="w"> </span>load<span class="w"> </span>rocm

<span class="nb">export</span><span class="w"> </span><span class="nv">MPICH_GPU_SUPPORT_ENABLED</span><span class="o">=</span><span class="m">1</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>There are extra steps needed to enable GPU-aware MPI on Crusher, which depend on the compiler that is used (see 1. and 2. below).</p>
</div>
<section id="compiling-with-the-cray-compiler-wrappers-cc-or-cc">
<h5>1. Compiling with the Cray compiler wrappers, <code class="docutils literal notranslate"><span class="pre">cc</span></code> or <code class="docutils literal notranslate"><span class="pre">CC</span></code><a class="headerlink" href="#compiling-with-the-cray-compiler-wrappers-cc-or-cc" title="Link to this heading"></a></h5>
<p>To use GPU-aware Cray MPICH with the Cray compiler wrappers, the following environment variables must be set before compiling. These variables are automatically set by the <code class="docutils literal notranslate"><span class="pre">cray-mpich</span></code> modulefile:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1">## These must be set before compiling so the executable picks up GTL</span>
<span class="nv">PE_MPICH_GTL_DIR_amd_gfx90a</span><span class="o">=</span><span class="s2">&quot;-L</span><span class="si">${</span><span class="nv">CRAY_MPICH_ROOTDIR</span><span class="si">}</span><span class="s2">/gtl/lib&quot;</span>
<span class="nv">PE_MPICH_GTL_LIBS_amd_gfx90a</span><span class="o">=</span><span class="s2">&quot;-lmpi_gtl_hsa&quot;</span>
</pre></div>
</div>
<p>In addition, the following header files and libraries must be included:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>-I<span class="si">${</span><span class="nv">ROCM_PATH</span><span class="si">}</span>/include
-L<span class="si">${</span><span class="nv">ROCM_PATH</span><span class="si">}</span>/lib<span class="w"> </span>-lamdhip64
</pre></div>
</div>
<p>where the include path implies that <code class="docutils literal notranslate"><span class="pre">#include</span> <span class="pre">&lt;hip/hip_runtime.h&gt;</span></code> is included in the source file.</p>
</section>
<section id="compiling-with-hipcc">
<h5>2. Compiling with <code class="docutils literal notranslate"><span class="pre">hipcc</span></code><a class="headerlink" href="#compiling-with-hipcc" title="Link to this heading"></a></h5>
<p>To use GPU-aware Cray MPICH with <code class="docutils literal notranslate"><span class="pre">hipcc</span></code>, users must include appropriate headers, libraries, and flags:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>-I<span class="si">${</span><span class="nv">MPICH_DIR</span><span class="si">}</span>/include
-L<span class="si">${</span><span class="nv">MPICH_DIR</span><span class="si">}</span>/lib<span class="w"> </span>-lmpi<span class="w"> </span><span class="se">\</span>
<span class="w">  </span><span class="si">${</span><span class="nv">CRAY_XPMEM_POST_LINK_OPTS</span><span class="si">}</span><span class="w"> </span>-lxpmem<span class="w"> </span><span class="se">\</span>
<span class="w">  </span><span class="si">${</span><span class="nv">PE_MPICH_GTL_DIR_amd_gfx90a</span><span class="si">}</span><span class="w"> </span><span class="si">${</span><span class="nv">PE_MPICH_GTL_LIBS_amd_gfx90a</span><span class="si">}</span>

<span class="nv">HIPFLAGS</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>--amdgpu-target<span class="o">=</span>gfx90a
</pre></div>
</div>
</section>
<section id="determining-the-compatibility-of-cray-mpich-and-rocm">
<h5>Determining the Compatibility of Cray MPICH and ROCm<a class="headerlink" href="#determining-the-compatibility-of-cray-mpich-and-rocm" title="Link to this heading"></a></h5>
<p>Releases of <code class="docutils literal notranslate"><span class="pre">cray-mpich</span></code> are each built with a specific version of ROCm, and compatibility across multiple versions is not guaranteed. OLCF will maintain compatible default modules when possible. If using non-default modules, you can determine compatibility by reviewing the <em>Product and OS Dependencies</em> section in the <code class="docutils literal notranslate"><span class="pre">cray-mpich</span></code> release notes. This can be displayed by running <code class="docutils literal notranslate"><span class="pre">module</span> <span class="pre">show</span> <span class="pre">cray-mpich/&lt;version&gt;</span></code>. If the notes indicate compatibility with <em>AMD ROCM X.Y or later</em>, only use <code class="docutils literal notranslate"><span class="pre">rocm/X.Y.Z</span></code> modules. If using a non-default version of <code class="docutils literal notranslate"><span class="pre">cray-mpich</span></code>, you must add <code class="docutils literal notranslate"><span class="pre">${CRAY_MPICH_ROOTDIR}/gtl/lib</span></code> to either your <code class="docutils literal notranslate"><span class="pre">LD_LIBRARY_PATH</span></code> at run time or your executables rpath at build time.</p>
<p>The compatibility table below was determined by linker testing with all current combinations of <code class="docutils literal notranslate"><span class="pre">cray-mpich</span></code> and <code class="docutils literal notranslate"><span class="pre">rocm</span></code> modules on Crusher.</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>cray-mpich</p></th>
<th class="head"><p>ROCm</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>8.1.12</p></td>
<td><p>4.5.2, 4.5.0</p></td>
</tr>
<tr class="row-odd"><td><p>8.1.14</p></td>
<td><p>4.5.2, 4.5.0</p></td>
</tr>
<tr class="row-even"><td><p>8.1.15</p></td>
<td><p>5.4.0, 5.3.0, 5.2.0, 5.1.0, 5.0.2, 5.0.0</p></td>
</tr>
<tr class="row-odd"><td><p>8.1.16</p></td>
<td><p>5.4.0, 5.3.0, 5.2.0, 5.1.0, 5.0.2, 5.0.0</p></td>
</tr>
<tr class="row-even"><td><p>8.1.17</p></td>
<td><p>5.4.0, 5.3.0, 5.2.0, 5.1.0, 5.0.2, 5.0.0</p></td>
</tr>
<tr class="row-odd"><td><p>8.1.18</p></td>
<td><p>5.4.0, 5.3.0, 5.2.0, 5.1.0, 5.0.2, 5.0.0</p></td>
</tr>
<tr class="row-even"><td><p>8.1.19</p></td>
<td><p>5.4.0, 5.3.0, 5.2.0, 5.1.0, 5.0.2, 5.0.0</p></td>
</tr>
<tr class="row-odd"><td><p>8.1.21</p></td>
<td><p>5.4.0, 5.3.0, 5.2.0, 5.1.0, 5.0.2, 5.0.0</p></td>
</tr>
<tr class="row-even"><td><p>8.1.23</p></td>
<td><p>5.4.0, 5.3.0, 5.2.0, 5.1.0, 5.0.2, 5.0.0</p></td>
</tr>
</tbody>
</table>
</section>
</section>
</section>
<section id="openmp">
<h3>OpenMP<a class="headerlink" href="#openmp" title="Link to this heading"></a></h3>
<p>This section shows how to compile with OpenMP using the different compilers covered above.</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Vendor</p></th>
<th class="head"><p>Module</p></th>
<th class="head"><p>Language</p></th>
<th class="head"><p>Compiler</p></th>
<th class="head"><p>OpenMP flag (CPU thread)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td rowspan="2"><p>Cray</p></td>
<td rowspan="2"><p><code class="docutils literal notranslate"><span class="pre">cce</span></code></p></td>
<td><p>C, C++</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">cc</span></code> (wraps <code class="docutils literal notranslate"><span class="pre">craycc</span></code>)</div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">CC</span></code> (wraps <code class="docutils literal notranslate"><span class="pre">crayCC</span></code>)</div>
</div>
</td>
<td><p><code class="docutils literal notranslate"><span class="pre">-fopenmp</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Fortran</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">ftn</span></code> (wraps <code class="docutils literal notranslate"><span class="pre">crayftn</span></code>)</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">-homp</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">-fopenmp</span></code> (alias)</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>AMD</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">rocm</span></code></p></td>
<td><div class="line-block">
<div class="line">C</div>
<div class="line">C++</div>
<div class="line">Fortran</div>
</div>
</td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">cc</span></code> (wraps <code class="docutils literal notranslate"><span class="pre">amdclang</span></code>)</div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">CC</span></code> (wraps <code class="docutils literal notranslate"><span class="pre">amdclang++</span></code>)</div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">ftn</span></code> (wraps <code class="docutils literal notranslate"><span class="pre">amdflang</span></code>)</div>
</div>
</td>
<td><p><code class="docutils literal notranslate"><span class="pre">-fopenmp</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>GCC</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">gcc</span></code></p></td>
<td><div class="line-block">
<div class="line">C</div>
<div class="line">C++</div>
<div class="line">Fortran</div>
</div>
</td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">cc</span></code> (wraps <code class="docutils literal notranslate"><span class="pre">$GCC_PATH/bin/gcc</span></code>)</div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">CC</span></code> (wraps <code class="docutils literal notranslate"><span class="pre">$GCC_PATH/bin/g++</span></code>)</div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">ftn</span></code> (wraps <code class="docutils literal notranslate"><span class="pre">$GCC_PATH/bin/gfortran</span></code>)</div>
</div>
</td>
<td><p><code class="docutils literal notranslate"><span class="pre">-fopenmp</span></code></p></td>
</tr>
</tbody>
</table>
</section>
<section id="openmp-gpu-offload">
<h3>OpenMP GPU Offload<a class="headerlink" href="#openmp-gpu-offload" title="Link to this heading"></a></h3>
<p>This section shows how to compile with OpenMP Offload using the different compilers covered above.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Make sure the <code class="docutils literal notranslate"><span class="pre">craype-accel-amd-gfx90a</span></code> module is loaded when using OpenMP offload.</p>
</div>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Vendor</p></th>
<th class="head"><p>Module</p></th>
<th class="head"><p>Language</p></th>
<th class="head"><p>Compiler</p></th>
<th class="head"><p>OpenMP flag (GPU)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td rowspan="2"><p>Cray</p></td>
<td rowspan="2"><p><code class="docutils literal notranslate"><span class="pre">cce</span></code></p></td>
<td><p>C
C++</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">cc</span></code> (wraps <code class="docutils literal notranslate"><span class="pre">craycc</span></code>)</div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">CC</span></code> (wraps <code class="docutils literal notranslate"><span class="pre">crayCC</span></code>)</div>
</div>
</td>
<td><p><code class="docutils literal notranslate"><span class="pre">-fopenmp</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Fortran</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">ftn</span></code> (wraps <code class="docutils literal notranslate"><span class="pre">crayftn</span></code>)</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">-homp</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">-fopenmp</span></code> (alias)</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>AMD</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">rocm</span></code></p></td>
<td><div class="line-block">
<div class="line">C</div>
<div class="line">C++</div>
<div class="line">Fortran</div>
</div>
</td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">cc</span></code> (wraps <code class="docutils literal notranslate"><span class="pre">amdclang</span></code>)</div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">CC</span></code> (wraps <code class="docutils literal notranslate"><span class="pre">amdclang++</span></code>)</div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">ftn</span></code> (wraps <code class="docutils literal notranslate"><span class="pre">amdflang</span></code>)</div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">hipcc</span></code> (requires flags below)</div>
</div>
</td>
<td><p><code class="docutils literal notranslate"><span class="pre">-fopenmp</span></code></p></td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If invoking <code class="docutils literal notranslate"><span class="pre">amdclang</span></code>, <code class="docutils literal notranslate"><span class="pre">amdclang++</span></code>, or <code class="docutils literal notranslate"><span class="pre">amdflang</span></code> directly, or using <code class="docutils literal notranslate"><span class="pre">hipcc</span></code> you will need to add:
<code class="docutils literal notranslate"><span class="pre">-fopenmp</span> <span class="pre">-target</span> <span class="pre">x86_64-pc-linux-gnu</span> <span class="pre">-fopenmp-targets=amdgcn-amd-amdhsa</span> <span class="pre">-Xopenmp-target=amdgcn-amd-amdhsa</span> <span class="pre">-march=gfx90a</span></code>.</p>
</div>
</section>
<section id="hip">
<h3>HIP<a class="headerlink" href="#hip" title="Link to this heading"></a></h3>
<p>This section shows how to compile HIP codes using the Cray compiler wrappers and <code class="docutils literal notranslate"><span class="pre">hipcc</span></code> compiler driver.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Make sure the <code class="docutils literal notranslate"><span class="pre">craype-accel-amd-gfx90a</span></code> module is loaded when compiling HIP with the Cray compiler wrappers.</p>
</div>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Compiler</p></th>
<th class="head"><p>Compile/Link Flags, Header Files, and Libraries</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">CC</span></code></div>
<div class="line">Only with</div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">PrgEnv-cray</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">PrgEnv-amd</span></code></div>
</div>
</td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">CFLAGS</span> <span class="pre">=</span> <span class="pre">-std=c++11</span> <span class="pre">-D__HIP_ROCclr__</span> <span class="pre">-D__HIP_ARCH_GFX90A__=1</span> <span class="pre">--rocm-path=${ROCM_PATH}</span> <span class="pre">--offload-arch=gfx90a</span> <span class="pre">-x</span> <span class="pre">hip</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">LFLAGS</span> <span class="pre">=</span> <span class="pre">--rocm-path=${ROCM_PATH}</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">-L${ROCM_PATH}/lib</span> <span class="pre">-lamdhip64</span></code></div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">hipcc</span></code></p></td>
<td><div class="line-block">
<div class="line">Can be used directly to compile HIP source files.</div>
<div class="line">To see what is being invoked within this compiler driver, issue the command, <code class="docutils literal notranslate"><span class="pre">hipcc</span> <span class="pre">--verbose</span></code></div>
<div class="line">To explicitly target AMD MI250X, use <code class="docutils literal notranslate"><span class="pre">--amdgpu-target=gfx90a</span></code></div>
</div>
</td>
</tr>
</tbody>
</table>
</section>
<section id="hip-openmp-cpu-threading">
<h3>HIP + OpenMP CPU Threading<a class="headerlink" href="#hip-openmp-cpu-threading" title="Link to this heading"></a></h3>
<p>This section describes how to compile HIP + OpenMP CPU threading hybrid codes.
For all compiler toolchains, HIP kernels and kernel launch calls (ie <code class="docutils literal notranslate"><span class="pre">hipLaunchKernelGGL</span></code>) cannot be implemented in the same file that requires <code class="docutils literal notranslate"><span class="pre">-fopenmp</span></code>.
HIP API calls (<code class="docutils literal notranslate"><span class="pre">hipMalloc</span></code>, <code class="docutils literal notranslate"><span class="pre">hipMemcpy</span></code>) are allowed in files that require <code class="docutils literal notranslate"><span class="pre">-fopenmp</span></code>.
HIP source files should be compiled into object files using the instructions in the <code class="docutils literal notranslate"><span class="pre">HIP</span></code> section, with the <code class="docutils literal notranslate"><span class="pre">-c</span></code> flag added to generate an object file.
OpenMP and other non-HIP source files should be compiled into object files using the instructions in the <code class="docutils literal notranslate"><span class="pre">OpenMP</span></code> section.
Then these object files should be linked using the following link flags: <code class="docutils literal notranslate"><span class="pre">-fopenmp</span> <span class="pre">-L${ROCM_PATH}/lib</span> <span class="pre">-lamdhip64</span></code>.</p>
</section>
<section id="sycl">
<h3>SYCL<a class="headerlink" href="#sycl" title="Link to this heading"></a></h3>
<p>This section shows how to compile SYCL codes using the DPC++ compiler.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Make sure the <code class="docutils literal notranslate"><span class="pre">ums</span> <span class="pre">ums015</span> <span class="pre">dpcpp</span></code> module is loaded when compiling SYCL with <code class="docutils literal notranslate"><span class="pre">clang</span></code> or <code class="docutils literal notranslate"><span class="pre">clang++</span></code>.</p>
</div>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Compiler</p></th>
<th class="head"><p>Compile/Link Flags, Header Files, and Libraries</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">clang</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">clang++</span></code></div>
</div>
</td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">CFLAGS</span> <span class="pre">=</span> <span class="pre">-fsycl</span> <span class="pre">-fsycl-targets=amdgcn-amd-amdhsa</span> <span class="pre">-Xsycl-target-backend</span> <span class="pre">--offload-arch=gfx90a</span></code></div>
<div class="line"><br /></div>
</div>
</td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>These compilers are built weekly from the latest open-source rather than releases. As such, these compilers will get new features and updates quickly but may break on occasion. If you experience regressions, please load an older version of the module rather than the latest.</p>
</div>
</section>
</section>
<hr class="docutils" />
<section id="running-jobs">
<h2>Running Jobs<a class="headerlink" href="#running-jobs" title="Link to this heading"></a></h2>
<p>This section describes how to run programs on the Crusher compute nodes, including a brief overview of Slurm and also how to map processes and threads to CPU cores and GPUs.</p>
<section id="slurm-workload-manager">
<h3>Slurm Workload Manager<a class="headerlink" href="#slurm-workload-manager" title="Link to this heading"></a></h3>
<p><a class="reference external" href="https://slurm.schedmd.com/" target="_blank">Slurm</a> is the workload manager used to interact with the compute nodes on Crusher. In the following subsections, the most commonly used Slurm commands for submitting, running, and monitoring jobs will be covered, but users are encouraged to visit the official documentation and man pages for more information.</p>
<section id="batch-scheduler-and-job-launcher">
<h4>Batch Scheduler and Job Launcher<a class="headerlink" href="#batch-scheduler-and-job-launcher" title="Link to this heading"></a></h4>
<p>Slurm provides 3 ways of submitting and launching jobs on Crushers compute nodes: batch scripts, interactive, and single-command. The Slurm commands associated with these methods are shown in the table below and examples of their use can be found in the related subsections. Please note that regardless of the submission method used, the job will launch on compute nodes, with the first compute in the allocation serving as head-node.</p>
<table class="docutils align-default">
<tbody>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">sbatch</span></code></p></td>
<td><div class="line-block">
<div class="line">Used to submit a batch script to allocate a Slurm job allocation. The script contains options preceded with <code class="docutils literal notranslate"><span class="pre">#SBATCH</span></code>.</div>
<div class="line">(see Batch Scripts section below)</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">salloc</span></code></p></td>
<td><div class="line-block">
<div class="line">Used to allocate an interactive Slurm job allocation, where one or more job steps (i.e., <code class="docutils literal notranslate"><span class="pre">srun</span></code> commands) can then be launched on the allocated resources (i.e., nodes).</div>
<div class="line">(see Interactive Jobs section below)</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">srun</span></code></p></td>
<td><div class="line-block">
<div class="line">Used to run a parallel job (job step) on the resources allocated with sbatch or <code class="docutils literal notranslate"><span class="pre">salloc</span></code>.</div>
<div class="line">If necessary, srun will first create a resource allocation in which to run the parallel job(s).</div>
<div class="line">(see Single Command section below)</div>
</div>
</td>
</tr>
</tbody>
</table>
<section id="batch-scripts">
<h5>Batch Scripts<a class="headerlink" href="#batch-scripts" title="Link to this heading"></a></h5>
<p>A batch script can be used to submit a job to run on the compute nodes at a later time. In this case, stdout and stderr will be written to a file(s) that can be opened after the job completes. Here is an example of a simple batch script:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="linenos">1</span><span class="ch">#!/bin/bash</span>
<span class="linenos">2</span><span class="c1">#SBATCH -A &lt;project_id&gt;</span>
<span class="linenos">3</span><span class="c1">#SBATCH -J &lt;job_name&gt;</span>
<span class="linenos">4</span><span class="c1">#SBATCH -o %x-%j.out</span>
<span class="linenos">5</span><span class="c1">#SBATCH -t 00:05:00</span>
<span class="linenos">6</span><span class="c1">#SBATCH -p &lt;partition&gt;</span>
<span class="linenos">7</span><span class="c1">#SBATCH -N 2</span>
<span class="linenos">8</span>
<span class="linenos">9</span>srun<span class="w"> </span>-n4<span class="w"> </span>--ntasks-per-node<span class="o">=</span><span class="m">2</span><span class="w"> </span>./a.out
</pre></div>
</div>
<p>The Slurm submission options are preceded by <code class="docutils literal notranslate"><span class="pre">#SBATCH</span></code>, making them appear as comments to a shell (since comments begin with <code class="docutils literal notranslate"><span class="pre">#</span></code>). Slurm will look for submission options from the first line through the first non-comment line. Options encountered after the first non-comment line will not be read by Slurm. In the example script, the lines are:</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Line</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1</p></td>
<td><p>[Optional] shell interpreter line</p></td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td><p>OLCF project to charge</p></td>
</tr>
<tr class="row-even"><td><p>3</p></td>
<td><p>Job name</p></td>
</tr>
<tr class="row-odd"><td><p>4</p></td>
<td><p>stdout file name ( <code class="docutils literal notranslate"><span class="pre">%x</span></code> represents job name, <code class="docutils literal notranslate"><span class="pre">%j</span></code> represents job id)</p></td>
</tr>
<tr class="row-even"><td><p>5</p></td>
<td><p>Walltime requested (<code class="docutils literal notranslate"><span class="pre">HH:MM:SS</span></code>)</p></td>
</tr>
<tr class="row-odd"><td><p>6</p></td>
<td><p>Batch queue</p></td>
</tr>
<tr class="row-even"><td><p>7</p></td>
<td><p>Number of compute nodes requested</p></td>
</tr>
<tr class="row-odd"><td><p>8</p></td>
<td><p>Blank line</p></td>
</tr>
<tr class="row-even"><td><p>9</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">srun</span></code> command to launch parallel job (requesting 4 processes - 2 per node)</p></td>
</tr>
</tbody>
</table>
</section>
<section id="interactive-jobs">
<h5>Interactive Jobs<a class="headerlink" href="#interactive-jobs" title="Link to this heading"></a></h5>
<p>To request an interactive job where multiple job steps (i.e., multiple <code class="docutils literal notranslate"><span class="pre">srun</span></code> commands) can be launched on the allocated compute node(s), the <code class="docutils literal notranslate"><span class="pre">salloc</span></code> command can be used:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>salloc<span class="w"> </span>-A<span class="w"> </span>&lt;project_id&gt;<span class="w"> </span>-J<span class="w"> </span>&lt;job_name&gt;<span class="w"> </span>-t<span class="w"> </span><span class="m">00</span>:05:00<span class="w"> </span>-p<span class="w"> </span>&lt;partition&gt;<span class="w"> </span>-N<span class="w"> </span><span class="m">2</span>
salloc:<span class="w"> </span>Granted<span class="w"> </span>job<span class="w"> </span>allocation<span class="w"> </span><span class="m">4258</span>
salloc:<span class="w"> </span>Waiting<span class="w"> </span><span class="k">for</span><span class="w"> </span>resource<span class="w"> </span>configuration
salloc:<span class="w"> </span>Nodes<span class="w"> </span>crusher<span class="o">[</span><span class="m">010</span>-011<span class="o">]</span><span class="w"> </span>are<span class="w"> </span>ready<span class="w"> </span><span class="k">for</span><span class="w"> </span>job

$<span class="w"> </span>srun<span class="w"> </span>-n<span class="w"> </span><span class="m">4</span><span class="w"> </span>--ntasks-per-node<span class="o">=</span><span class="m">2</span><span class="w"> </span>./a.out
&lt;output<span class="w"> </span>printed<span class="w"> </span>to<span class="w"> </span>terminal&gt;

$<span class="w"> </span>srun<span class="w"> </span>-n<span class="w"> </span><span class="m">2</span><span class="w"> </span>--ntasks-per-node<span class="o">=</span><span class="m">1</span><span class="w"> </span>./a.out
&lt;output<span class="w"> </span>printed<span class="w"> </span>to<span class="w"> </span>terminal&gt;
</pre></div>
</div>
<p>Here, <code class="docutils literal notranslate"><span class="pre">salloc</span></code> is used to request an allocation of 2 compute nodes for 5 minutes. Once the resources become available, the user is granted access to the compute nodes (<code class="docutils literal notranslate"><span class="pre">crusher010</span></code> and <code class="docutils literal notranslate"><span class="pre">crusher011</span></code> in this case) and can launch job steps on them using <code class="docutils literal notranslate"><span class="pre">srun</span></code>.</p>
</section>
<section id="single-command-non-interactive">
<span id="single-command-crusher"></span><h5>Single Command (non-interactive)<a class="headerlink" href="#single-command-non-interactive" title="Link to this heading"></a></h5>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>srun<span class="w"> </span>-A<span class="w"> </span>&lt;project_id&gt;<span class="w"> </span>-t<span class="w"> </span><span class="m">00</span>:05:00<span class="w"> </span>-p<span class="w"> </span>&lt;partition&gt;<span class="w"> </span>-N<span class="w"> </span><span class="m">2</span><span class="w"> </span>-n<span class="w"> </span><span class="m">4</span><span class="w"> </span>--ntasks-per-node<span class="o">=</span><span class="m">2</span><span class="w"> </span>./a.out
&lt;output<span class="w"> </span>printed<span class="w"> </span>to<span class="w"> </span>terminal&gt;
</pre></div>
</div>
<p>The job name and output options have been removed since stdout/stderr are typically desired in the terminal window in this usage mode.</p>
</section>
</section>
<section id="common-slurm-submission-options">
<h4>Common Slurm Submission Options<a class="headerlink" href="#common-slurm-submission-options" title="Link to this heading"></a></h4>
<p>The table below summarizes commonly-used Slurm job submission options:</p>
<table class="docutils align-default">
<tbody>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">-A</span> <span class="pre">&lt;project_id&gt;</span></code></p></td>
<td><p>Project ID to charge</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">-J</span> <span class="pre">&lt;job_name&gt;</span></code></p></td>
<td><p>Name of job</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">-p</span> <span class="pre">&lt;partition&gt;</span></code></p></td>
<td><p>Partition / batch queue</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">-t</span> <span class="pre">&lt;time&gt;</span></code></p></td>
<td><p>Wall clock time &lt;<code class="docutils literal notranslate"><span class="pre">HH:MM:SS</span></code>&gt;</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">-N</span> <span class="pre">&lt;number_of_nodes&gt;</span></code></p></td>
<td><p>Number of compute nodes</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">-o</span> <span class="pre">&lt;file_name&gt;</span></code></p></td>
<td><p>Standard output file name</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">-e</span> <span class="pre">&lt;file_name&gt;</span></code></p></td>
<td><p>Standard error file name</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--threads-per-core=&lt;threads&gt;</span></code></p></td>
<td><p>Number of active hardware threads per core [1 (default) or 2]</p></td>
</tr>
</tbody>
</table>
<p>For more information about these and/or other options, please see the <code class="docutils literal notranslate"><span class="pre">sbatch</span></code> man page.</p>
</section>
<section id="other-common-slurm-commands">
<h4>Other Common Slurm Commands<a class="headerlink" href="#other-common-slurm-commands" title="Link to this heading"></a></h4>
<p>The table below summarizes commonly-used Slurm commands:</p>
<table class="docutils align-default">
<tbody>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">sinfo</span></code></p></td>
<td><div class="line-block">
<div class="line">Used to view partition and node information.</div>
<div class="line">E.g., to view user-defined details about the batch queue:</div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">sinfo</span> <span class="pre">-p</span> <span class="pre">batch</span> <span class="pre">-o</span> <span class="pre">&quot;%15N</span> <span class="pre">%10D</span> <span class="pre">%10P</span> <span class="pre">%10a</span> <span class="pre">%10c</span> <span class="pre">%10z&quot;</span></code></div>
</div>
</td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">squeue</span></code></p></td>
<td><div class="line-block">
<div class="line">Used to view job and job step information for jobs in the scheduling queue.</div>
<div class="line">E.g., to see all jobs from a specific user:</div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">squeue</span> <span class="pre">-l</span> <span class="pre">-u</span> <span class="pre">&lt;user_id&gt;</span></code></div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">sacct</span></code></p></td>
<td><div class="line-block">
<div class="line">Used to view accounting data for jobs and job steps in the job accounting log (currently in the queue or recently completed).</div>
<div class="line">E.g., to see a list of specified information about all jobs submitted/run by a users since 1 PM on January 4, 2021:</div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">sacct</span> <span class="pre">-u</span> <span class="pre">&lt;username&gt;</span> <span class="pre">-S</span> <span class="pre">2021-01-04T13:00:00</span> <span class="pre">-o</span> <span class="pre">&quot;jobid%5,jobname%25,user%15,nodelist%20&quot;</span> <span class="pre">-X</span></code></div>
</div>
</td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">scancel</span></code></p></td>
<td><div class="line-block">
<div class="line">Used to signal or cancel jobs or job steps.</div>
<div class="line">E.g., to cancel a job:</div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">scancel</span> <span class="pre">&lt;jobid&gt;</span></code></div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">scontrol</span></code></p></td>
<td><div class="line-block">
<div class="line">Used to view or modify job configuration.</div>
<div class="line">E.g., to place a job on hold:</div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">scontrol</span> <span class="pre">hold</span> <span class="pre">&lt;jobid&gt;</span></code></div>
</div>
</td>
</tr>
</tbody>
</table>
</section>
</section>
<hr class="docutils" />
<section id="slurm-compute-node-partitions">
<h3>Slurm Compute Node Partitions<a class="headerlink" href="#slurm-compute-node-partitions" title="Link to this heading"></a></h3>
<p>Crushers compute nodes are contained within a single Slurm partition (queue) for both CAAR and ECP projects. Please see the table below for details.</p>
<section id="partition">
<h4>Partition<a class="headerlink" href="#partition" title="Link to this heading"></a></h4>
<p>The CAAR and ECP batch partition consists of 192 total compute nodes. On a per-project basis, each user can have 2 running and 2 eligible jobs at a time, with up to 20 jobs submitted.</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Number of Nodes</p></th>
<th class="head"><p>Max Walltime</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1 - 8</p></td>
<td><p>8 hours</p></td>
</tr>
<tr class="row-odd"><td><p>9 - 64</p></td>
<td><p>4 hours</p></td>
</tr>
<tr class="row-even"><td><p>65 - 160</p></td>
<td><p>2 hours</p></td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If CAAR or ECP teams require a temporary exception to this policy, please email <a class="reference external" href="mailto:help&#37;&#52;&#48;olcf&#46;ornl&#46;gov" target="_blank">help<span>&#64;</span>olcf<span>&#46;</span>ornl<span>&#46;</span>gov</a> with your request and it will be given to the OLCF Resource Utilization Council (RUC) for review.</p>
</div>
</section>
</section>
<section id="process-and-thread-mapping">
<h3>Process and Thread Mapping<a class="headerlink" href="#process-and-thread-mapping" title="Link to this heading"></a></h3>
<p>This section describes how to map processes (e.g., MPI ranks) and process threads (e.g., OpenMP threads) to the CPUs and GPUs on Crusher. The <a class="reference internal" href="#crusher-compute-nodes"><span class="std std-ref">Crusher Compute Nodes</span></a> diagram will be helpful when reading this section to understand which physical CPU cores (and hardware threads) your processes and threads run on.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Users are highly encouraged to use the CPU- and GPU-mapping programs used in the following sections to check their understanding of the job steps (i.e., <code class="docutils literal notranslate"><span class="pre">srun</span></code> commands) the intend to use in their actual jobs.</p>
</div>
<section id="cpu-mapping">
<h4>CPU Mapping<a class="headerlink" href="#cpu-mapping" title="Link to this heading"></a></h4>
<p>In this sub-section, a simple MPI+OpenMP Hello, World program (<a class="reference external" href="https://code.ornl.gov/olcf/hello_mpi_omp" target="_blank">hello_mpi_omp</a>) will be used to clarify the mappings. Slurms <a class="reference internal" href="../ace_testbed/defiant_quick_start_guide.html#interactive"><span class="std std-ref">Interactive Jobs</span></a> method was used to request an allocation of 1 compute node for these examples: <code class="docutils literal notranslate"><span class="pre">salloc</span> <span class="pre">-A</span> <span class="pre">&lt;project_id&gt;</span> <span class="pre">-t</span> <span class="pre">30</span> <span class="pre">-p</span> <span class="pre">&lt;parition&gt;</span> <span class="pre">-N</span> <span class="pre">1</span></code></p>
<p>The <code class="docutils literal notranslate"><span class="pre">srun</span></code> options used in this section are (see <code class="docutils literal notranslate"><span class="pre">man</span> <span class="pre">srun</span></code> for more information):</p>
<table class="docutils align-default">
<tbody>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">-c,</span> <span class="pre">--cpus-per-task=&lt;ncpus&gt;</span></code></p></td>
<td><div class="line-block">
<div class="line">Request that <code class="docutils literal notranslate"><span class="pre">ncpus</span></code> be allocated per process (default is 1).</div>
<div class="line">(<code class="docutils literal notranslate"><span class="pre">ncpus</span></code> refers to hardware threads)</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--threads-per-core=&lt;threads&gt;</span></code></p></td>
<td><div class="line-block">
<div class="line">In task layout, use the specified maximum number of hardware threads per core</div>
<div class="line">(default is 1; there are 2 hardware threads per physical CPU core).</div>
<div class="line">Must also be set in <code class="docutils literal notranslate"><span class="pre">salloc</span></code> or <code class="docutils literal notranslate"><span class="pre">sbatch</span></code> if using 2 threads per core.</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">--cpu-bind=threads</span></code></p></td>
<td><div class="line-block">
<div class="line">Bind tasks to CPUs.</div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">threads</span></code> - Automatically generate masks binding tasks to threads.</div>
<div class="line">(Although this option is not explicitly used in these examples, it is the default CPU binding.)</div>
</div>
</td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In the <code class="docutils literal notranslate"><span class="pre">srun</span></code> man page (and so the table above), threads refers to hardware threads.</p>
</div>
<section id="mpi-ranks-each-with-2-openmp-threads">
<h5>2 MPI ranks - each with 2 OpenMP threads<a class="headerlink" href="#mpi-ranks-each-with-2-openmp-threads" title="Link to this heading"></a></h5>
<p>In this example, the intent is to launch 2 MPI ranks, each of which spawn 2 OpenMP threads, and have all of the 4 OpenMP threads run on different physical CPU cores.</p>
<p><strong>First (INCORRECT) attempt</strong></p>
<p>To set the number of OpenMP threads spawned per MPI rank, the <code class="docutils literal notranslate"><span class="pre">OMP_NUM_THREADS</span></code> environment variable can be used. To set the number of MPI ranks launched, the <code class="docutils literal notranslate"><span class="pre">srun</span></code> flag <code class="docutils literal notranslate"><span class="pre">-n</span></code> can be used.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span><span class="nb">export</span><span class="w"> </span><span class="nv">OMP_NUM_THREADS</span><span class="o">=</span><span class="m">2</span>
$<span class="w"> </span>srun<span class="w"> </span>-N1<span class="w"> </span>-n2<span class="w"> </span>./hello_mpi_omp<span class="w"> </span><span class="p">|</span><span class="w"> </span>sort

WARNING:<span class="w"> </span>Requested<span class="w"> </span>total<span class="w"> </span>thread<span class="w"> </span>count<span class="w"> </span>and/or<span class="w"> </span>thread<span class="w"> </span>affinity<span class="w"> </span>may<span class="w"> </span>result<span class="w"> </span><span class="k">in</span>
oversubscription<span class="w"> </span>of<span class="w"> </span>available<span class="w"> </span>CPU<span class="w"> </span>resources!<span class="w">  </span>Performance<span class="w"> </span>may<span class="w"> </span>be<span class="w"> </span>degraded.
Explicitly<span class="w"> </span><span class="nb">set</span><span class="w"> </span><span class="nv">OMP_WAIT_POLICY</span><span class="o">=</span>PASSIVE<span class="w"> </span>or<span class="w"> </span>ACTIVE<span class="w"> </span>to<span class="w"> </span>suppress<span class="w"> </span>this<span class="w"> </span>message.
Set<span class="w"> </span><span class="nv">CRAY_OMP_CHECK_AFFINITY</span><span class="o">=</span>TRUE<span class="w"> </span>to<span class="w"> </span>print<span class="w"> </span>detailed<span class="w"> </span>thread-affinity<span class="w"> </span>messages.
WARNING:<span class="w"> </span>Requested<span class="w"> </span>total<span class="w"> </span>thread<span class="w"> </span>count<span class="w"> </span>and/or<span class="w"> </span>thread<span class="w"> </span>affinity<span class="w"> </span>may<span class="w"> </span>result<span class="w"> </span><span class="k">in</span>
oversubscription<span class="w"> </span>of<span class="w"> </span>available<span class="w"> </span>CPU<span class="w"> </span>resources!<span class="w">  </span>Performance<span class="w"> </span>may<span class="w"> </span>be<span class="w"> </span>degraded.
Explicitly<span class="w"> </span><span class="nb">set</span><span class="w"> </span><span class="nv">OMP_WAIT_POLICY</span><span class="o">=</span>PASSIVE<span class="w"> </span>or<span class="w"> </span>ACTIVE<span class="w"> </span>to<span class="w"> </span>suppress<span class="w"> </span>this<span class="w"> </span>message.
Set<span class="w"> </span><span class="nv">CRAY_OMP_CHECK_AFFINITY</span><span class="o">=</span>TRUE<span class="w"> </span>to<span class="w"> </span>print<span class="w"> </span>detailed<span class="w"> </span>thread-affinity<span class="w"> </span>messages.

MPI<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher001
MPI<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher001
MPI<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">009</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher001
MPI<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">009</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher001
</pre></div>
</div>
<p>The first thing to notice here is the <code class="docutils literal notranslate"><span class="pre">WARNING</span></code> about oversubscribing the available CPU cores. Also, the output shows each MPI rank did spawn 2 OpenMP threads, but both OpenMP threads ran on the same hardware thread (for a given MPI rank). This was not the intended behavior; each OpenMP thread was meant to run on its own physical CPU core.</p>
<p>The problem here arises from two default settings; 1) each MPI rank is only allocated 1 physical CPU core (<code class="docutils literal notranslate"><span class="pre">-c</span> <span class="pre">1</span></code>) and, 2) only 1 hardware thread per physical CPU core is enabled (<code class="docutils literal notranslate"><span class="pre">--threads-per-core=1</span></code>). So in this case, each MPI rank only has 1 physical core (with 1 hardware thread) to run on - including any threads the process spawns - hence the WARNING and undesired behavior.</p>
<p><strong>Second (CORRECT) attempt</strong></p>
<p>In order for each OpenMP thread to run on its own physical CPU core, each MPI rank should be given 2 physical CPU cores (<code class="docutils literal notranslate"><span class="pre">-c</span> <span class="pre">2</span></code>). Now the OpenMP threads will be mapped to unique hardware threads on separate physical CPU cores.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span><span class="nb">export</span><span class="w"> </span><span class="nv">OMP_NUM_THREADS</span><span class="o">=</span><span class="m">2</span>
$<span class="w"> </span>srun<span class="w"> </span>-N1<span class="w"> </span>-n2<span class="w"> </span>-c2<span class="w"> </span>./hello_mpi_omp<span class="w"> </span><span class="p">|</span><span class="w"> </span>sort

MPI<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher001
MPI<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">002</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher001
MPI<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">009</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher001
MPI<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">010</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher001
</pre></div>
</div>
<p>Now the output shows that each OpenMP thread ran on (one of the hardware threads of) its own physical CPU core. More specifically (see the Crusher Compute Node diagram), OpenMP thread 000 of MPI rank 000 ran on hardware thread 001 (i.e., physical CPU core 01), OpenMP thread 001 of MPI rank 000 ran on hardware thread 002 (i.e., physical CPU core 02), OpenMP thread 000 of MPI rank 001 ran on hardware thread 009 (i.e., physical CPU core 09), and OpenMP thread 001 of MPI rank 001 ran on hardware thread 010 (i.e., physical CPU core 10) - as intended.</p>
<p><strong>Third attempt - Using multiple threads per core</strong></p>
<p>To use both available hardware threads per core, the <em>job</em> must be allocated with <code class="docutils literal notranslate"><span class="pre">--threads-per-core=2</span></code> (as opposed to only the job step - i.e., <code class="docutils literal notranslate"><span class="pre">srun</span></code> command). That value will then be inherited by <code class="docutils literal notranslate"><span class="pre">srun</span></code> unless explcitly overridden with <code class="docutils literal notranslate"><span class="pre">--threads-per-core=1</span></code>.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>salloc<span class="w"> </span>-N1<span class="w"> </span>-A<span class="w"> </span>&lt;project_id&gt;<span class="w"> </span>-t<span class="w"> </span>&lt;time&gt;<span class="w"> </span>-p<span class="w"> </span>&lt;partition&gt;<span class="w"> </span>--threads-per-core<span class="o">=</span><span class="m">2</span>

$<span class="w"> </span><span class="nb">export</span><span class="w"> </span><span class="nv">OMP_NUM_THREADS</span><span class="o">=</span><span class="m">2</span>
$<span class="w"> </span>srun<span class="w"> </span>-N1<span class="w"> </span>-n2<span class="w"> </span>-c2<span class="w"> </span>./hello_mpi_omp<span class="w"> </span><span class="p">|</span><span class="w"> </span>sort

MPI<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher001
MPI<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">065</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher001
MPI<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">009</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher001
MPI<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">073</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher001
</pre></div>
</div>
<p>Comparing this output to the Crusher Compute Node diagram, we see that each pair of OpenMP threads is contained within a single physical core. MPI rank 000 ran on hardware threads 001 and 065 (i.e. physical CPU core 01) and MPI rank 001 ran on hardware threads 009 and 073 (i.e. physical CPU core 09).</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>There are many different ways users might choose to perform these mappings, so users are encouraged to clone the <code class="docutils literal notranslate"><span class="pre">hello_mpi_omp</span></code> program and test whether or not processes and threads are running where intended.</p>
</div>
</section>
</section>
<section id="gpu-mapping">
<h4>GPU Mapping<a class="headerlink" href="#gpu-mapping" title="Link to this heading"></a></h4>
<p>In this sub-section, an MPI+OpenMP+HIP Hello, World program (<a class="reference external" href="https://code.ornl.gov/olcf/hello_jobstep" target="_blank">hello_jobstep</a>) will be used to clarify the GPU mappings. Again, Slurms <a class="reference internal" href="../ace_testbed/defiant_quick_start_guide.html#interactive"><span class="std std-ref">Interactive Jobs</span></a> method was used to request an allocation of 2 compute nodes for these examples: <code class="docutils literal notranslate"><span class="pre">salloc</span> <span class="pre">-A</span> <span class="pre">&lt;project_id&gt;</span> <span class="pre">-t</span> <span class="pre">30</span> <span class="pre">-p</span> <span class="pre">&lt;parition&gt;</span> <span class="pre">-N</span> <span class="pre">2</span></code>. The CPU mapping part of this example is very similar to the example used above in the CPU Mapping sub-section, so the focus here will be on the GPU mapping part.</p>
<p>The following <code class="docutils literal notranslate"><span class="pre">srun</span></code> options will be used in the examples below. See <code class="docutils literal notranslate"><span class="pre">man</span> <span class="pre">srun</span></code> for a complete list of options and more information.</p>
<table class="docutils align-default">
<tbody>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">--gpus</span></code></p></td>
<td><p>Specify the number of GPUs required for the job (total GPUs across all nodes).</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--gpus-per-node</span></code></p></td>
<td><p>Specify the number of GPUs per node required for the job.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">--gpu-bind=closest</span></code></p></td>
<td><p>Binds each task to the GPU which is on the same NUMA domain as the CPU core the MPI rank is running on.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--gpu-bind=map_gpu:&lt;list&gt;</span></code></p></td>
<td><p>Bind tasks to specific GPUs by setting GPU masks on tasks (or ranks) as specified where
<code class="docutils literal notranslate"><span class="pre">&lt;list&gt;</span></code> is <code class="docutils literal notranslate"><span class="pre">&lt;gpu_id_for_task_0&gt;,&lt;gpu_id_for_task_1&gt;,...</span></code>. If the number of tasks (or
ranks) exceeds the number of elements in this list, elements in the list will be reused as
needed starting from the beginning of the list. To simplify support for large task
counts, the lists may follow a map with an asterisk and repetition count. (For example
<code class="docutils literal notranslate"><span class="pre">map_gpu:0*4,1*4</span></code>)</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">--ntasks-per-gpu=&lt;ntasks&gt;</span></code></p></td>
<td><p>Request that there are ntasks tasks invoked for every GPU.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--distribution=&lt;value&gt;[:&lt;value&gt;][:&lt;value&gt;]</span></code></p></td>
<td><p>Specifies the distribution of MPI ranks across compute nodes, sockets (L3 regions on Crusher), and cores,
respectively. The default values are <code class="docutils literal notranslate"><span class="pre">block:cyclic:cyclic</span></code></p></td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Due to the unique architecture of Crusher compute nodes and the way that Slurm currently allocates GPUs and CPU cores to job steps, it is suggested that all 8 GPUs on a node are allocated to the job step to ensure that optimal bindings are possible.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In general, GPU mapping can be accomplished in different ways. For example, an application might map MPI ranks to GPUs programmatically within the code using, say, <code class="docutils literal notranslate"><span class="pre">hipSetDevice</span></code>. In this case, since all GPUs on a node are available to all MPI ranks on that node by default, there might not be a need to map to GPUs using Slurm (just do it in the code). However, in another application, there might be a reason to make only a subset of GPUs available to the MPI ranks on a node. It is this latter case that the following examples refer to.</p>
</div>
<section id="mapping-1-task-per-gpu">
<h5>Mapping 1 task per GPU<a class="headerlink" href="#mapping-1-task-per-gpu" title="Link to this heading"></a></h5>
<p>In the following examples, each MPI rank (and its OpenMP threads) will be mapped to a single GPU.</p>
<p><strong>Example 0: 1 MPI rank with 1 OpenMP thread and 1 GPU (single-node)</strong></p>
<p>Somewhat counterintuitively, this common test case is currently among the most difficult. Slurm ignores GPU bindings for nodes with only a single task, so we do not use <code class="docutils literal notranslate"><span class="pre">--gpu-bind</span></code> here. We must allocate only a single GPU to ensure that only one GPU is available to the task, and since we get the first GPU available we should bind the task to the CPU closest to the allocated GPU.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span><span class="nb">export</span><span class="w"> </span><span class="nv">OMP_NUM_THREADS</span><span class="o">=</span><span class="m">1</span>
$<span class="w"> </span>srun<span class="w"> </span>-N1<span class="w"> </span>-n1<span class="w"> </span>-c1<span class="w"> </span>--cpu-bind<span class="o">=</span>map_cpu:49<span class="w"> </span>--gpus<span class="o">=</span><span class="m">1</span><span class="w"> </span>./hello_jobstep

MPI<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">049</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher001<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c1
</pre></div>
</div>
<p><strong>Example 1: 8 MPI ranks - each with 2 OpenMP threads and 1 GPU (single-node)</strong></p>
<p>This example launches 8 MPI ranks (<code class="docutils literal notranslate"><span class="pre">-n8</span></code>), each with 2 physical CPU cores (<code class="docutils literal notranslate"><span class="pre">-c2</span></code>) to launch 2 OpenMP threads (<code class="docutils literal notranslate"><span class="pre">OMP_NUM_THREADS=2</span></code>) on. In addition, each MPI rank (and its 2 OpenMP threads) should have access to only 1 GPU. To accomplish the GPU mapping, two new <code class="docutils literal notranslate"><span class="pre">srun</span></code> options will be used:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">--gpus-per-node</span></code> specifies the number of GPUs required for the job</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--gpu-bind=closest</span></code> binds each task to the GPU which is closest.</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Without these additional flags, all MPI ranks would have access to all GPUs (which is the default behavior).</p>
</div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span><span class="nb">export</span><span class="w"> </span><span class="nv">OMP_NUM_THREADS</span><span class="o">=</span><span class="m">2</span>
$<span class="w"> </span>srun<span class="w"> </span>-N1<span class="w"> </span>-n8<span class="w"> </span>-c2<span class="w"> </span>--gpus-per-node<span class="o">=</span><span class="m">8</span><span class="w"> </span>--gpu-bind<span class="o">=</span>closest<span class="w"> </span>./hello_jobstep<span class="w"> </span><span class="p">|</span><span class="w"> </span>sort

MPI<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher001<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">4</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d1
MPI<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">002</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher001<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">4</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d1
MPI<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">009</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher001<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">5</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d6
MPI<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">010</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher001<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">5</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d6
MPI<span class="w"> </span><span class="m">002</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">017</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher001<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">2</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c9
MPI<span class="w"> </span><span class="m">002</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">018</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher001<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">2</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c9
MPI<span class="w"> </span><span class="m">003</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">025</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher001<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">3</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>ce
MPI<span class="w"> </span><span class="m">003</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">026</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher001<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">3</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>ce
MPI<span class="w"> </span><span class="m">004</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">033</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher001<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">6</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d9
MPI<span class="w"> </span><span class="m">004</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">034</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher001<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">6</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d9
MPI<span class="w"> </span><span class="m">005</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">041</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher001<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">7</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>de
MPI<span class="w"> </span><span class="m">005</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">042</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher001<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">7</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>de
MPI<span class="w"> </span><span class="m">006</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">049</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher001<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c1
MPI<span class="w"> </span><span class="m">006</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">050</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher001<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c1
MPI<span class="w"> </span><span class="m">007</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">057</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher001<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">1</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c6
MPI<span class="w"> </span><span class="m">007</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">058</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher001<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">1</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c6
</pre></div>
</div>
<p>The output from the program contains a lot of information, so lets unpack it. First, there are different IDs associated with the GPUs so it is important to describe them before moving on. <code class="docutils literal notranslate"><span class="pre">GPU_ID</span></code> is the node-level (or global) GPU ID, which is labeled as one might expect from looking at the Crusher Node Diagram: 0, 1, 2, 3, 4, 5, 6, 7. <code class="docutils literal notranslate"><span class="pre">RT_GPU_ID</span></code> is the HIP runtime GPU ID, which can be though of as each MPI ranks local GPU ID number (with zero-based indexing). So in the output above, each MPI rank has access to only 1 unique GPU - where MPI 000 has access to global GPU 4, MPI 001 has access to global GPU 5, etc., but all MPI ranks show a HIP runtime GPU ID of 0. The reason is that each MPI rank only sees one GPU and so the HIP runtime labels it as 0, even though it might be global GPU ID 0, 1, 2, 3, 4, 5, 6, or 7. The GPUs bus ID is included to definitively show that different GPUs are being used.</p>
<p>Here is a summary of the different GPU IDs reported by the example program:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">GPU_ID</span></code> is the node-level (or global) GPU ID read from <code class="docutils literal notranslate"><span class="pre">ROCR_VISIBLE_DEVICES</span></code>. If this environment variable is not set (either by the user or by Slurm), the value of <code class="docutils literal notranslate"><span class="pre">GPU_ID</span></code> will be set to <code class="docutils literal notranslate"><span class="pre">N/A</span></code> by this program.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">RT_GPU_ID</span></code> is the HIP runtime GPU ID (as reported from, say <code class="docutils literal notranslate"><span class="pre">hipGetDevice</span></code>).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Bus_ID</span></code> is the physical bus ID associated with the GPUs. Comparing the bus IDs is meant to definitively show that different GPUs are being used.</p></li>
</ul>
<p>So the job step (i.e., <code class="docutils literal notranslate"><span class="pre">srun</span></code> command) used above gave the desired output. Each MPI rank spawned 2 OpenMP threads and had access to a unique GPU. The <code class="docutils literal notranslate"><span class="pre">--gpus-per-node=8</span></code> allocated 8 GPUs for node and the <code class="docutils literal notranslate"><span class="pre">--gpu-bind=closest</span></code> ensured that the closest GPU to each rank was the one used.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This example shows an important peculiarity of the Crusher nodes; the closest GPUs to each MPI rank are not in sequential order. For example, MPI rank 000 and its two OpenMP threads ran on hardware threads 000 and 001. As can be seen in the Crusher node diagram, these two hardware threads reside in the same L3 cache region, and that L3 region is connected via Infinity Fabric (blue line in the diagram) to GPU 4. This is an important distinction that can affect performance if not considered carefully.</p>
</div>
<p><strong>Example 2: 16 MPI ranks - each with 2 OpenMP threads and 1 GPU (multi-node)</strong></p>
<p>This example will extend Example 1 to run on 2 nodes. As the output shows, it is a very straightforward exercise of changing the number of nodes to 2 (<code class="docutils literal notranslate"><span class="pre">-N2</span></code>) and the number of MPI ranks to 16 (<code class="docutils literal notranslate"><span class="pre">-n16</span></code>).</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span><span class="nb">export</span><span class="w"> </span><span class="nv">OMP_NUM_THREADS</span><span class="o">=</span><span class="m">2</span>
$<span class="w"> </span>srun<span class="w"> </span>-N2<span class="w"> </span>-n16<span class="w"> </span>-c2<span class="w"> </span>--gpus-per-node<span class="o">=</span><span class="m">8</span><span class="w"> </span>--gpu-bind<span class="o">=</span>closest<span class="w"> </span>./hello_jobstep<span class="w"> </span><span class="p">|</span><span class="w"> </span>sort

MPI<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher001<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">4</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d1
MPI<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">002</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher001<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">4</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d1
MPI<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">009</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher001<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">5</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d6
MPI<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">010</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher001<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">5</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d6
MPI<span class="w"> </span><span class="m">002</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">017</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher001<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">2</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c9
MPI<span class="w"> </span><span class="m">002</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">018</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher001<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">2</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c9
MPI<span class="w"> </span><span class="m">003</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">025</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher001<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">3</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>ce
MPI<span class="w"> </span><span class="m">003</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">026</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher001<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">3</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>ce
MPI<span class="w"> </span><span class="m">004</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">033</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher001<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">6</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d9
MPI<span class="w"> </span><span class="m">004</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">034</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher001<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">6</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d9
MPI<span class="w"> </span><span class="m">005</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">041</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher001<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">7</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>de
MPI<span class="w"> </span><span class="m">005</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">042</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher001<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">7</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>de
MPI<span class="w"> </span><span class="m">006</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">049</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher001<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c1
MPI<span class="w"> </span><span class="m">006</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">050</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher001<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c1
MPI<span class="w"> </span><span class="m">007</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">057</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher001<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">1</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c6
MPI<span class="w"> </span><span class="m">007</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">058</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher001<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">1</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c6
MPI<span class="w"> </span><span class="m">008</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">4</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d1
MPI<span class="w"> </span><span class="m">008</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">002</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">4</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d1
MPI<span class="w"> </span><span class="m">009</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">009</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">5</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d6
MPI<span class="w"> </span><span class="m">009</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">010</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">5</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d6
MPI<span class="w"> </span><span class="m">010</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">017</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">2</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c9
MPI<span class="w"> </span><span class="m">010</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">018</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">2</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c9
MPI<span class="w"> </span><span class="m">011</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">025</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">3</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>ce
MPI<span class="w"> </span><span class="m">011</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">026</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">3</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>ce
MPI<span class="w"> </span><span class="m">012</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">033</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">6</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d9
MPI<span class="w"> </span><span class="m">012</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">034</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">6</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d9
MPI<span class="w"> </span><span class="m">013</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">041</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">7</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>de
MPI<span class="w"> </span><span class="m">013</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">042</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">7</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>de
MPI<span class="w"> </span><span class="m">014</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">049</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c1
MPI<span class="w"> </span><span class="m">014</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">050</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c1
MPI<span class="w"> </span><span class="m">015</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">057</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">1</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c6
MPI<span class="w"> </span><span class="m">015</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">058</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">1</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c6
</pre></div>
</div>
<p><strong>Example 3: 8 MPI ranks - each with 2 OpenMP threads and 1 *specific* GPU (single-node)</strong></p>
<p>This example will be very similar to Example 1, but instead of using <code class="docutils literal notranslate"><span class="pre">--gpu-bind=closest</span></code> to map each MPI rank to the closest GPU, <code class="docutils literal notranslate"><span class="pre">--gpu-bind=map_gpu</span></code> will be used to map each MPI rank to a <em>specific</em> GPU. The <code class="docutils literal notranslate"><span class="pre">map_gpu</span></code> option takes a comma-separated list of GPU IDs to specify how the MPI ranks are mapped to GPUs, where the form of the comma-separated list is <code class="docutils literal notranslate"><span class="pre">&lt;gpu_id_for_task_0&gt;,</span> <span class="pre">&lt;gpu_id_for_task_1&gt;,...</span></code>.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span><span class="nb">export</span><span class="w"> </span><span class="nv">OMP_NUM_THREADS</span><span class="o">=</span><span class="m">2</span>
$<span class="w"> </span>srun<span class="w"> </span>-N1<span class="w"> </span>-n8<span class="w"> </span>-c2<span class="w"> </span>--gpus-per-node<span class="o">=</span><span class="m">8</span><span class="w"> </span>--gpu-bind<span class="o">=</span>map_gpu:4,5,2,3,6,7,0,1<span class="w"> </span>./hello_jobstep<span class="w"> </span><span class="p">|</span><span class="w"> </span>sort

MPI<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher001<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">4</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d1
MPI<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">002</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher001<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">4</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d1
MPI<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">009</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher001<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">5</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d6
MPI<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">010</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher001<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">5</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d6
MPI<span class="w"> </span><span class="m">002</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">017</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher001<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">2</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c9
MPI<span class="w"> </span><span class="m">002</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">018</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher001<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">2</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c9
MPI<span class="w"> </span><span class="m">003</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">025</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher001<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">3</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>ce
MPI<span class="w"> </span><span class="m">003</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">026</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher001<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">3</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>ce
MPI<span class="w"> </span><span class="m">004</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">033</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher001<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">6</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d9
MPI<span class="w"> </span><span class="m">004</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">034</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher001<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">6</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d9
MPI<span class="w"> </span><span class="m">005</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">041</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher001<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">7</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>de
MPI<span class="w"> </span><span class="m">005</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">042</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher001<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">7</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>de
MPI<span class="w"> </span><span class="m">006</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">049</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher001<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c1
MPI<span class="w"> </span><span class="m">006</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">050</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher001<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c1
MPI<span class="w"> </span><span class="m">007</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">057</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher001<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">1</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c6
MPI<span class="w"> </span><span class="m">007</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">058</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher001<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">1</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c6
</pre></div>
</div>
<p>Here, the output is the same as the results from Example 1. This is because the 8 GPU IDs in the comma-separated list happen to specify the GPUs within the same L3 cache region that the MPI ranks are in. So MPI 000 is mapped to GPU 4, MPI 001 is mapped to GPU 5, etc.</p>
<p>While this level of control over mapping MPI ranks to GPUs might be useful for some applications, it is always important to consider the implication of the mapping. For example, if the order of the GPU IDs in the <code class="docutils literal notranslate"><span class="pre">map_gpu</span></code> option is reversed, the MPI ranks and the GPUs they are mapped to would be in different L3 cache regions, which could potentially lead to poorer performance.</p>
<p><strong>Example 4: 16 MPI ranks - each with 2 OpenMP threads and 1 *specific* GPU (multi-node)</strong></p>
<p>Extending Examples 2 and 3 to run on 2 nodes is also a straightforward exercise by changing the number of nodes to 2 (<code class="docutils literal notranslate"><span class="pre">-N2</span></code>) and the number of MPI ranks to 16 (<code class="docutils literal notranslate"><span class="pre">-n16</span></code>).</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span><span class="nb">export</span><span class="w"> </span><span class="nv">OMP_NUM_THREADS</span><span class="o">=</span><span class="m">2</span>
$<span class="w"> </span>srun<span class="w"> </span>-N2<span class="w"> </span>-n16<span class="w"> </span>-c2<span class="w"> </span>--gpus-per-node<span class="o">=</span><span class="m">8</span><span class="w"> </span>--gpu-bind<span class="o">=</span>map_gpu:4,5,2,3,6,7,0,1<span class="w"> </span>./hello_jobstep<span class="w"> </span><span class="p">|</span><span class="w"> </span>sort

MPI<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">4</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d1
MPI<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">002</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">4</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d1
MPI<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">009</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">5</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d6
MPI<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">010</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">5</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d6
MPI<span class="w"> </span><span class="m">002</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">017</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">2</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c9
MPI<span class="w"> </span><span class="m">002</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">018</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">2</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c9
MPI<span class="w"> </span><span class="m">003</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">025</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">3</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>ce
MPI<span class="w"> </span><span class="m">003</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">026</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">3</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>ce
MPI<span class="w"> </span><span class="m">004</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">033</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">6</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d9
MPI<span class="w"> </span><span class="m">004</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">034</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">6</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d9
MPI<span class="w"> </span><span class="m">005</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">041</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">7</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>de
MPI<span class="w"> </span><span class="m">005</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">042</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">7</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>de
MPI<span class="w"> </span><span class="m">006</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">049</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c1
MPI<span class="w"> </span><span class="m">006</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">050</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c1
MPI<span class="w"> </span><span class="m">007</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">057</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">1</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c6
MPI<span class="w"> </span><span class="m">007</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">058</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">1</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c6
MPI<span class="w"> </span><span class="m">008</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher004<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">4</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d1
MPI<span class="w"> </span><span class="m">008</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">002</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher004<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">4</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d1
MPI<span class="w"> </span><span class="m">009</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">009</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher004<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">5</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d6
MPI<span class="w"> </span><span class="m">009</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">010</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher004<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">5</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d6
MPI<span class="w"> </span><span class="m">010</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">017</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher004<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">2</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c9
MPI<span class="w"> </span><span class="m">010</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">018</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher004<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">2</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c9
MPI<span class="w"> </span><span class="m">011</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">025</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher004<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">3</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>ce
MPI<span class="w"> </span><span class="m">011</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">026</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher004<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">3</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>ce
MPI<span class="w"> </span><span class="m">012</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">033</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher004<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">6</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d9
MPI<span class="w"> </span><span class="m">012</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">034</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher004<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">6</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d9
MPI<span class="w"> </span><span class="m">013</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">041</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher004<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">7</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>de
MPI<span class="w"> </span><span class="m">013</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">042</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher004<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">7</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>de
MPI<span class="w"> </span><span class="m">014</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">049</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher004<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c1
MPI<span class="w"> </span><span class="m">014</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">050</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher004<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c1
MPI<span class="w"> </span><span class="m">015</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">057</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher004<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">1</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c6
MPI<span class="w"> </span><span class="m">015</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">058</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher004<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">1</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c6
</pre></div>
</div>
</section>
<section id="mapping-multiple-mpi-ranks-to-a-single-gpu">
<h5>Mapping multiple MPI ranks to a single GPU<a class="headerlink" href="#mapping-multiple-mpi-ranks-to-a-single-gpu" title="Link to this heading"></a></h5>
<p>In the following examples, 2 MPI ranks will be mapped to 1 GPU. For the sake of brevity, <code class="docutils literal notranslate"><span class="pre">OMP_NUM_THREADS</span></code> will be set to <code class="docutils literal notranslate"><span class="pre">1</span></code>, so <code class="docutils literal notranslate"><span class="pre">-c1</span></code> will be used unless otherwise specified.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>On AMDs MI250X, multi-process service (MPS) is not needed since multiple MPI ranks per GPU is supported natively.</p>
</div>
<p><strong>Example 5: 16 MPI ranks - where 2 ranks share a GPU (round-robin, single-node)</strong></p>
<p>This example launches 16 MPI ranks (<code class="docutils literal notranslate"><span class="pre">-n16</span></code>), each with 1 physical CPU core (<code class="docutils literal notranslate"><span class="pre">-c1</span></code>) to launch 1 OpenMP thread (<code class="docutils literal notranslate"><span class="pre">OMP_NUM_THREADS=1</span></code>) on. The MPI ranks will be assigned to GPUs in a round-robin fashion so that each of the 8 GPUs on the node are shared by 2 MPI ranks. To accomplish this GPU mapping, a new <code class="docutils literal notranslate"><span class="pre">srun</span></code> options will be used:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">--ntasks-per-gpu</span></code> specifies the number of MPI ranks that will share access to a GPU.</p></li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span><span class="nb">export</span><span class="w"> </span><span class="nv">OMP_NUM_THREADS</span><span class="o">=</span><span class="m">1</span>
$<span class="w"> </span>srun<span class="w"> </span>-N1<span class="w"> </span>-n16<span class="w"> </span>-c1<span class="w"> </span>--ntasks-per-gpu<span class="o">=</span><span class="m">2</span><span class="w"> </span>--gpu-bind<span class="o">=</span>closest<span class="w"> </span>./hello_jobstep<span class="w"> </span><span class="p">|</span><span class="w"> </span>sort

MPI<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">4</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d1
MPI<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">009</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">5</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d6
MPI<span class="w"> </span><span class="m">002</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">017</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">2</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c9
MPI<span class="w"> </span><span class="m">003</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">025</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">3</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>ce
MPI<span class="w"> </span><span class="m">004</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">033</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">6</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d9
MPI<span class="w"> </span><span class="m">005</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">041</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">7</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>de
MPI<span class="w"> </span><span class="m">006</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">049</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c1
MPI<span class="w"> </span><span class="m">007</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">057</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">1</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c6
MPI<span class="w"> </span><span class="m">008</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">002</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">4</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d1
MPI<span class="w"> </span><span class="m">009</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">010</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">5</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d6
MPI<span class="w"> </span><span class="m">010</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">018</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">2</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c9
MPI<span class="w"> </span><span class="m">011</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">026</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">3</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>ce
MPI<span class="w"> </span><span class="m">012</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">034</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">6</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d9
MPI<span class="w"> </span><span class="m">013</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">042</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">7</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>de
MPI<span class="w"> </span><span class="m">014</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">050</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c1
MPI<span class="w"> </span><span class="m">015</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">058</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">1</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c6
</pre></div>
</div>
<p>The output shows the round-robin (<code class="docutils literal notranslate"><span class="pre">cyclic</span></code>) distribution of MPI ranks to GPUs. In fact, it is a round-robin distribution of MPI ranks <em>to L3 cache regions</em> (the default distribution). The GPU mapping is a consequence of where the MPI ranks are distributed; <code class="docutils literal notranslate"><span class="pre">--gpu-bind=closest</span></code> simply maps the GPU in an L3 cache region to the MPI ranks in the same L3 region.</p>
<p><strong>Example 6: 32 MPI ranks - where 2 ranks share a GPU (round-robin, multi-node)</strong></p>
<p>This example is an extension of Example 5 to run on 2 nodes.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span><span class="nb">export</span><span class="w"> </span><span class="nv">OMP_NUM_THREADS</span><span class="o">=</span><span class="m">1</span>
$<span class="w"> </span>srun<span class="w"> </span>-N2<span class="w"> </span>-n32<span class="w"> </span>-c1<span class="w"> </span>--ntasks-per-gpu<span class="o">=</span><span class="m">2</span><span class="w"> </span>--gpu-bind<span class="o">=</span>closest<span class="w"> </span>./hello_jobstep<span class="w"> </span><span class="p">|</span><span class="w"> </span>sort

MPI<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">4</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d1
MPI<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">009</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">5</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d6
MPI<span class="w"> </span><span class="m">002</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">017</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">2</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c9
MPI<span class="w"> </span><span class="m">003</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">025</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">3</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>ce
MPI<span class="w"> </span><span class="m">004</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">033</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">6</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d9
MPI<span class="w"> </span><span class="m">005</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">041</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">7</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>de
MPI<span class="w"> </span><span class="m">006</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">049</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c1
MPI<span class="w"> </span><span class="m">007</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">057</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">1</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c6
MPI<span class="w"> </span><span class="m">008</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">002</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">4</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d1
MPI<span class="w"> </span><span class="m">009</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">010</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">5</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d6
MPI<span class="w"> </span><span class="m">010</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">018</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">2</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c9
MPI<span class="w"> </span><span class="m">011</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">026</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">3</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>ce
MPI<span class="w"> </span><span class="m">012</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">034</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">6</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d9
MPI<span class="w"> </span><span class="m">013</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">042</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">7</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>de
MPI<span class="w"> </span><span class="m">014</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">050</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c1
MPI<span class="w"> </span><span class="m">015</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">058</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">1</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c6
MPI<span class="w"> </span><span class="m">016</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher004<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">4</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d1
MPI<span class="w"> </span><span class="m">017</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">009</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher004<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">5</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d6
MPI<span class="w"> </span><span class="m">018</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">017</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher004<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">2</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c9
MPI<span class="w"> </span><span class="m">019</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">025</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher004<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">3</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>ce
MPI<span class="w"> </span><span class="m">020</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">033</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher004<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">6</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d9
MPI<span class="w"> </span><span class="m">021</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">041</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher004<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">7</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>de
MPI<span class="w"> </span><span class="m">022</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">049</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher004<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c1
MPI<span class="w"> </span><span class="m">023</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">057</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher004<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">1</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c6
MPI<span class="w"> </span><span class="m">024</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">002</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher004<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">4</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d1
MPI<span class="w"> </span><span class="m">025</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">010</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher004<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">5</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d6
MPI<span class="w"> </span><span class="m">026</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">018</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher004<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">2</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c9
MPI<span class="w"> </span><span class="m">027</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">026</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher004<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">3</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>ce
MPI<span class="w"> </span><span class="m">028</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">034</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher004<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">6</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d9
MPI<span class="w"> </span><span class="m">029</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">042</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher004<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">7</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>de
MPI<span class="w"> </span><span class="m">030</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">050</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher004<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c1
MPI<span class="w"> </span><span class="m">031</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">058</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher004<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">1</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c6
</pre></div>
</div>
<p><strong>Example 7: 16 MPI ranks - where 2 ranks share a GPU (packed, single-node)</strong></p>
<p>This example launches 16 MPI ranks (<code class="docutils literal notranslate"><span class="pre">-n16</span></code>), each with 4 physical CPU cores (<code class="docutils literal notranslate"><span class="pre">-c4</span></code>) to launch 1 OpenMP thread (<code class="docutils literal notranslate"><span class="pre">OMP_NUM_THREADS=1</span></code>) on. Because it is using 4 physical CPU cores per task, core specialization needs to be disabled (<code class="docutils literal notranslate"><span class="pre">-S</span> <span class="pre">0</span></code>). The MPI ranks will be assigned to GPUs in a packed fashion so that each of the 8 GPUs on the node are shared by 2 MPI ranks. Similar to Example 5, <code class="docutils literal notranslate"><span class="pre">-ntasks-per-gpu=2</span></code> will be used, but a new <code class="docutils literal notranslate"><span class="pre">srun</span></code> flag will be used to change the default round-robin (<code class="docutils literal notranslate"><span class="pre">cyclic</span></code>) distribution of MPI ranks across NUMA domains:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">--distribution=&lt;value&gt;[:&lt;value&gt;][:&lt;value&gt;]</span></code> specifies the distribution of MPI ranks across compute nodes, sockets (L3 cache regions on Crusher), and cores, respectively. The default values are <code class="docutils literal notranslate"><span class="pre">block:cyclic:cyclic</span></code>, which is where the <code class="docutils literal notranslate"><span class="pre">cyclic</span></code> assignment comes from in the previous examples.</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In the job step for this example, <code class="docutils literal notranslate"><span class="pre">--distribution=*:block</span></code> is used, where <code class="docutils literal notranslate"><span class="pre">*</span></code> represents the default value of <code class="docutils literal notranslate"><span class="pre">block</span></code> for the distribution of MPI ranks across compute nodes and the distribution of MPI ranks across L3 cache regions has been changed to <code class="docutils literal notranslate"><span class="pre">block</span></code> from its default value of <code class="docutils literal notranslate"><span class="pre">cyclic</span></code>.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Because the distribution across L3 cache regions has been changed to a packed (<code class="docutils literal notranslate"><span class="pre">block</span></code>) configuration, caution must be taken to ensure MPI ranks end up in the L3 cache regions where the GPUs they intend to be mapped to are located. To accomplish this, the number of physical CPU cores assigned to an MPI rank was increased - in this case to 4. Doing so ensures that only 2 MPI ranks can fit into a single L3 cache region. If the value of <code class="docutils literal notranslate"><span class="pre">-c</span></code> was left at <code class="docutils literal notranslate"><span class="pre">1</span></code>, all 8 MPI ranks would be packed into the first L3 region, where the closest GPU would be GPU 4 - the only GPU in that L3 region.</p>
<p>Notice that this is not a workaround like in Example 6, but a requirement due to the <code class="docutils literal notranslate"><span class="pre">block</span></code> distribution of MPI ranks across NUMA domains.</p>
</div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>salloc<span class="w"> </span>-N<span class="w"> </span><span class="m">1</span><span class="w"> </span>-S<span class="w"> </span><span class="m">0</span><span class="w"> </span>...
&lt;job<span class="w"> </span>starts&gt;
$<span class="w"> </span><span class="nb">export</span><span class="w"> </span><span class="nv">OMP_NUM_THREADS</span><span class="o">=</span><span class="m">1</span>
$<span class="w"> </span>srun<span class="w"> </span>-N1<span class="w"> </span>-n16<span class="w"> </span>-c3<span class="w"> </span>--ntasks-per-gpu<span class="o">=</span><span class="m">2</span><span class="w"> </span>--gpu-bind<span class="o">=</span>closest<span class="w"> </span>--distribution<span class="o">=</span>*:block<span class="w"> </span>./hello_jobstep<span class="w"> </span><span class="p">|</span><span class="w"> </span>sort

MPI<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">002</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">4</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d1
MPI<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">006</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">4</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d1
MPI<span class="w"> </span><span class="m">002</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">010</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">5</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d6
MPI<span class="w"> </span><span class="m">003</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">014</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">5</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d6
MPI<span class="w"> </span><span class="m">004</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">016</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">2</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c9
MPI<span class="w"> </span><span class="m">005</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">021</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">2</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c9
MPI<span class="w"> </span><span class="m">006</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">027</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">3</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>ce
MPI<span class="w"> </span><span class="m">007</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">031</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">3</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>ce
MPI<span class="w"> </span><span class="m">008</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">034</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">6</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d9
MPI<span class="w"> </span><span class="m">009</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">037</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">6</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d9
MPI<span class="w"> </span><span class="m">010</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">042</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">7</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>de
MPI<span class="w"> </span><span class="m">011</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">045</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">7</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>de
MPI<span class="w"> </span><span class="m">012</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">049</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c1
MPI<span class="w"> </span><span class="m">013</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">053</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c1
MPI<span class="w"> </span><span class="m">014</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">057</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">1</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c6
MPI<span class="w"> </span><span class="m">015</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">061</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">1</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c6
</pre></div>
</div>
<p>The overall effect of using <code class="docutils literal notranslate"><span class="pre">--distribution=*:block</span></code> and increasing the number of physical CPU cores available to each MPI rank is to place the first two MPI ranks in the first L3 cache region with GPU 4, the next two MPI ranks in the second L3 cache region with GPU 5, and so on.</p>
<p><strong>Example 8: 32 MPI ranks - where 2 ranks share a GPU (packed, multi-node)</strong></p>
<p>This example is an extension of Example 7 to use 2 compute nodes. Again, because it is using 4 physical CPU cores per task, core specialization needs to be disabled (<code class="docutils literal notranslate"><span class="pre">-S</span> <span class="pre">0</span></code>). With the appropriate changes put in place in Example 7, it is a straightforward exercise to change to using 2 nodes (<code class="docutils literal notranslate"><span class="pre">-N2</span></code>) and 32 MPI ranks (<code class="docutils literal notranslate"><span class="pre">-n32</span></code>).</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span><span class="nb">export</span><span class="w"> </span><span class="nv">OMP_NUM_THREADS</span><span class="o">=</span><span class="m">1</span>
$<span class="w"> </span>srun<span class="w"> </span>-N2<span class="w"> </span>-n32<span class="w"> </span>-c4<span class="w"> </span>--ntasks-per-gpu<span class="o">=</span><span class="m">2</span><span class="w"> </span>--gpu-bind<span class="o">=</span>closest<span class="w"> </span>--distribution<span class="o">=</span>*:block<span class="w"> </span>./hello_jobstep<span class="w"> </span><span class="p">|</span><span class="w"> </span>sort

MPI<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">4</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d1
MPI<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">005</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">4</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d1
MPI<span class="w"> </span><span class="m">002</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">009</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">5</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d6
MPI<span class="w"> </span><span class="m">003</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">014</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">5</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d6
MPI<span class="w"> </span><span class="m">004</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">017</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">2</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c9
MPI<span class="w"> </span><span class="m">005</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">020</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">2</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c9
MPI<span class="w"> </span><span class="m">006</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">025</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">3</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>ce
MPI<span class="w"> </span><span class="m">007</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">029</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">3</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>ce
MPI<span class="w"> </span><span class="m">008</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">033</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">6</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d9
MPI<span class="w"> </span><span class="m">009</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">037</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">6</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d9
MPI<span class="w"> </span><span class="m">010</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">042</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">7</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>de
MPI<span class="w"> </span><span class="m">011</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">045</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">7</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>de
MPI<span class="w"> </span><span class="m">012</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">049</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c1
MPI<span class="w"> </span><span class="m">013</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">054</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c1
MPI<span class="w"> </span><span class="m">014</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">058</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">1</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c6
MPI<span class="w"> </span><span class="m">015</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">063</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">1</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c6
MPI<span class="w"> </span><span class="m">016</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher004<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">4</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d1
MPI<span class="w"> </span><span class="m">017</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">005</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher004<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">4</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d1
MPI<span class="w"> </span><span class="m">018</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">009</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher004<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">5</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d6
MPI<span class="w"> </span><span class="m">019</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">014</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher004<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">5</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d6
MPI<span class="w"> </span><span class="m">020</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">017</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher004<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">2</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c9
MPI<span class="w"> </span><span class="m">021</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">021</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher004<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">2</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c9
MPI<span class="w"> </span><span class="m">022</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">026</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher004<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">3</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>ce
MPI<span class="w"> </span><span class="m">023</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">028</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher004<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">3</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>ce
MPI<span class="w"> </span><span class="m">024</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">033</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher004<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">6</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d9
MPI<span class="w"> </span><span class="m">025</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">037</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher004<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">6</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d9
MPI<span class="w"> </span><span class="m">026</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">042</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher004<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">7</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>de
MPI<span class="w"> </span><span class="m">027</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">045</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher004<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">7</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>de
MPI<span class="w"> </span><span class="m">028</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">049</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher004<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c1
MPI<span class="w"> </span><span class="m">029</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">052</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher004<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c1
MPI<span class="w"> </span><span class="m">030</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">059</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher004<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">1</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c6
MPI<span class="w"> </span><span class="m">031</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">061</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher004<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">1</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c6
</pre></div>
</div>
<p><strong>Example 9: 4 independent and simultaneous job steps in a single allocation</strong></p>
<p>This example shows how to run multiple job steps simultaneously in a single allocation. The example below demonstrates running 4 independent, single rank MPI executions on a single node, however the example could be extrapolated to more complex invocations using the above examples.</p>
<p>Submission script:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="c1">#SBATCH -A &lt;projid&gt;</span>
<span class="c1">#SBATCH -N 1</span>
<span class="c1">#SBATCH -t 10</span>

srun<span class="w"> </span>-N1<span class="w"> </span>-c1<span class="w"> </span>--gpus-per-task<span class="o">=</span><span class="m">1</span><span class="w"> </span>--exact<span class="w"> </span>./hello_jobstep<span class="w"> </span><span class="p">&amp;</span>
sleep<span class="w"> </span><span class="m">1</span>
srun<span class="w"> </span>-N1<span class="w"> </span>-c1<span class="w"> </span>--gpus-per-task<span class="o">=</span><span class="m">1</span><span class="w"> </span>--exact<span class="w"> </span>./hello_jobstep<span class="w"> </span><span class="p">&amp;</span>
sleep<span class="w"> </span><span class="m">1</span>
srun<span class="w"> </span>-N1<span class="w"> </span>-c1<span class="w"> </span>--gpus-per-task<span class="o">=</span><span class="m">1</span><span class="w"> </span>--exact<span class="w"> </span>./hello_jobstep<span class="w"> </span><span class="p">&amp;</span>
sleep<span class="w"> </span><span class="m">1</span>
srun<span class="w"> </span>-N1<span class="w"> </span>-c1<span class="w"> </span>--gpus-per-task<span class="o">=</span><span class="m">1</span><span class="w"> </span>--exact<span class="w"> </span>./hello_jobstep<span class="w"> </span><span class="p">&amp;</span>
<span class="nb">wait</span>
</pre></div>
</div>
<p>Output:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>MPI<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">017</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher166<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">2</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c9
MPI<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">057</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher166<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">1</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c6
MPI<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">049</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher166<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c1
MPI<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">025</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>crusher166<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">3</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>ce
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The <code class="docutils literal notranslate"><span class="pre">--exact</span></code> parameter is important to avoid the error message <code class="docutils literal notranslate"><span class="pre">srun:</span> <span class="pre">Job</span> <span class="pre">&lt;job</span> <span class="pre">id&gt;</span> <span class="pre">step</span> <span class="pre">creation</span> <span class="pre">temporarily</span> <span class="pre">disabled,</span> <span class="pre">retrying</span> <span class="pre">(Requested</span> <span class="pre">nodes</span> <span class="pre">are</span> <span class="pre">busy)</span></code>. The <code class="docutils literal notranslate"><span class="pre">wait</span></code> command is also critical, or your job script and allocation will immediately end after launching your jobs in the background. The <code class="docutils literal notranslate"><span class="pre">sleep</span></code> command is currently required to work around a known issue that causes MPICH ERROR. <code class="docutils literal notranslate"><span class="pre">sleep</span></code> will no longer be needed in a future update.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This may result in a sub-optimal alignment of CPU and GPU on the node, as shown in the example output. Unfortunately, at the moment there is not a workaround for this, however improvements are possible in future SLURM updates.</p>
</div>
</section>
<section id="multiple-gpus-per-mpi-rank">
<h5>Multiple GPUs per MPI rank<a class="headerlink" href="#multiple-gpus-per-mpi-rank" title="Link to this heading"></a></h5>
<p>As mentioned previously, all GPUs are accessible by all MPI ranks by default, so it is possible to <em>programatically</em> map any combination of GPUs to MPI ranks. It should be noted however that Cray MPICH does not support GPU-aware MPI for multiple GPUs per rank, so this binding is not suggested.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>There are many different ways users might choose to perform these mappings, so users are encouraged to clone the <code class="docutils literal notranslate"><span class="pre">hello_jobstep</span></code> program and test whether or not processes and threads are running where intended.</p>
</div>
</section>
</section>
</section>
<section id="nvme-usage">
<h3>NVMe Usage<a class="headerlink" href="#nvme-usage" title="Link to this heading"></a></h3>
<p>Each Crusher compute node has [2x] 1.92 TB NVMe devices (SSDs) with a peak sequential performance of 5500 MB/s (read) and 2000 MB/s (write). To use the NVMe, users must request access during job allocation using the <code class="docutils literal notranslate"><span class="pre">-C</span> <span class="pre">nvme</span></code> option to <code class="docutils literal notranslate"><span class="pre">sbatch</span></code>, <code class="docutils literal notranslate"><span class="pre">salloc</span></code>, or <code class="docutils literal notranslate"><span class="pre">srun</span></code>. Once the devices have been granted to a job, users can access them at <code class="docutils literal notranslate"><span class="pre">/mnt/bb/&lt;userid&gt;</span></code>. Users are responsible for moving data to/from the NVMe before/after their jobs. Here is a simple example script:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="c1">#SBATCH -A &lt;projid&gt;</span>
<span class="c1">#SBATCH -J nvme_test</span>
<span class="c1">#SBATCH -o %x-%j.out</span>
<span class="c1">#SBATCH -t 00:05:00</span>
<span class="c1">#SBATCH -p batch</span>
<span class="c1">#SBATCH -N 1</span>
<span class="c1">#SBATCH -C nvme</span>

date

<span class="c1"># Change directory to user scratch space (Orion)</span>
<span class="nb">cd</span><span class="w"> </span>/lustre/orion/&lt;projid&gt;/scratch/&lt;userid&gt;

<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot; &quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;*****ORIGINAL FILE*****&quot;</span>
cat<span class="w"> </span>test.txt
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;***********************&quot;</span>

<span class="c1"># Move file from Orion to SSD</span>
mv<span class="w"> </span>test.txt<span class="w"> </span>/mnt/bb/&lt;userid&gt;

<span class="c1"># Edit file from compute node</span>
srun<span class="w"> </span>-n1<span class="w"> </span>hostname<span class="w"> </span>&gt;&gt;<span class="w"> </span>/mnt/bb/&lt;userid&gt;/test.txt

<span class="c1"># Move file from SSD back to Orion</span>
mv<span class="w"> </span>/mnt/bb/&lt;userid&gt;/test.txt<span class="w"> </span>.

<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot; &quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;*****UPDATED FILE******&quot;</span>
cat<span class="w"> </span>test.txt
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;***********************&quot;</span>
</pre></div>
</div>
<p>And here is the output from the script:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>cat<span class="w"> </span>nvme_test-&lt;jobid&gt;.out
Fri<span class="w"> </span>Oct<span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">12</span>:28:18<span class="w"> </span>EDT<span class="w"> </span><span class="m">2021</span>

*****ORIGINAL<span class="w"> </span>FILE*****
This<span class="w"> </span>is<span class="w"> </span>my<span class="w"> </span>file.<span class="w"> </span>There<span class="w"> </span>are<span class="w"> </span>many<span class="w"> </span>like<span class="w"> </span>it<span class="w"> </span>but<span class="w"> </span>this<span class="w"> </span>one<span class="w"> </span>is<span class="w"> </span>mine.
***********************

*****UPDATED<span class="w"> </span>FILE******
This<span class="w"> </span>is<span class="w"> </span>my<span class="w"> </span>file.<span class="w"> </span>There<span class="w"> </span>are<span class="w"> </span>many<span class="w"> </span>like<span class="w"> </span>it<span class="w"> </span>but<span class="w"> </span>this<span class="w"> </span>one<span class="w"> </span>is<span class="w"> </span>mine.
crusher025
***********************
</pre></div>
</div>
</section>
<section id="tips-for-launching-at-scale">
<h3>Tips for Launching at Scale<a class="headerlink" href="#tips-for-launching-at-scale" title="Link to this heading"></a></h3>
<section id="sbcast-your-executable-and-libraries">
<h4>SBCAST your executable and libraries<a class="headerlink" href="#sbcast-your-executable-and-libraries" title="Link to this heading"></a></h4>
<p>Slurm contains a utility called <code class="docutils literal notranslate"><span class="pre">sbcast</span></code>. This program takes a file and broadcasts it to each nodes node-local storage (ie, <code class="docutils literal notranslate"><span class="pre">/tmp</span></code>, NVMe).
This is useful for sharing large input files, binaries and shared libraries, while reducing the overhead on shared file systems and overhead at startup.
This is highly recommended at scale if you have multiple shared libraries on Lustre/NFS file systems.</p>
<section id="sbcasting-a-single-file">
<h5>SBCASTing a single file<a class="headerlink" href="#sbcasting-a-single-file" title="Link to this heading"></a></h5>
<p>Here is a simple example of a file <code class="docutils literal notranslate"><span class="pre">sbcast</span></code> from a users scratch space on Lustre to each nodes NVMe drive:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="c1">#SBATCH -A &lt;projid&gt;</span>
<span class="c1">#SBATCH -J sbcast_to_nvme</span>
<span class="c1">#SBATCH -o %x-%j.out</span>
<span class="c1">#SBATCH -t 00:05:00</span>
<span class="c1">#SBATCH -p batch</span>
<span class="c1">#SBATCH -N 2</span>
<span class="c1">#SBATCH -C nvme</span>

date

<span class="c1"># Change directory to user scratch space (Orion)</span>
<span class="nb">cd</span><span class="w"> </span>/lustre/orion/&lt;projid&gt;/scratch/&lt;userid&gt;

<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;This is an example file&quot;</span><span class="w"> </span>&gt;<span class="w"> </span>test.txt
<span class="nb">echo</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;*****ORIGINAL FILE*****&quot;</span>
cat<span class="w"> </span>test.txt
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;***********************&quot;</span>

<span class="c1"># SBCAST file from Orion to NVMe -- NOTE: ``-C nvme`` is required to use the NVMe drive</span>
sbcast<span class="w"> </span>-pf<span class="w"> </span>test.txt<span class="w"> </span>/mnt/bb/<span class="nv">$USER</span>/test.txt
<span class="k">if</span><span class="w"> </span><span class="o">[</span><span class="w"> </span>!<span class="w"> </span><span class="s2">&quot;</span><span class="nv">$?</span><span class="s2">&quot;</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="s2">&quot;0&quot;</span><span class="w"> </span><span class="o">]</span><span class="p">;</span><span class="w"> </span><span class="k">then</span>
<span class="w">    </span><span class="c1"># CHECK EXIT CODE. When SBCAST fails, it may leave partial files on the compute nodes, and if you continue to launch srun,</span>
<span class="w">    </span><span class="c1"># your application may pick up partially complete shared library files, which would give you confusing errors.</span>
<span class="w">    </span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;SBCAST failed!&quot;</span>
<span class="w">    </span><span class="nb">exit</span><span class="w"> </span><span class="m">1</span>
<span class="k">fi</span>

<span class="nb">echo</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;*****DISPLAYING FILES ON EACH NODE IN THE ALLOCATION*****&quot;</span>
<span class="c1"># Check to see if file exists</span>
srun<span class="w"> </span>-N<span class="w"> </span><span class="si">${</span><span class="nv">SLURM_NNODES</span><span class="si">}</span><span class="w"> </span>-n<span class="w"> </span><span class="si">${</span><span class="nv">SLURM_NNODES</span><span class="si">}</span><span class="w"> </span>--ntasks-per-node<span class="o">=</span><span class="m">1</span><span class="w"> </span>bash<span class="w"> </span>-c<span class="w"> </span><span class="s2">&quot;echo \&quot;\$(hostname): \$(ls -lh /mnt/bb/</span><span class="nv">$USER</span><span class="s2">/test.txt)\&quot;&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;*********************************************************&quot;</span>

<span class="nb">echo</span>
<span class="c1"># Showing the file on the current node -- this will be the same on all other nodes in the allocation</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;*****SBCAST FILE ON CURRENT NODE******&quot;</span>
cat<span class="w"> </span>/mnt/bb/<span class="nv">$USER</span>/test.txt
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;**************************************&quot;</span>
</pre></div>
</div>
<p>and here is the output from that script:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>Fri<span class="w"> </span><span class="m">03</span><span class="w"> </span>Mar<span class="w"> </span><span class="m">2023</span><span class="w"> </span><span class="m">03</span>:43:30<span class="w"> </span>PM<span class="w"> </span>EST

*****ORIGINAL<span class="w"> </span>FILE*****
This<span class="w"> </span>is<span class="w"> </span>an<span class="w"> </span>example<span class="w"> </span>file
***********************

*****DISPLAYING<span class="w"> </span>FILES<span class="w"> </span>ON<span class="w"> </span>EACH<span class="w"> </span>NODE<span class="w"> </span>IN<span class="w"> </span>THE<span class="w"> </span>ALLOCATION*****
crusher001:<span class="w"> </span>-rw-r--r--<span class="w"> </span><span class="m">1</span><span class="w"> </span>hagertnl<span class="w"> </span>hagertnl<span class="w"> </span><span class="m">24</span><span class="w"> </span>Mar<span class="w">  </span><span class="m">3</span><span class="w"> </span><span class="m">15</span>:43<span class="w"> </span>/mnt/bb/hagertnl/test.txt
crusher002:<span class="w"> </span>-rw-r--r--<span class="w"> </span><span class="m">1</span><span class="w"> </span>hagertnl<span class="w"> </span>hagertnl<span class="w"> </span><span class="m">24</span><span class="w"> </span>Mar<span class="w">  </span><span class="m">3</span><span class="w"> </span><span class="m">15</span>:43<span class="w"> </span>/mnt/bb/hagertnl/test.txt
*********************************************************

*****SBCAST<span class="w"> </span>FILE<span class="w"> </span>ON<span class="w"> </span>CURRENT<span class="w"> </span>NODE******
This<span class="w"> </span>is<span class="w"> </span>an<span class="w"> </span>example<span class="w"> </span>file
**************************************
</pre></div>
</div>
</section>
<section id="sbcasting-a-binary-with-libraries-stored-on-shared-file-systems">
<h5>SBCASTing a binary with libraries stored on shared file systems<a class="headerlink" href="#sbcasting-a-binary-with-libraries-stored-on-shared-file-systems" title="Link to this heading"></a></h5>
<p><code class="docutils literal notranslate"><span class="pre">sbcast</span></code> also handles binaries and their libraries:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="c1">#SBATCH -A &lt;projid&gt;</span>
<span class="c1">#SBATCH -J sbcast_binary_to_nvme</span>
<span class="c1">#SBATCH -o %x-%j.out</span>
<span class="c1">#SBATCH -t 00:05:00</span>
<span class="c1">#SBATCH -p batch</span>
<span class="c1">#SBATCH -N 2</span>
<span class="c1">#SBATCH -C nvme</span>

date

<span class="c1"># Change directory to user scratch space (Orion)</span>
<span class="nb">cd</span><span class="w"> </span>/lustre/orion/&lt;projid&gt;/scratch/&lt;userid&gt;

<span class="c1"># For this example, I use a HIP-enabled LAMMPS binary, with dependencies to MPI, HIP, and HWLOC</span>
<span class="nv">exe</span><span class="o">=</span><span class="s2">&quot;lmp&quot;</span>

<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;*****ldd ./</span><span class="si">${</span><span class="nv">exe</span><span class="si">}</span><span class="s2">*****&quot;</span>
ldd<span class="w"> </span>./<span class="si">${</span><span class="nv">exe</span><span class="si">}</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;*************************&quot;</span>

<span class="c1"># SBCAST executable from Orion to NVMe -- NOTE: ``-C nvme`` is needed in SBATCH headers to use the NVMe drive</span>
<span class="c1"># NOTE: dlopen&#39;d files will NOT be picked up by sbcast</span>
<span class="c1"># SBCAST automatically excludes several directories: /lib,/usr/lib,/lib64,/usr/lib64,/opt</span>
<span class="c1">#   - These directories are node-local and are very fast to read from, so SBCASTing them isn&#39;t critical</span>
<span class="c1">#   - see ``$ scontrol show config | grep BcastExclude`` for current list</span>
<span class="c1">#   - OLCF-provided libraries in ``/sw`` are not on the exclusion list. ``/sw`` is an NFS shared file system</span>
<span class="c1">#   - To override, add ``--exclude=NONE`` to arguments</span>
sbcast<span class="w"> </span>--send-libs<span class="w"> </span>-pf<span class="w"> </span><span class="si">${</span><span class="nv">exe</span><span class="si">}</span><span class="w"> </span>/mnt/bb/<span class="nv">$USER</span>/<span class="si">${</span><span class="nv">exe</span><span class="si">}</span>
<span class="k">if</span><span class="w"> </span><span class="o">[</span><span class="w"> </span>!<span class="w"> </span><span class="s2">&quot;</span><span class="nv">$?</span><span class="s2">&quot;</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="s2">&quot;0&quot;</span><span class="w"> </span><span class="o">]</span><span class="p">;</span><span class="w"> </span><span class="k">then</span>
<span class="w">    </span><span class="c1"># CHECK EXIT CODE. When SBCAST fails, it may leave partial files on the compute nodes, and if you continue to launch srun,</span>
<span class="w">    </span><span class="c1"># your application may pick up partially complete shared library files, which would give you confusing errors.</span>
<span class="w">    </span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;SBCAST failed!&quot;</span>
<span class="w">    </span><span class="nb">exit</span><span class="w"> </span><span class="m">1</span>
<span class="k">fi</span>

<span class="c1"># Check to see if file exists</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;*****ls -lh /mnt/bb/</span><span class="nv">$USER</span><span class="s2">*****&quot;</span>
ls<span class="w"> </span>-lh<span class="w"> </span>/mnt/bb/<span class="nv">$USER</span>/
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;*****ls -lh /mnt/bb/</span><span class="nv">$USER</span><span class="s2">/</span><span class="si">${</span><span class="nv">exe</span><span class="si">}</span><span class="s2">_libs*****&quot;</span>
ls<span class="w"> </span>-lh<span class="w"> </span>/mnt/bb/<span class="nv">$USER</span>/<span class="si">${</span><span class="nv">exe</span><span class="si">}</span>_libs

<span class="c1"># SBCAST sends all libraries detected by `ld` (minus any excluded), and stores them in the same directory in each node&#39;s node-local storage</span>
<span class="c1"># Any libraries opened by `dlopen` are NOT sent, since they are not known by the linker at run-time.</span>

<span class="c1"># At minimum: prepend the node-local path to LD_LIBRARY_PATH to pick up the SBCAST libraries</span>
<span class="c1"># It is also recommended that you **remove** any paths that you don&#39;t need, like those that contain the libraries that you just SBCAST&#39;d</span>
<span class="c1"># Failure to remove may result in unnecessary calls to stat shared file systems</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="s2">&quot;/mnt/bb/</span><span class="nv">$USER</span><span class="s2">/</span><span class="si">${</span><span class="nv">exe</span><span class="si">}</span><span class="s2">_libs:</span><span class="si">${</span><span class="nv">LD_LIBRARY_PATH</span><span class="si">}</span><span class="s2">&quot;</span>

<span class="c1"># If you SBCAST **all** your libraries (ie, `--exclude=NONE`), you may use the following line:</span>
<span class="c1">#export LD_LIBRARY_PATH=&quot;/mnt/bb/$USER/${exe}_libs:$(pkg-config --variable=libdir libfabric)&quot;</span>
<span class="c1"># Use with caution -- certain libraries may use ``dlopen`` at runtime, and that is NOT covered by sbcast</span>
<span class="c1"># If you use this option, we recommend you contact OLCF Help Desk for the latest list of additional steps required</span>

<span class="c1"># You may notice that some libraries are still linked from /sw/frontier, even after SBCASTing.</span>
<span class="c1"># This is because the Spack-build modules use RPATH to find their dependencies.</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;*****ldd /mnt/bb/</span><span class="nv">$USER</span><span class="s2">/</span><span class="si">${</span><span class="nv">exe</span><span class="si">}</span><span class="s2">*****&quot;</span>
ldd<span class="w"> </span>/mnt/bb/<span class="nv">$USER</span>/<span class="si">${</span><span class="nv">exe</span><span class="si">}</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;*************************************&quot;</span>
</pre></div>
</div>
<p>and here is the output from that script:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>Tue<span class="w"> </span><span class="m">28</span><span class="w"> </span>Mar<span class="w"> </span><span class="m">2023</span><span class="w"> </span><span class="m">05</span>:01:41<span class="w"> </span>PM<span class="w"> </span>EDT
*****ldd<span class="w"> </span>./lmp*****
<span class="w">    </span>linux-vdso.so.1<span class="w"> </span><span class="o">(</span>0x00007fffeda02000<span class="o">)</span>
<span class="w">    </span>libgcc_s.so.1<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/lib64/libgcc_s.so.1<span class="w"> </span><span class="o">(</span>0x00007fffed5bb000<span class="o">)</span>
<span class="w">    </span>libpthread.so.0<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/lib64/libpthread.so.0<span class="w"> </span><span class="o">(</span>0x00007fffed398000<span class="o">)</span>
<span class="w">    </span>libm.so.6<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/lib64/libm.so.6<span class="w"> </span><span class="o">(</span>0x00007fffed04d000<span class="o">)</span>
<span class="w">    </span>librt.so.1<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/lib64/librt.so.1<span class="w"> </span><span class="o">(</span>0x00007fffece44000<span class="o">)</span>
<span class="w">    </span>libamdhip64.so.4<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/opt/rocm-4.5.2/lib/libamdhip64.so.4<span class="w"> </span><span class="o">(</span>0x00007fffec052000<span class="o">)</span>
<span class="w">    </span>libmpi_cray.so.12<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/opt/cray/pe/lib64/libmpi_cray.so.12<span class="w"> </span><span class="o">(</span>0x00007fffe96cc000<span class="o">)</span>
<span class="w">    </span>libmpi_gtl_hsa.so.0<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/opt/cray/pe/lib64/libmpi_gtl_hsa.so.0<span class="w"> </span><span class="o">(</span>0x00007fffe9469000<span class="o">)</span>
<span class="w">    </span>libhsa-runtime64.so.1<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/opt/rocm-5.3.0/lib/libhsa-runtime64.so.1<span class="w"> </span><span class="o">(</span>0x00007fffe8fdc000<span class="o">)</span>
<span class="w">    </span>libhwloc.so.15<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/sw/crusher/spack-envs/base/opt/linux-sles15-x86_64/gcc-7.5.0/hwloc-2.5.0-4p6jkgf5ez6wr27pytkzyptppzpugu3e/lib/libhwloc.so.15<span class="w"> </span><span class="o">(</span>0x00007fffe8d82000<span class="o">)</span>
<span class="w">    </span>libdl.so.2<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/lib64/libdl.so.2<span class="w"> </span><span class="o">(</span>0x00007fffe8b7e000<span class="o">)</span>
<span class="w">    </span>libhipfft.so<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/opt/rocm-5.3.0/lib/libhipfft.so<span class="w"> </span><span class="o">(</span>0x00007fffed9d2000<span class="o">)</span>
<span class="w">    </span>libstdc++.so.6<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libstdc++.so.6<span class="w"> </span><span class="o">(</span>0x00007fffe875b000<span class="o">)</span>
<span class="w">    </span>libc.so.6<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/lib64/libc.so.6<span class="w"> </span><span class="o">(</span>0x00007fffe8366000<span class="o">)</span>
<span class="w">    </span>/lib64/ld-linux-x86-64.so.2<span class="w"> </span><span class="o">(</span>0x00007fffed7da000<span class="o">)</span>
<span class="w">    </span>libamd_comgr.so.2<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/opt/rocm-5.3.0/lib/libamd_comgr.so.2<span class="w"> </span><span class="o">(</span>0x00007fffe06e0000<span class="o">)</span>
<span class="w">    </span>libnuma.so.1<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libnuma.so.1<span class="w"> </span><span class="o">(</span>0x00007fffe04d4000<span class="o">)</span>
<span class="w">    </span>libfabric.so.1<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/opt/cray/libfabric/1.15.2.0/lib64/libfabric.so.1<span class="w"> </span><span class="o">(</span>0x00007fffe01e2000<span class="o">)</span>
<span class="w">    </span>libatomic.so.1<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libatomic.so.1<span class="w"> </span><span class="o">(</span>0x00007fffdffd9000<span class="o">)</span>
<span class="w">    </span>libpmi.so.0<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/opt/cray/pe/lib64/libpmi.so.0<span class="w"> </span><span class="o">(</span>0x00007fffdfdd7000<span class="o">)</span>
<span class="w">    </span>libpmi2.so.0<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/opt/cray/pe/lib64/libpmi2.so.0<span class="w"> </span><span class="o">(</span>0x00007fffdfb9e000<span class="o">)</span>
<span class="w">    </span>libquadmath.so.0<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libquadmath.so.0<span class="w"> </span><span class="o">(</span>0x00007fffdf959000<span class="o">)</span>
<span class="w">    </span>libmodules.so.1<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/opt/cray/pe/lib64/cce/libmodules.so.1<span class="w"> </span><span class="o">(</span>0x00007fffed9b5000<span class="o">)</span>
<span class="w">    </span>libfi.so.1<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/opt/cray/pe/lib64/cce/libfi.so.1<span class="w"> </span><span class="o">(</span>0x00007fffdf3b4000<span class="o">)</span>
<span class="w">    </span>libcraymath.so.1<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/opt/cray/pe/lib64/cce/libcraymath.so.1<span class="w"> </span><span class="o">(</span>0x00007fffed8ce000<span class="o">)</span>
<span class="w">    </span>libf.so.1<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/opt/cray/pe/lib64/cce/libf.so.1<span class="w"> </span><span class="o">(</span>0x00007fffed83b000<span class="o">)</span>
<span class="w">    </span>libu.so.1<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/opt/cray/pe/lib64/cce/libu.so.1<span class="w"> </span><span class="o">(</span>0x00007fffdf2ab000<span class="o">)</span>
<span class="w">    </span>libcsup.so.1<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/opt/cray/pe/lib64/cce/libcsup.so.1<span class="w"> </span><span class="o">(</span>0x00007fffed832000<span class="o">)</span>
<span class="w">    </span>libamdhip64.so.5<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/opt/rocm-5.3.0/lib/libamdhip64.so.5<span class="w"> </span><span class="o">(</span>0x00007fffdda8a000<span class="o">)</span>
<span class="w">    </span>libelf.so.1<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libelf.so.1<span class="w"> </span><span class="o">(</span>0x00007fffdd871000<span class="o">)</span>
<span class="w">    </span>libdrm.so.2<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libdrm.so.2<span class="w"> </span><span class="o">(</span>0x00007fffdd65d000<span class="o">)</span>
<span class="w">    </span>libdrm_amdgpu.so.1<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libdrm_amdgpu.so.1<span class="w"> </span><span class="o">(</span>0x00007fffdd453000<span class="o">)</span>
<span class="w">    </span>libpciaccess.so.0<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/sw/crusher/spack-envs/base/opt/linux-sles15-x86_64/gcc-7.5.0/libpciaccess-0.16-6evcvl74myxfxdrddmnsvbc7sgfx6s5j/lib/libpciaccess.so.0<span class="w"> </span><span class="o">(</span>0x00007fffdd24a000<span class="o">)</span>
<span class="w">    </span>libxml2.so.2<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/sw/crusher/spack-envs/base/opt/linux-sles15-x86_64/gcc-7.5.0/libxml2-2.9.12-h7fe2yauotu3xit7rsaixaowd3noknur/lib/libxml2.so.2<span class="w"> </span><span class="o">(</span>0x00007fffdcee6000<span class="o">)</span>
<span class="w">    </span>librocfft.so.0<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/opt/rocm-5.3.0/lib/librocfft.so.0<span class="w"> </span><span class="o">(</span>0x00007fffdca1a000<span class="o">)</span>
<span class="w">    </span>libz.so.1<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/lib64/libz.so.1<span class="w"> </span><span class="o">(</span>0x00007fffdc803000<span class="o">)</span>
<span class="w">    </span>libtinfo.so.6<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/lib64/libtinfo.so.6<span class="w"> </span><span class="o">(</span>0x00007fffdc5d5000<span class="o">)</span>
<span class="w">    </span>libcxi.so.1<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libcxi.so.1<span class="w"> </span><span class="o">(</span>0x00007fffdc3b0000<span class="o">)</span>
<span class="w">    </span>libcurl.so.4<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libcurl.so.4<span class="w"> </span><span class="o">(</span>0x00007fffdc311000<span class="o">)</span>
<span class="w">    </span>libjson-c.so.3<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libjson-c.so.3<span class="w"> </span><span class="o">(</span>0x00007fffdc101000<span class="o">)</span>
<span class="w">    </span>libpals.so.0<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/opt/cray/pe/lib64/libpals.so.0<span class="w"> </span><span class="o">(</span>0x00007fffdbefc000<span class="o">)</span>
<span class="w">    </span>libgfortran.so.5<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/opt/cray/pe/gcc-libs/libgfortran.so.5<span class="w"> </span><span class="o">(</span>0x00007fffdba50000<span class="o">)</span>
<span class="w">    </span>liblzma.so.5<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/sw/crusher/spack-envs/base/opt/linux-sles15-x86_64/gcc-7.5.0/xz-5.2.5-zwra4gpckelt5umczowf3jtmeiz3yd7u/lib/liblzma.so.5<span class="w"> </span><span class="o">(</span>0x00007fffdb82a000<span class="o">)</span>
<span class="w">    </span>libiconv.so.2<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/sw/crusher/spack-envs/base/opt/linux-sles15-x86_64/gcc-7.5.0/libiconv-1.16-coz5dzhtoeq5unhjirayfn2xftnxk43l/lib/libiconv.so.2<span class="w"> </span><span class="o">(</span>0x00007fffdb52e000<span class="o">)</span>
<span class="w">    </span>librocfft-device-0.so.0<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/opt/rocm-5.3.0/lib/librocfft-device-0.so.0<span class="w"> </span><span class="o">(</span>0x00007fffa11a0000<span class="o">)</span>
<span class="w">    </span>librocfft-device-1.so.0<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/opt/rocm-5.3.0/lib/librocfft-device-1.so.0<span class="w"> </span><span class="o">(</span>0x00007fff6491b000<span class="o">)</span>
<span class="w">    </span>librocfft-device-2.so.0<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/opt/rocm-5.3.0/lib/librocfft-device-2.so.0<span class="w"> </span><span class="o">(</span>0x00007fff2a828000<span class="o">)</span>
<span class="w">    </span>librocfft-device-3.so.0<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/opt/rocm-5.3.0/lib/librocfft-device-3.so.0<span class="w"> </span><span class="o">(</span>0x00007ffef7eee000<span class="o">)</span>
<span class="w">    </span>libnghttp2.so.14<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libnghttp2.so.14<span class="w"> </span><span class="o">(</span>0x00007ffef7cc6000<span class="o">)</span>
<span class="w">    </span>libidn2.so.0<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libidn2.so.0<span class="w"> </span><span class="o">(</span>0x00007ffef7aa9000<span class="o">)</span>
<span class="w">    </span>libssh.so.4<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libssh.so.4<span class="w"> </span><span class="o">(</span>0x00007ffef783b000<span class="o">)</span>
<span class="w">    </span>libpsl.so.5<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libpsl.so.5<span class="w"> </span><span class="o">(</span>0x00007ffef7629000<span class="o">)</span>
<span class="w">    </span>libssl.so.1.1<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libssl.so.1.1<span class="w"> </span><span class="o">(</span>0x00007ffef758a000<span class="o">)</span>
<span class="w">    </span>libcrypto.so.1.1<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libcrypto.so.1.1<span class="w"> </span><span class="o">(</span>0x00007ffef724a000<span class="o">)</span>
<span class="w">    </span>libgssapi_krb5.so.2<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libgssapi_krb5.so.2<span class="w"> </span><span class="o">(</span>0x00007ffef6ff8000<span class="o">)</span>
<span class="w">    </span>libldap_r-2.4.so.2<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libldap_r-2.4.so.2<span class="w"> </span><span class="o">(</span>0x00007ffef6da4000<span class="o">)</span>
<span class="w">    </span>liblber-2.4.so.2<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/liblber-2.4.so.2<span class="w"> </span><span class="o">(</span>0x00007ffef6b95000<span class="o">)</span>
<span class="w">    </span>libzstd.so.1<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libzstd.so.1<span class="w"> </span><span class="o">(</span>0x00007ffef6865000<span class="o">)</span>
<span class="w">    </span>libbrotlidec.so.1<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libbrotlidec.so.1<span class="w"> </span><span class="o">(</span>0x00007ffef6659000<span class="o">)</span>
<span class="w">    </span>libunistring.so.2<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libunistring.so.2<span class="w"> </span><span class="o">(</span>0x00007ffef62d6000<span class="o">)</span>
<span class="w">    </span>libjitterentropy.so.3<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libjitterentropy.so.3<span class="w"> </span><span class="o">(</span>0x00007ffef60cf000<span class="o">)</span>
<span class="w">    </span>libkrb5.so.3<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libkrb5.so.3<span class="w"> </span><span class="o">(</span>0x00007ffef5df6000<span class="o">)</span>
<span class="w">    </span>libk5crypto.so.3<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libk5crypto.so.3<span class="w"> </span><span class="o">(</span>0x00007ffef5bde000<span class="o">)</span>
<span class="w">    </span>libcom_err.so.2<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/lib64/libcom_err.so.2<span class="w"> </span><span class="o">(</span>0x00007ffef59da000<span class="o">)</span>
<span class="w">    </span>libkrb5support.so.0<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libkrb5support.so.0<span class="w"> </span><span class="o">(</span>0x00007ffef57cb000<span class="o">)</span>
<span class="w">    </span>libresolv.so.2<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/lib64/libresolv.so.2<span class="w"> </span><span class="o">(</span>0x00007ffef55b3000<span class="o">)</span>
<span class="w">    </span>libsasl2.so.3<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libsasl2.so.3<span class="w"> </span><span class="o">(</span>0x00007ffef5396000<span class="o">)</span>
<span class="w">    </span>libbrotlicommon.so.1<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libbrotlicommon.so.1<span class="w"> </span><span class="o">(</span>0x00007ffef5175000<span class="o">)</span>
<span class="w">    </span>libkeyutils.so.1<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libkeyutils.so.1<span class="w"> </span><span class="o">(</span>0x00007ffef4f70000<span class="o">)</span>
<span class="w">    </span>libselinux.so.1<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/lib64/libselinux.so.1<span class="w"> </span><span class="o">(</span>0x00007ffef4d47000<span class="o">)</span>
<span class="w">    </span>libpcre.so.1<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libpcre.so.1<span class="w"> </span><span class="o">(</span>0x00007ffef4abe000<span class="o">)</span>
*************************
*****ls<span class="w"> </span>-lh<span class="w"> </span>/mnt/bb/hagertnl*****
total<span class="w"> </span>236M
-rwxr-xr-x<span class="w"> </span><span class="m">1</span><span class="w"> </span>hagertnl<span class="w"> </span>hagertnl<span class="w"> </span>236M<span class="w"> </span>Mar<span class="w"> </span><span class="m">28</span><span class="w"> </span><span class="m">17</span>:01<span class="w"> </span>lmp
drwx------<span class="w"> </span><span class="m">2</span><span class="w"> </span>hagertnl<span class="w"> </span>hagertnl<span class="w">  </span><span class="m">114</span><span class="w"> </span>Mar<span class="w"> </span><span class="m">28</span><span class="w"> </span><span class="m">17</span>:01<span class="w"> </span>lmp_libs
*****ls<span class="w"> </span>-lh<span class="w"> </span>/mnt/bb/hagertnl/lmp_libs*****
total<span class="w"> </span><span class="m">9</span>.2M
-rwxr-xr-x<span class="w"> </span><span class="m">1</span><span class="w"> </span>hagertnl<span class="w"> </span>hagertnl<span class="w"> </span><span class="m">1</span>.6M<span class="w"> </span>Oct<span class="w">  </span><span class="m">6</span><span class="w">  </span><span class="m">2021</span><span class="w"> </span>libhwloc.so.15
-rwxr-xr-x<span class="w"> </span><span class="m">1</span><span class="w"> </span>hagertnl<span class="w"> </span>hagertnl<span class="w"> </span><span class="m">1</span>.6M<span class="w"> </span>Oct<span class="w">  </span><span class="m">6</span><span class="w">  </span><span class="m">2021</span><span class="w"> </span>libiconv.so.2
-rwxr-xr-x<span class="w"> </span><span class="m">1</span><span class="w"> </span>hagertnl<span class="w"> </span>hagertnl<span class="w"> </span>783K<span class="w"> </span>Oct<span class="w">  </span><span class="m">6</span><span class="w">  </span><span class="m">2021</span><span class="w"> </span>liblzma.so.5
-rwxr-xr-x<span class="w"> </span><span class="m">1</span><span class="w"> </span>hagertnl<span class="w"> </span>hagertnl<span class="w"> </span>149K<span class="w"> </span>Oct<span class="w">  </span><span class="m">6</span><span class="w">  </span><span class="m">2021</span><span class="w"> </span>libpciaccess.so.0
-rwxr-xr-x<span class="w"> </span><span class="m">1</span><span class="w"> </span>hagertnl<span class="w"> </span>hagertnl<span class="w"> </span><span class="m">5</span>.2M<span class="w"> </span>Oct<span class="w">  </span><span class="m">6</span><span class="w">  </span><span class="m">2021</span><span class="w"> </span>libxml2.so.2
*****ldd<span class="w"> </span>/mnt/bb/hagertnl/lmp*****
<span class="w">    </span>linux-vdso.so.1<span class="w"> </span><span class="o">(</span>0x00007fffeda02000<span class="o">)</span>
<span class="w">    </span>libgcc_s.so.1<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/lib64/libgcc_s.so.1<span class="w"> </span><span class="o">(</span>0x00007fffed5bb000<span class="o">)</span>
<span class="w">    </span>libpthread.so.0<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/lib64/libpthread.so.0<span class="w"> </span><span class="o">(</span>0x00007fffed398000<span class="o">)</span>
<span class="w">    </span>libm.so.6<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/lib64/libm.so.6<span class="w"> </span><span class="o">(</span>0x00007fffed04d000<span class="o">)</span>
<span class="w">    </span>librt.so.1<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/lib64/librt.so.1<span class="w"> </span><span class="o">(</span>0x00007fffece44000<span class="o">)</span>
<span class="w">    </span>libamdhip64.so.4<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/opt/rocm-4.5.2/lib/libamdhip64.so.4<span class="w"> </span><span class="o">(</span>0x00007fffec052000<span class="o">)</span>
<span class="w">    </span>libmpi_cray.so.12<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/opt/cray/pe/lib64/libmpi_cray.so.12<span class="w"> </span><span class="o">(</span>0x00007fffe96cc000<span class="o">)</span>
<span class="w">    </span>libmpi_gtl_hsa.so.0<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/opt/cray/pe/lib64/libmpi_gtl_hsa.so.0<span class="w"> </span><span class="o">(</span>0x00007fffe9469000<span class="o">)</span>
<span class="w">    </span>libhsa-runtime64.so.1<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/opt/rocm-5.3.0/lib/libhsa-runtime64.so.1<span class="w"> </span><span class="o">(</span>0x00007fffe8fdc000<span class="o">)</span>
<span class="w">    </span>libhwloc.so.15<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/mnt/bb/hagertnl/lmp_libs/libhwloc.so.15<span class="w"> </span><span class="o">(</span>0x00007fffe8d82000<span class="o">)</span>
<span class="w">    </span>libdl.so.2<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/lib64/libdl.so.2<span class="w"> </span><span class="o">(</span>0x00007fffe8b7e000<span class="o">)</span>
<span class="w">    </span>libhipfft.so<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/opt/rocm-5.3.0/lib/libhipfft.so<span class="w"> </span><span class="o">(</span>0x00007fffed9d2000<span class="o">)</span>
<span class="w">    </span>libstdc++.so.6<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libstdc++.so.6<span class="w"> </span><span class="o">(</span>0x00007fffe875b000<span class="o">)</span>
<span class="w">    </span>libc.so.6<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/lib64/libc.so.6<span class="w"> </span><span class="o">(</span>0x00007fffe8366000<span class="o">)</span>
<span class="w">    </span>/lib64/ld-linux-x86-64.so.2<span class="w"> </span><span class="o">(</span>0x00007fffed7da000<span class="o">)</span>
<span class="w">    </span>libamd_comgr.so.2<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/opt/rocm-5.3.0/lib/libamd_comgr.so.2<span class="w"> </span><span class="o">(</span>0x00007fffe06e0000<span class="o">)</span>
<span class="w">    </span>libnuma.so.1<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libnuma.so.1<span class="w"> </span><span class="o">(</span>0x00007fffe04d4000<span class="o">)</span>
<span class="w">    </span>libfabric.so.1<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/opt/cray/libfabric/1.15.2.0/lib64/libfabric.so.1<span class="w"> </span><span class="o">(</span>0x00007fffe01e2000<span class="o">)</span>
<span class="w">    </span>libatomic.so.1<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libatomic.so.1<span class="w"> </span><span class="o">(</span>0x00007fffdffd9000<span class="o">)</span>
<span class="w">    </span>libpmi.so.0<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/opt/cray/pe/lib64/libpmi.so.0<span class="w"> </span><span class="o">(</span>0x00007fffdfdd7000<span class="o">)</span>
<span class="w">    </span>libpmi2.so.0<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/opt/cray/pe/lib64/libpmi2.so.0<span class="w"> </span><span class="o">(</span>0x00007fffdfb9e000<span class="o">)</span>
<span class="w">    </span>libquadmath.so.0<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libquadmath.so.0<span class="w"> </span><span class="o">(</span>0x00007fffdf959000<span class="o">)</span>
<span class="w">    </span>libmodules.so.1<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/opt/cray/pe/lib64/cce/libmodules.so.1<span class="w"> </span><span class="o">(</span>0x00007fffed9b5000<span class="o">)</span>
<span class="w">    </span>libfi.so.1<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/opt/cray/pe/lib64/cce/libfi.so.1<span class="w"> </span><span class="o">(</span>0x00007fffdf3b4000<span class="o">)</span>
<span class="w">    </span>libcraymath.so.1<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/opt/cray/pe/lib64/cce/libcraymath.so.1<span class="w"> </span><span class="o">(</span>0x00007fffed8ce000<span class="o">)</span>
<span class="w">    </span>libf.so.1<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/opt/cray/pe/lib64/cce/libf.so.1<span class="w"> </span><span class="o">(</span>0x00007fffed83b000<span class="o">)</span>
<span class="w">    </span>libu.so.1<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/opt/cray/pe/lib64/cce/libu.so.1<span class="w"> </span><span class="o">(</span>0x00007fffdf2ab000<span class="o">)</span>
<span class="w">    </span>libcsup.so.1<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/opt/cray/pe/lib64/cce/libcsup.so.1<span class="w"> </span><span class="o">(</span>0x00007fffed832000<span class="o">)</span>
<span class="w">    </span>libamdhip64.so.5<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/opt/rocm-5.3.0/lib/libamdhip64.so.5<span class="w"> </span><span class="o">(</span>0x00007fffdda8a000<span class="o">)</span>
<span class="w">    </span>libelf.so.1<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libelf.so.1<span class="w"> </span><span class="o">(</span>0x00007fffdd871000<span class="o">)</span>
<span class="w">    </span>libdrm.so.2<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libdrm.so.2<span class="w"> </span><span class="o">(</span>0x00007fffdd65d000<span class="o">)</span>
<span class="w">    </span>libdrm_amdgpu.so.1<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libdrm_amdgpu.so.1<span class="w"> </span><span class="o">(</span>0x00007fffdd453000<span class="o">)</span>
<span class="w">    </span>libpciaccess.so.0<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/sw/crusher/spack-envs/base/opt/linux-sles15-x86_64/gcc-7.5.0/libpciaccess-0.16-6evcvl74myxfxdrddmnsvbc7sgfx6s5j/lib/libpciaccess.so.0<span class="w"> </span><span class="o">(</span>0x00007fffdd24a000<span class="o">)</span>
<span class="w">    </span>libxml2.so.2<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/sw/crusher/spack-envs/base/opt/linux-sles15-x86_64/gcc-7.5.0/libxml2-2.9.12-h7fe2yauotu3xit7rsaixaowd3noknur/lib/libxml2.so.2<span class="w"> </span><span class="o">(</span>0x00007fffdcee6000<span class="o">)</span>
<span class="w">    </span>librocfft.so.0<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/opt/rocm-5.3.0/lib/librocfft.so.0<span class="w"> </span><span class="o">(</span>0x00007fffdca1a000<span class="o">)</span>
<span class="w">    </span>libz.so.1<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/lib64/libz.so.1<span class="w"> </span><span class="o">(</span>0x00007fffdc803000<span class="o">)</span>
<span class="w">    </span>libtinfo.so.6<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/lib64/libtinfo.so.6<span class="w"> </span><span class="o">(</span>0x00007fffdc5d5000<span class="o">)</span>
<span class="w">    </span>libcxi.so.1<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libcxi.so.1<span class="w"> </span><span class="o">(</span>0x00007fffdc3b0000<span class="o">)</span>
<span class="w">    </span>libcurl.so.4<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libcurl.so.4<span class="w"> </span><span class="o">(</span>0x00007fffdc311000<span class="o">)</span>
<span class="w">    </span>libjson-c.so.3<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libjson-c.so.3<span class="w"> </span><span class="o">(</span>0x00007fffdc101000<span class="o">)</span>
<span class="w">    </span>libpals.so.0<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/opt/cray/pe/lib64/libpals.so.0<span class="w"> </span><span class="o">(</span>0x00007fffdbefc000<span class="o">)</span>
<span class="w">    </span>libgfortran.so.5<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/opt/cray/pe/gcc-libs/libgfortran.so.5<span class="w"> </span><span class="o">(</span>0x00007fffdba50000<span class="o">)</span>
<span class="w">    </span>liblzma.so.5<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/sw/crusher/spack-envs/base/opt/linux-sles15-x86_64/gcc-7.5.0/xz-5.2.5-zwra4gpckelt5umczowf3jtmeiz3yd7u/lib/liblzma.so.5<span class="w"> </span><span class="o">(</span>0x00007fffdb82a000<span class="o">)</span>
<span class="w">    </span>libiconv.so.2<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/sw/crusher/spack-envs/base/opt/linux-sles15-x86_64/gcc-7.5.0/libiconv-1.16-coz5dzhtoeq5unhjirayfn2xftnxk43l/lib/libiconv.so.2<span class="w"> </span><span class="o">(</span>0x00007fffdb52e000<span class="o">)</span>
<span class="w">    </span>librocfft-device-0.so.0<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/opt/rocm-5.3.0/lib/librocfft-device-0.so.0<span class="w"> </span><span class="o">(</span>0x00007fffa11a0000<span class="o">)</span>
<span class="w">    </span>librocfft-device-1.so.0<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/opt/rocm-5.3.0/lib/librocfft-device-1.so.0<span class="w"> </span><span class="o">(</span>0x00007fff6491b000<span class="o">)</span>
<span class="w">    </span>librocfft-device-2.so.0<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/opt/rocm-5.3.0/lib/librocfft-device-2.so.0<span class="w"> </span><span class="o">(</span>0x00007fff2a828000<span class="o">)</span>
<span class="w">    </span>librocfft-device-3.so.0<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/opt/rocm-5.3.0/lib/librocfft-device-3.so.0<span class="w"> </span><span class="o">(</span>0x00007ffef7eee000<span class="o">)</span>
<span class="w">    </span>libnghttp2.so.14<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libnghttp2.so.14<span class="w"> </span><span class="o">(</span>0x00007ffef7cc6000<span class="o">)</span>
<span class="w">    </span>libidn2.so.0<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libidn2.so.0<span class="w"> </span><span class="o">(</span>0x00007ffef7aa9000<span class="o">)</span>
<span class="w">    </span>libssh.so.4<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libssh.so.4<span class="w"> </span><span class="o">(</span>0x00007ffef783b000<span class="o">)</span>
<span class="w">    </span>libpsl.so.5<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libpsl.so.5<span class="w"> </span><span class="o">(</span>0x00007ffef7629000<span class="o">)</span>
<span class="w">    </span>libssl.so.1.1<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libssl.so.1.1<span class="w"> </span><span class="o">(</span>0x00007ffef758a000<span class="o">)</span>
<span class="w">    </span>libcrypto.so.1.1<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libcrypto.so.1.1<span class="w"> </span><span class="o">(</span>0x00007ffef724a000<span class="o">)</span>
<span class="w">    </span>libgssapi_krb5.so.2<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libgssapi_krb5.so.2<span class="w"> </span><span class="o">(</span>0x00007ffef6ff8000<span class="o">)</span>
<span class="w">    </span>libldap_r-2.4.so.2<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libldap_r-2.4.so.2<span class="w"> </span><span class="o">(</span>0x00007ffef6da4000<span class="o">)</span>
<span class="w">    </span>liblber-2.4.so.2<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/liblber-2.4.so.2<span class="w"> </span><span class="o">(</span>0x00007ffef6b95000<span class="o">)</span>
<span class="w">    </span>libzstd.so.1<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libzstd.so.1<span class="w"> </span><span class="o">(</span>0x00007ffef6865000<span class="o">)</span>
<span class="w">    </span>libbrotlidec.so.1<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libbrotlidec.so.1<span class="w"> </span><span class="o">(</span>0x00007ffef6659000<span class="o">)</span>
<span class="w">    </span>libunistring.so.2<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libunistring.so.2<span class="w"> </span><span class="o">(</span>0x00007ffef62d6000<span class="o">)</span>
<span class="w">    </span>libjitterentropy.so.3<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libjitterentropy.so.3<span class="w"> </span><span class="o">(</span>0x00007ffef60cf000<span class="o">)</span>
<span class="w">    </span>libkrb5.so.3<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libkrb5.so.3<span class="w"> </span><span class="o">(</span>0x00007ffef5df6000<span class="o">)</span>
<span class="w">    </span>libk5crypto.so.3<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libk5crypto.so.3<span class="w"> </span><span class="o">(</span>0x00007ffef5bde000<span class="o">)</span>
<span class="w">    </span>libcom_err.so.2<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/lib64/libcom_err.so.2<span class="w"> </span><span class="o">(</span>0x00007ffef59da000<span class="o">)</span>
<span class="w">    </span>libkrb5support.so.0<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libkrb5support.so.0<span class="w"> </span><span class="o">(</span>0x00007ffef57cb000<span class="o">)</span>
<span class="w">    </span>libresolv.so.2<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/lib64/libresolv.so.2<span class="w"> </span><span class="o">(</span>0x00007ffef55b3000<span class="o">)</span>
<span class="w">    </span>libsasl2.so.3<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libsasl2.so.3<span class="w"> </span><span class="o">(</span>0x00007ffef5396000<span class="o">)</span>
<span class="w">    </span>libbrotlicommon.so.1<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libbrotlicommon.so.1<span class="w"> </span><span class="o">(</span>0x00007ffef5175000<span class="o">)</span>
<span class="w">    </span>libkeyutils.so.1<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libkeyutils.so.1<span class="w"> </span><span class="o">(</span>0x00007ffef4f70000<span class="o">)</span>
<span class="w">    </span>libselinux.so.1<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/lib64/libselinux.so.1<span class="w"> </span><span class="o">(</span>0x00007ffef4d47000<span class="o">)</span>
<span class="w">    </span>libpcre.so.1<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libpcre.so.1<span class="w"> </span><span class="o">(</span>0x00007ffef4abe000<span class="o">)</span>
*************************************
</pre></div>
</div>
<p>Notice that the libraries are sent to the <code class="docutils literal notranslate"><span class="pre">${exe}_libs</span></code> directory in the same prefix as the executable.
Once libraries are here, you cannot tell where they came from, so consider doing an <code class="docutils literal notranslate"><span class="pre">ldd</span></code> of your executable prior to <code class="docutils literal notranslate"><span class="pre">sbcast</span></code>.</p>
</section>
<section id="alternative-sbcasting-a-binary-with-all-libraries">
<h5>Alternative: SBCASTing a binary with all libraries<a class="headerlink" href="#alternative-sbcasting-a-binary-with-all-libraries" title="Link to this heading"></a></h5>
<p>As mentioned above, you can use <code class="docutils literal notranslate"><span class="pre">--exclude=NONE</span></code> on <code class="docutils literal notranslate"><span class="pre">sbcast</span></code> to send all libraries along with the binary.
Using <code class="docutils literal notranslate"><span class="pre">--exclude=NONE</span></code> requires more effort but substantially simplifies the linker configuration at run-time.
A job script for the previous example, modified for sending all libraries is shown below.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="c1">#SBATCH -A &lt;projid&gt;</span>
<span class="c1">#SBATCH -J sbcast_binary_to_nvme</span>
<span class="c1">#SBATCH -o %x-%j.out</span>
<span class="c1">#SBATCH -t 00:05:00</span>
<span class="c1">#SBATCH -p batch</span>
<span class="c1">#SBATCH -N 2</span>
<span class="c1">#SBATCH -C nvme</span>

date

<span class="c1"># Change directory to user scratch space (Orion)</span>
<span class="nb">cd</span><span class="w"> </span>/lustre/orion/&lt;projid&gt;/scratch/&lt;userid&gt;

<span class="c1"># For this example, I use a HIP-enabled LAMMPS binary, with dependencies to MPI, HIP, and HWLOC</span>
<span class="nv">exe</span><span class="o">=</span><span class="s2">&quot;lmp&quot;</span>

<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;*****ldd ./</span><span class="si">${</span><span class="nv">exe</span><span class="si">}</span><span class="s2">*****&quot;</span>
ldd<span class="w"> </span>./<span class="si">${</span><span class="nv">exe</span><span class="si">}</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;*************************&quot;</span>

<span class="c1"># SBCAST executable from Orion to NVMe -- NOTE: ``-C nvme`` is needed in SBATCH headers to use the NVMe drive</span>
<span class="c1"># NOTE: dlopen&#39;d files will NOT be picked up by sbcast</span>
sbcast<span class="w"> </span>--send-libs<span class="w"> </span>--exclude<span class="o">=</span>NONE<span class="w"> </span>-pf<span class="w"> </span><span class="si">${</span><span class="nv">exe</span><span class="si">}</span><span class="w"> </span>/mnt/bb/<span class="nv">$USER</span>/<span class="si">${</span><span class="nv">exe</span><span class="si">}</span>
<span class="k">if</span><span class="w"> </span><span class="o">[</span><span class="w"> </span>!<span class="w"> </span><span class="s2">&quot;</span><span class="nv">$?</span><span class="s2">&quot;</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="s2">&quot;0&quot;</span><span class="w"> </span><span class="o">]</span><span class="p">;</span><span class="w"> </span><span class="k">then</span>
<span class="w">    </span><span class="c1"># CHECK EXIT CODE. When SBCAST fails, it may leave partial files on the compute nodes, and if you continue to launch srun,</span>
<span class="w">    </span><span class="c1"># your application may pick up partially complete shared library files, which would give you confusing errors.</span>
<span class="w">    </span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;SBCAST failed!&quot;</span>
<span class="w">    </span><span class="nb">exit</span><span class="w"> </span><span class="m">1</span>
<span class="k">fi</span>

<span class="c1"># Check to see if file exists</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;*****ls -lh /mnt/bb/</span><span class="nv">$USER</span><span class="s2">*****&quot;</span>
ls<span class="w"> </span>-lh<span class="w"> </span>/mnt/bb/<span class="nv">$USER</span>/
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;*****ls -lh /mnt/bb/</span><span class="nv">$USER</span><span class="s2">/</span><span class="si">${</span><span class="nv">exe</span><span class="si">}</span><span class="s2">_libs*****&quot;</span>
ls<span class="w"> </span>-lh<span class="w"> </span>/mnt/bb/<span class="nv">$USER</span>/<span class="si">${</span><span class="nv">exe</span><span class="si">}</span>_libs

<span class="c1"># SBCAST sends all libraries detected by `ld` (minus any excluded), and stores them in the same directory in each node&#39;s node-local storage</span>
<span class="c1"># Any libraries opened by `dlopen` are NOT sent, since they are not known by the linker at run-time.</span>

<span class="c1"># All required libraries now reside in /mnt/bb/$USER/${exe}_libs</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="s2">&quot;/mnt/bb/</span><span class="nv">$USER</span><span class="s2">/</span><span class="si">${</span><span class="nv">exe</span><span class="si">}</span><span class="s2">_libs&quot;</span>

<span class="c1"># libfabric dlopen&#39;s several libraries:</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">LD_LIBRARY_PATH</span><span class="si">}</span><span class="s2">:</span><span class="k">$(</span>pkg-config<span class="w"> </span>--variable<span class="o">=</span>libdir<span class="w"> </span>libfabric<span class="k">)</span><span class="s2">&quot;</span>

<span class="c1"># cray-mpich dlopen&#39;s libhsa-runtime64.so and libamdhip64.so (non-versioned), so symlink on each node:</span>
srun<span class="w"> </span>-N<span class="w"> </span><span class="si">${</span><span class="nv">SLURM_NNODES</span><span class="si">}</span><span class="w"> </span>-n<span class="w"> </span><span class="si">${</span><span class="nv">SLURM_NNODES</span><span class="si">}</span><span class="w"> </span>--ntasks-per-node<span class="o">=</span><span class="m">1</span><span class="w"> </span>--label<span class="w"> </span>-D<span class="w"> </span>/mnt/bb/<span class="nv">$USER</span>/<span class="si">${</span><span class="nv">exe</span><span class="si">}</span>_libs<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>bash<span class="w"> </span>-c<span class="w"> </span><span class="s2">&quot;if [ -f libhsa-runtime64.so.1 ]; then ln -s libhsa-runtime64.so.1 libhsa-runtime64.so; fi;</span>
<span class="s2">    if [ -f libamdhip64.so.5 ]; then ln -s libamdhip64.so.5 libamdhip64.so; fi&quot;</span>

<span class="c1"># RocBLAS has over 1,000 device libraries that may be `dlopen`&#39;d by RocBLAS during a run.</span>
<span class="c1"># It&#39;s impractical to SBCAST all of these, so you can set this path instead, if you use RocBLAS:</span>
<span class="c1">#export ROCBLAS_TENSILE_LIBPATH=${ROCM_PATH}/lib/rocblas/library</span>

<span class="c1"># You may notice that some libraries are still linked from /sw/crusher, even after SBCASTing.</span>
<span class="c1"># This is because the Spack-build modules use RPATH to find their dependencies. This behavior cannot be changed.</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;*****ldd /mnt/bb/</span><span class="nv">$USER</span><span class="s2">/</span><span class="si">${</span><span class="nv">exe</span><span class="si">}</span><span class="s2">*****&quot;</span>
ldd<span class="w"> </span>/mnt/bb/<span class="nv">$USER</span>/<span class="si">${</span><span class="nv">exe</span><span class="si">}</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;*************************************&quot;</span>
</pre></div>
</div>
<p>Some libraries still resolved to paths outside of <code class="docutils literal notranslate"><span class="pre">/mnt/bb</span></code>, and the reason for that is that the executable may have several paths in <code class="docutils literal notranslate"><span class="pre">RPATH</span></code>.</p>
</section>
</section>
</section>
</section>
<hr class="docutils" />
<section id="profiling-applications">
<h2>Profiling Applications<a class="headerlink" href="#profiling-applications" title="Link to this heading"></a></h2>
<section id="getting-started-with-the-hpe-performance-analysis-tools-pat">
<h3>Getting Started with the HPE Performance Analysis Tools (PAT)<a class="headerlink" href="#getting-started-with-the-hpe-performance-analysis-tools-pat" title="Link to this heading"></a></h3>
<p>The Performance Analysis Tools (PAT), formerly CrayPAT, are a suite of utilities that enable users to capture and analyze performance data generated during program execution. These tools provide an integrated infrastructure for measurement, analysis, and visualization of computation, communication, I/O, and memory utilization to help users optimize programs for faster execution and more efficient computing resource usage.</p>
<p>There are three programming interfaces available: (1) <code class="docutils literal notranslate"><span class="pre">Perftools-lite</span></code>, (2) <code class="docutils literal notranslate"><span class="pre">Perftools</span></code>, and (3) <code class="docutils literal notranslate"><span class="pre">Perftools-preload</span></code>.</p>
<p>Below are two examples that generate an instrumented executable using <code class="docutils literal notranslate"><span class="pre">Perftools</span></code>, which is an advanced interface that provides full-featured data collection and analysis capability, including full traces with timeline displays.</p>
<p>The first example generates an instrumented executable using a <code class="docutils literal notranslate"><span class="pre">PrgEnv-amd</span></code> build:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>module<span class="w"> </span>load<span class="w"> </span>PrgEnv-amd
module<span class="w"> </span>load<span class="w"> </span>craype-accel-amd-gfx90a
module<span class="w"> </span>load<span class="w"> </span>rocm
module<span class="w"> </span>load<span class="w"> </span>perftools

<span class="nb">export</span><span class="w"> </span><span class="nv">PATH</span><span class="o">=</span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">PATH</span><span class="si">}</span><span class="s2">:</span><span class="si">${</span><span class="nv">ROCM_PATH</span><span class="si">}</span><span class="s2">/llvm/bin&quot;</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">CXX</span><span class="o">=</span><span class="s1">&#39;CC -x hip&#39;</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">CXXFLAGS</span><span class="o">=</span><span class="s1">&#39;-ggdb -O3 -std=c++17 Wall&#39;</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">LD</span><span class="o">=</span><span class="s1">&#39;CC&#39;</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">LDFLAGS</span><span class="o">=</span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">CXXFLAGS</span><span class="si">}</span><span class="s2"> -L</span><span class="si">${</span><span class="nv">ROCM_PATH</span><span class="si">}</span><span class="s2">/lib&quot;</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">LIBS</span><span class="o">=</span><span class="s1">&#39;-lamdhip64&#39;</span>

make<span class="w"> </span>clean
make

pat_build<span class="w"> </span>-g<span class="w"> </span>hip,io,mpi<span class="w"> </span>-w<span class="w"> </span>-f<span class="w"> </span>&lt;executable&gt;
</pre></div>
</div>
<p>The second example generates an instrumened executable using a <code class="docutils literal notranslate"><span class="pre">hipcc</span></code> build:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>module<span class="w"> </span>load<span class="w"> </span>perftools
module<span class="w"> </span>load<span class="w"> </span>craype-accel-amd-gfx90a
module<span class="w"> </span>load<span class="w"> </span>rocm

<span class="nb">export</span><span class="w"> </span><span class="nv">CXX</span><span class="o">=</span><span class="s1">&#39;hipcc&#39;</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">CXXFLAGS</span><span class="o">=</span><span class="s2">&quot;</span><span class="k">$(</span>pat_opts<span class="w"> </span>include<span class="w"> </span>hipcc<span class="k">)</span><span class="s2"> \</span>
<span class="s2">  </span><span class="k">$(</span>pat_opts<span class="w"> </span>pre_compile<span class="w"> </span>hipcc<span class="k">)</span><span class="s2"> -g -O3 -std=c++17 -Wall \</span>
<span class="s2">  --offload-arch=gfx90a -I</span><span class="si">${</span><span class="nv">CRAY_MPICH_DIR</span><span class="si">}</span><span class="s2">/include \</span>
<span class="s2">  </span><span class="k">$(</span>pat_opts<span class="w"> </span>post_compile<span class="w"> </span>hipcc<span class="k">)</span><span class="s2">&quot;</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">LD</span><span class="o">=</span><span class="s1">&#39;hipcc&#39;</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">LDFLAGS</span><span class="o">=</span><span class="s2">&quot;</span><span class="k">$(</span>pat_opts<span class="w"> </span>pre_link<span class="w"> </span>hipcc<span class="k">)</span><span class="s2"> </span><span class="si">${</span><span class="nv">CXXFLAGS</span><span class="si">}</span><span class="s2"> \</span>
<span class="s2">  -L</span><span class="si">${</span><span class="nv">CRAY_MPICH_DIR</span><span class="si">}</span><span class="s2">/lib </span><span class="si">${</span><span class="nv">PE_MPICH_GTL_DIR_amd_gfx908</span><span class="si">}</span><span class="s2">&quot;</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">LIBS</span><span class="o">=</span><span class="s2">&quot;-lmpi </span><span class="si">${</span><span class="nv">PE_MPICH_GTL_LIBS_amd_gfx908</span><span class="si">}</span><span class="s2"> \</span>
<span class="s2">  </span><span class="k">$(</span>pat_opts<span class="w"> </span>post_link<span class="w"> </span>hipcc<span class="k">)</span><span class="s2">&quot;</span>

make<span class="w"> </span>clean
make

pat_build<span class="w"> </span>-g<span class="w"> </span>hip,io,mpi<span class="w"> </span>-w<span class="w"> </span>-f<span class="w"> </span>&lt;executable&gt;
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">pat_build</span></code> command in the above examples generates an instrumented executable with <code class="docutils literal notranslate"><span class="pre">+pat</span></code> appended to the executable name (e.g., <code class="docutils literal notranslate"><span class="pre">hello_jobstep+pat</span></code>).</p>
<p>When run, the instrumented executable will trace HIP, I/O, MPI, and all user functions and generate a folder of results (e.g., <code class="docutils literal notranslate"><span class="pre">hello_jobstep+pat+39545-2t</span></code>).</p>
<p>To analyze these results, use the <code class="docutils literal notranslate"><span class="pre">pat_report</span></code> command, e.g.:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pat_report<span class="w"> </span>hello_jobstep+pat+39545-2t
</pre></div>
</div>
<p>The resulting report includes profiles of functions, profiles of maximum function times, details on load imbalance, details on program energy and power usages, details on memory high water mark, and more.</p>
<p>More detailed information on the HPE Performance Analysis Tools can be found in the <a class="reference external" href="https://support.hpe.com/hpesc/public/docDisplay?docLocale=en_US&amp;docId=a00123563en_us" target="_blank">HPE Performance Analysis Tools User Guide</a>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When using <code class="docutils literal notranslate"><span class="pre">perftools-lite-gpu</span></code>, there is a known issue causing <code class="docutils literal notranslate"><span class="pre">ld.lld</span></code> not to be found. A workaround this issue can be found <a class="reference external" href="https://docs.olcf.ornl.gov/systems/crusher_quick_start_guide.html#olcfdev-513-error-with-perftools-lite-gpu" target="_blank">here</a>.</p>
</div>
</section>
<section id="getting-started-with-hpctoolkit">
<h3>Getting Started with HPCToolkit<a class="headerlink" href="#getting-started-with-hpctoolkit" title="Link to this heading"></a></h3>
<p>HPCToolkit is an integrated suite of tools for measurement and analysis of program performance on computers ranging from multicore desktop systems to the nations largest supercomputers. HPCToolkit provides accurate measurements of a programs work, resource consumption, and inefficiency, correlates these metrics with the programs source code, works with multilingual, fully optimized binaries, has very low measurement overhead, and scales to large parallel systems. HPCToolkits measurements provide support for analyzing a program execution cost, inefficiency, and scaling characteristics both within and across nodes of a parallel system.</p>
<p>Programming models supported by HPCToolkit include MPI, OpenMP, OpenACC, CUDA, OpenCL, DPC++, HIP, RAJA, Kokkos, and others.</p>
<p>Below is an example that generates a profile and loads the results in their GUI-based viewer.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>module<span class="w"> </span>use<span class="w"> </span>/gpfs/alpine/csc322/world-shared/modulefiles/x86_64
module<span class="w"> </span>load<span class="w"> </span>hpctoolkit

<span class="c1"># 1. Profile and trace an application using CPU time and GPU performance counters</span>
srun<span class="w"> </span>&lt;srun_options&gt;<span class="w"> </span>hpcrun<span class="w"> </span>-o<span class="w"> </span>&lt;measurement_dir&gt;<span class="w"> </span>-t<span class="w"> </span>-e<span class="w"> </span>CPUTIME<span class="w"> </span>-e<span class="w"> </span><span class="nv">gpu</span><span class="o">=</span>amd<span class="w"> </span>&lt;application&gt;

<span class="c1"># 2. Analyze the binary of executables and its dependent libraries</span>
hpcstruct<span class="w"> </span>&lt;measurement_dir&gt;

<span class="c1"># 3. Combine measurements with program structure information and generate a database</span>
hpcprof<span class="w"> </span>-o<span class="w"> </span>&lt;database_dir&gt;<span class="w"> </span>&lt;measurement_dir&gt;

<span class="c1"># 4. Understand performance issues by analyzing profiles and traces with the GUI</span>
hpcviewer<span class="w"> </span>&lt;database_dir&gt;
</pre></div>
</div>
<p>More detailed information on HPCToolkit can be found in the <a class="reference external" href="http://hpctoolkit.org/manual/HPCToolkit-users-manual.pdf" target="_blank">HPCToolkit Users Manual</a>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>HPCToolkit does not require a recompile to profile the code. It is recommended to use the -g optimization flag for attribution to source lines.</p>
</div>
</section>
<section id="getting-started-with-the-rocm-profiler">
<h3>Getting Started with the ROCm Profiler<a class="headerlink" href="#getting-started-with-the-rocm-profiler" title="Link to this heading"></a></h3>
<p><code class="docutils literal notranslate"><span class="pre">rocprof</span></code> gathers metrics on kernels run on AMD GPU architectures. The profiler works for HIP kernels, as well as offloaded kernels from OpenMP target offloading, OpenCL, and abstraction layers such as Kokkos.
For a simple view of kernels being run, <code class="docutils literal notranslate"><span class="pre">rocprof</span> <span class="pre">--stats</span> <span class="pre">--timestamp</span> <span class="pre">on</span></code> is a great place to start.
With the <code class="docutils literal notranslate"><span class="pre">--stats</span></code> option enabled, <code class="docutils literal notranslate"><span class="pre">rocprof</span></code> will generate a file that is named <code class="docutils literal notranslate"><span class="pre">results.stats.csv</span></code> by default, but named <code class="docutils literal notranslate"><span class="pre">&lt;output&gt;.stats.csv</span></code> if the <code class="docutils literal notranslate"><span class="pre">-o</span></code> flag is supplied.
This file will list all kernels being run, the number of times they are run, the total duration and the average duration (in nanoseconds) of the kernel, and the GPU usage percentage.
More detailed infromation on <code class="docutils literal notranslate"><span class="pre">rocprof</span></code> profiling modes can be found at <a class="reference external" href="https://rocmdocs.amd.com/en/latest/ROCm_Tools/ROCm-Tools.html" target="_blank">ROCm Profiler</a> documentation.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you are using <code class="docutils literal notranslate"><span class="pre">sbcast</span></code>, you need to explicitly <code class="docutils literal notranslate"><span class="pre">sbcast</span></code> the AQL profiling library found in <code class="docutils literal notranslate"><span class="pre">${ROCM_PATH}/hsa-amd-aqlprofile/lib/libhsa-amd-aqlprofile64.so</span></code>.
A symbolic link to this library can also be found in <code class="docutils literal notranslate"><span class="pre">${ROCM_PATH}/lib</span></code>.
Alternatively, you may leave <code class="docutils literal notranslate"><span class="pre">${ROCM_PATH}/lib</span></code> in your <code class="docutils literal notranslate"><span class="pre">LD_LIBRARY_PATH</span></code>.</p>
</div>
</section>
<section id="roofline-profiling-with-the-rocm-profiler">
<h3>Roofline Profiling with the ROCm Profiler<a class="headerlink" href="#roofline-profiling-with-the-rocm-profiler" title="Link to this heading"></a></h3>
<p>The <a class="reference external" href="https://docs.nersc.gov/tools/performance/roofline/" target="_blank">Roofline</a> performance model is an increasingly popular way to demonstrate and understand application performance.
This section documents how to construct a simple roofline model for a single kernel using <code class="docutils literal notranslate"><span class="pre">rocprof</span></code>.
This roofline model is designed to be comparable to rooflines constructed by NVIDIAs <a class="reference external" href="https://developer.nvidia.com/blog/accelerating-hpc-applications-with-nsight-compute-roofline-analysis/" target="_blank">NSight Compute</a>.
A roofline model plots the achieved performance (in floating-point operations per second, FLOPS/s) as a function of arithmetic (or operational) intensity (in FLOPS per Byte).
The model detailed here calculates the bytes moved as they move to and from the GPUs HBM.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Integer instructions and cache levels are currently not documented here.</p>
</div>
<p>To get started, you will need to make an input file for <code class="docutils literal notranslate"><span class="pre">rocprof</span></code>, to be passed in through <code class="docutils literal notranslate"><span class="pre">rocprof</span> <span class="pre">-i</span> <span class="pre">&lt;input_file&gt;</span> <span class="pre">--timestamp</span> <span class="pre">on</span> <span class="pre">-o</span> <span class="pre">my_output.csv</span> <span class="pre">&lt;my_exe&gt;</span></code>.
Below is an example, and contains the information needed to roofline profile GPU 0, as seen by each rank:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pmc</span> <span class="p">:</span> <span class="n">TCC_EA_RDREQ_32B_sum</span> <span class="n">TCC_EA_RDREQ_sum</span> <span class="n">TCC_EA_WRREQ_sum</span> <span class="n">TCC_EA_WRREQ_64B_sum</span> <span class="n">SQ_INSTS_VALU_ADD_F16</span> <span class="n">SQ_INSTS_VALU_MUL_F16</span> <span class="n">SQ_INSTS_VALU_FMA_F16</span> <span class="n">SQ_INSTS_VALU_TRANS_F16</span> <span class="n">SQ_INSTS_VALU_ADD_F32</span> <span class="n">SQ_INSTS_VALU_MUL_F32</span> <span class="n">SQ_INSTS_VALU_FMA_F32</span> <span class="n">SQ_INSTS_VALU_TRANS_F32</span>
<span class="n">pmc</span> <span class="p">:</span> <span class="n">SQ_INSTS_VALU_ADD_F64</span> <span class="n">SQ_INSTS_VALU_MUL_F64</span> <span class="n">SQ_INSTS_VALU_FMA_F64</span> <span class="n">SQ_INSTS_VALU_TRANS_F64</span> <span class="n">SQ_INSTS_VALU_MFMA_MOPS_F16</span> <span class="n">SQ_INSTS_VALU_MFMA_MOPS_BF16</span> <span class="n">SQ_INSTS_VALU_MFMA_MOPS_F32</span> <span class="n">SQ_INSTS_VALU_MFMA_MOPS_F64</span>
<span class="n">gpu</span><span class="p">:</span> <span class="mi">0</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In an application with more than one kernel, you should strongly consider filtering by kernel name by adding a line like: <code class="docutils literal notranslate"><span class="pre">kernel:</span> <span class="pre">&lt;kernel_name&gt;</span></code> to the <code class="docutils literal notranslate"><span class="pre">rocprof</span></code> input file.</p>
</div>
<p>This provides the minimum set of metrics used to construct a roofline model, in the minimum number of passes.
Each line that begins with <code class="docutils literal notranslate"><span class="pre">pmc</span></code> indicates that the application will be re-run, and the metrics in that line will be collected.
<code class="docutils literal notranslate"><span class="pre">rocprof</span></code> can collect up to 8 counters from each block (<code class="docutils literal notranslate"><span class="pre">SQ</span></code>, <code class="docutils literal notranslate"><span class="pre">TCC</span></code>) in each application re-run.
To gather metrics across multiple MPI ranks, you will need to use a command that redirects the output of rocprof to a unique file for each task.
For example:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>srun<span class="w"> </span>-N<span class="w"> </span><span class="m">2</span><span class="w"> </span>-n<span class="w"> </span><span class="m">16</span><span class="w"> </span>--ntasks-per-node<span class="o">=</span><span class="m">8</span><span class="w"> </span>--gpus-per-node<span class="o">=</span><span class="m">8</span><span class="w"> </span>--gpu-bind<span class="o">=</span>closest<span class="w"> </span>bash<span class="w"> </span>-c<span class="w"> </span><span class="s1">&#39;rocprof -o ${SLURM_JOBID}_${SLURM_PROCID}.csv -i &lt;input_file&gt; --timestamp on &lt;exe&gt;&#39;</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The <code class="docutils literal notranslate"><span class="pre">gpu:</span></code> filter in the <code class="docutils literal notranslate"><span class="pre">rocprof</span></code> input file identifies GPUs by the number the MPI rank would see them as. In the <code class="docutils literal notranslate"><span class="pre">srun</span></code> example above,
each MPI rank only has 1 GPU, so each rank sees its GPU as GPU 0.</p>
</div>
<section id="theoretical-roofline">
<h4>Theoretical Roofline<a class="headerlink" href="#theoretical-roofline" title="Link to this heading"></a></h4>
<p>The theoretical (not attainable) peak roofline constructs a theoretical maximum performance for each operational intensity.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="docutils literal notranslate"><span class="pre">theoretical</span></code> peak is determined by the hardware specifications and is not attainable in practice. <code class="docutils literal notranslate"><span class="pre">attaiable</span></code> peak is the performance as measured by
in-situ microbenchmarks designed to best utilize the hardware. <code class="docutils literal notranslate"><span class="pre">achieved</span></code> performance is what the profiled application actually achieves.</p>
</div>
<p>The theoretical roofline can be constructed as:</p>
<div class="math notranslate nohighlight">
\[FLOPS_{peak} = minimum(ArithmeticIntensity * BW_{HBM}, TheoreticalFLOPS)\]</div>
<p>On Crusher, the memory bandwidth for HBM is 1.6 TB/s, and the theoretical peak floating-point FLOPS/s when using vector registers is calculated by:</p>
<div class="math notranslate nohighlight">
\[TheoreticalFLOPS = 128 FLOP/cycle/CU * 110 CU * 1700000000 cycles/second = 23.9 TFLOP/s\]</div>
<p>However, when using MFMA instructions, the theoretical peak floating-point FLOPS/s is calculated by:</p>
<div class="math notranslate nohighlight">
\[TheoreticalFLOPS = flop\_per\_cycle(precision) FLOP/cycle/CU * 110 CU * 1700000000 cycles/second\]</div>
<p>where <code class="docutils literal notranslate"><span class="pre">flop_per_cycle(precision)</span></code> is the published floating-point operations per clock cycle, per compute unit.
Those values are:</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Data Type</p></th>
<th class="head"><p>Flops/Clock/CU</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>FP64</p></td>
<td><p>256</p></td>
</tr>
<tr class="row-odd"><td><p>FP32</p></td>
<td><p>256</p></td>
</tr>
<tr class="row-even"><td><p>FP16</p></td>
<td><p>1024</p></td>
</tr>
<tr class="row-odd"><td><p>BF16</p></td>
<td><p>1024</p></td>
</tr>
<tr class="row-even"><td><p>INT8</p></td>
<td><p>1024</p></td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Attainable peak rooflines are constructed using microbenchmarks, and are not currently discussed here.
Attainable rooflines consider the limitations of cooling and power consumption and are more representative of what an application can achieve.</p>
</div>
</section>
<section id="achieved-flops-s">
<h4>Achieved FLOPS/s<a class="headerlink" href="#achieved-flops-s" title="Link to this heading"></a></h4>
<p>We calculate the achieved performance at the desired level (here, double-precision floating point, FP64), by summing each metric count and weighting the FMA metric by 2, since a fused multiply-add is considered 2 floating point operations.
Also note that these <code class="docutils literal notranslate"><span class="pre">SQ_INSTS_VALU_&lt;ADD,MUL,TRANS&gt;</span></code> metrics are reported as per-simd, so we mutliply by the wavefront size as well.
The <code class="docutils literal notranslate"><span class="pre">SQ_INSTS_VALU_MFMA_MOPS_*</span></code> instructions should be multiplied by the <code class="docutils literal notranslate"><span class="pre">Flops/Cycle/CU</span></code> value listed above.
We use this equation to calculate the number of double-precision FLOPS:</p>
<div class="math notranslate nohighlight">
\[\begin{split}FP64\_FLOPS =   64  *&amp;(SQ\_INSTS\_VALU\_ADD\_F64         \\\\
                     &amp;+ SQ\_INSTS\_VALU\_MUL\_F64       \\\\
                     &amp;+ SQ\_INSTS\_VALU\_TRANS\_F64     \\\\
                     &amp;+ 2 * SQ\_INSTS\_VALU\_FMA\_F64)  \\\\
              + 256 *&amp;(SQ\_INSTS\_VALU\_MFMA\_MOPS\_F64)\end{split}\]</div>
<p>When <code class="docutils literal notranslate"><span class="pre">SQ_INSTS_VALU_MFMA_MOPS_*_F64</span></code> instructions are used, then 47.8 TF/s is considered the theoretical maximum FLOPS/s.
If only <code class="docutils literal notranslate"><span class="pre">SQ_INSTS_VALU_&lt;ADD,MUL,TRANS&gt;</span></code> are found, then 23.9 TF/s is the theoretical maximum FLOPS/s.
Then, we divide the number of FLOPS by the elapsed time of the kernel to find FLOPS per second.
This is found from subtracting the <code class="docutils literal notranslate"><span class="pre">rocprof</span></code> metrics <code class="docutils literal notranslate"><span class="pre">EndNs</span></code> by <code class="docutils literal notranslate"><span class="pre">BeginNs</span></code>, provided by <code class="docutils literal notranslate"><span class="pre">--timestamp</span> <span class="pre">on</span></code>, then converting from nanoseconds to seconds by dividing by 1,000,000,000 (power(10,9)).</p>
<section id="calculating-for-all-precisions">
<h5>Calculating for all precisions<a class="headerlink" href="#calculating-for-all-precisions" title="Link to this heading"></a></h5>
<p>The above formula can be adapted to compute the total FLOPS across all floating-point precisions (<code class="docutils literal notranslate"><span class="pre">INT</span></code> excluded).</p>
<div class="math notranslate nohighlight">
\[\begin{split}TOTAL\_FLOPS =   64  *&amp;(SQ\_INSTS\_VALU\_ADD\_F16         \\\\
                     &amp;+ SQ\_INSTS\_VALU\_MUL\_F16       \\\\
                     &amp;+ SQ\_INSTS\_VALU\_TRANS\_F16     \\\\
                     &amp;+ 2 * SQ\_INSTS\_VALU\_FMA\_F16)  \\\\
              + 64  *&amp;(SQ\_INSTS\_VALU\_ADD\_F32         \\\\
                     &amp;+ SQ\_INSTS\_VALU\_MUL\_F32       \\\\
                     &amp;+ SQ\_INSTS\_VALU\_TRANS\_F32     \\\\
                     &amp;+ 2 * SQ\_INSTS\_VALU\_FMA\_F32)  \\\\
              + 64  *&amp;(SQ\_INSTS\_VALU\_ADD\_F64         \\\\
                     &amp;+ SQ\_INSTS\_VALU\_MUL\_F64       \\\\
                     &amp;+ SQ\_INSTS\_VALU\_TRANS\_F64     \\\\
                     &amp;+ 2 * SQ\_INSTS\_VALU\_FMA\_F64)  \\\\
              + 1024 &amp;*(SQ\_INSTS\_VALU\_MFMA\_MOPS\_F16) \\\\
              + 1024 &amp;*(SQ\_INSTS\_VALU\_MFMA\_MOPS\_BF16) \\\\
              + 256 *&amp;(SQ\_INSTS\_VALU\_MFMA\_MOPS\_F32) \\\\
              + 256 *&amp;(SQ\_INSTS\_VALU\_MFMA\_MOPS\_F64) \\\\\end{split}\]</div>
</section>
</section>
<section id="arithmetic-intensity">
<h4>Arithmetic Intensity<a class="headerlink" href="#arithmetic-intensity" title="Link to this heading"></a></h4>
<p>Arithmetic intensity calculates the ratio of FLOPS to bytes moved between HBM and L2 cache.
We calculated FLOPS above (FP64_FLOPS).
We can calculate the number of bytes moved using the <code class="docutils literal notranslate"><span class="pre">rocprof</span></code> metrics <code class="docutils literal notranslate"><span class="pre">TCC_EA_WRREQ_64B</span></code>, <code class="docutils literal notranslate"><span class="pre">TCC_EA_WRREQ_sum</span></code>, <code class="docutils literal notranslate"><span class="pre">TCC_EA_RDREQ_32B</span></code>, and <code class="docutils literal notranslate"><span class="pre">TCC_EA_RDREQ_sum</span></code>.
<code class="docutils literal notranslate"><span class="pre">TCC</span></code> refers to the L2 cache, and <code class="docutils literal notranslate"><span class="pre">EA</span></code> is the interface between L2 and HBM.
<code class="docutils literal notranslate"><span class="pre">WRREQ</span></code> and <code class="docutils literal notranslate"><span class="pre">RDREQ</span></code> are write-requests and read-requests, respectively.
Each of these requests is either 32 bytes or 64 bytes.
So we calculate the number of bytes traveling over the EA interface as:</p>
<div class="math notranslate nohighlight">
\[BytesMoved = BytesWritten + BytesRead\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[BytesWritten = 64 * TCC\_EA\_WRREQ\_64B\_sum + 32 * (TCC\_EA\_WRREQ\_sum - TCC\_EA\_WRREQ\_64B\_sum)\]</div>
<div class="math notranslate nohighlight">
\[BytesRead = 32 * TCC\_EA\_RDREQ\_32B\_sum + 64 * (TCC\_EA\_RDREQ\_sum - TCC\_EA\_RDREQ\_32B\_sum)\]</div>
</section>
</section>
<section id="omnitrace">
<h3>Omnitrace<a class="headerlink" href="#omnitrace" title="Link to this heading"></a></h3>
<p>OLCF provides installations of AMDs <a class="reference external" href="https://github.com/AMDResearch/omnitrace" target="_blank">Omnitrace</a> profiling tools on Frontier.
AMD provides documentation on the usage of Omnitrace at <a class="reference external" href="https://amdresearch.github.io/omnitrace/" target="_blank">https://amdresearch.github.io/omnitrace/</a>.
This section details the installation and common pitfalls of the <code class="docutils literal notranslate"><span class="pre">omnitrace</span></code> module on Frontier.</p>
<p>Unlike <code class="docutils literal notranslate"><span class="pre">omniperf</span></code>, the <code class="docutils literal notranslate"><span class="pre">omnitrace</span></code> module only relies on a ROCm module (<code class="docutils literal notranslate"><span class="pre">amd</span></code>, <code class="docutils literal notranslate"><span class="pre">amd-mixed</span></code>, or <code class="docutils literal notranslate"><span class="pre">rocm</span></code>).
A ROCm module must be loaded before being able to do view or load the <code class="docutils literal notranslate"><span class="pre">omnitrace</span></code> module.
As a rule of thumb, always load the <code class="docutils literal notranslate"><span class="pre">omnitrace</span></code> module last (especially after you load a ROCm module like <code class="docutils literal notranslate"><span class="pre">amd</span></code>, <code class="docutils literal notranslate"><span class="pre">rocm</span></code>, <code class="docutils literal notranslate"><span class="pre">amd-mixed</span></code>).
If you load a new version of ROCm, you will need to re-load <code class="docutils literal notranslate"><span class="pre">omnitrace</span></code>.</p>
<p>To use <code class="docutils literal notranslate"><span class="pre">omnitrace</span></code>, you may use the following commands</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">module</span> <span class="n">load</span> <span class="n">amd</span><span class="o">-</span><span class="n">mixed</span>
<span class="n">module</span> <span class="n">load</span> <span class="n">omnitrace</span>
</pre></div>
</div>
</section>
<section id="omniperf">
<h3>Omniperf<a class="headerlink" href="#omniperf" title="Link to this heading"></a></h3>
<p>OLCF provides installations of AMDs <a class="reference external" href="https://github.com/AMDResearch/omniperf" target="_blank">Omniperf</a> profiling tools on Frontier.
AMD provides documentation on the usage of Omniperf at <a class="reference external" href="https://amdresearch.github.io/omniperf/" target="_blank">https://amdresearch.github.io/omniperf/</a>.
This section details the installation and common pitfalls of the <code class="docutils literal notranslate"><span class="pre">omniperf</span></code> module on Frontier.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">omniperf</span></code> module relies on two other modules  a ROCm module (<code class="docutils literal notranslate"><span class="pre">amd</span></code>, <code class="docutils literal notranslate"><span class="pre">amd-mixed</span></code>, or <code class="docutils literal notranslate"><span class="pre">rocm</span></code>) and optionally a <code class="docutils literal notranslate"><span class="pre">cray-python</span></code> module.
A ROCm module must be loaded before being able to do view or load the <code class="docutils literal notranslate"><span class="pre">omniperf</span></code> module.
As for <code class="docutils literal notranslate"><span class="pre">cray-python</span></code>, <code class="docutils literal notranslate"><span class="pre">omniperf</span></code> is a Python script and has several dependencies that cannot be met by the systems default Python, and are not met by the default <code class="docutils literal notranslate"><span class="pre">cray-python</span></code> installation.
As such, you must either (1) load the <code class="docutils literal notranslate"><span class="pre">cray-python</span></code> module or (2) satisfy the Python dependencies in your own Python environment (ie, in a Conda environment).</p>
<p>As a rule of thumb, always load the <code class="docutils literal notranslate"><span class="pre">omniperf</span></code> module last (especially after you load a ROCm module like <code class="docutils literal notranslate"><span class="pre">amd</span></code>, <code class="docutils literal notranslate"><span class="pre">rocm</span></code>, <code class="docutils literal notranslate"><span class="pre">amd-mixed</span></code>).
If you load a new version of ROCm, you will need to re-load <code class="docutils literal notranslate"><span class="pre">omniperf</span></code>.</p>
<section id="using-cray-python">
<h4>Using <code class="docutils literal notranslate"><span class="pre">cray-python</span></code><a class="headerlink" href="#using-cray-python" title="Link to this heading"></a></h4>
<p>To use <code class="docutils literal notranslate"><span class="pre">omniperf</span></code> with <code class="docutils literal notranslate"><span class="pre">cray-python</span></code>, you may use the following commands:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">module</span> <span class="n">load</span> <span class="n">amd</span><span class="o">-</span><span class="n">mixed</span>
<span class="n">module</span> <span class="n">load</span> <span class="n">cray</span><span class="o">-</span><span class="n">python</span>
<span class="n">module</span> <span class="n">load</span> <span class="n">omniperf</span>
</pre></div>
</div>
<p>No more work is needed on your part  <code class="docutils literal notranslate"><span class="pre">omniperf</span></code> points to a directory that contains pre-built libraries for the <code class="docutils literal notranslate"><span class="pre">cray-python</span></code> version you are running.
It is <strong>critically</strong> important that if you load a different version of ROCm or <code class="docutils literal notranslate"><span class="pre">cray-python</span></code> that you re-load <code class="docutils literal notranslate"><span class="pre">omniperf</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Omniperf requires relatively new versions of many dependencies.
Installing dependencies may break some currently installed packages that require older versions of the dependencies.
It is recommended that you use the newest <code class="docutils literal notranslate"><span class="pre">cray-python</span></code> modules available.</p>
</div>
</section>
<section id="using-your-own-python">
<h4>Using your own Python<a class="headerlink" href="#using-your-own-python" title="Link to this heading"></a></h4>
<p>To use <code class="docutils literal notranslate"><span class="pre">omniperf</span></code> with your own Python installation, you must first install the dependencies of Omniperf in your Pythons environment.
To do so, use the <code class="docutils literal notranslate"><span class="pre">requirements.txt</span></code> file in the <a class="reference external" href="https://github.com/AMDResearch/omniperf" target="_blank">Omniperf GitHub Repo</a>.
You may install the dependencies using a command like:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">python3</span> <span class="o">-</span><span class="n">m</span> <span class="n">pip</span> <span class="n">install</span> <span class="o">-</span><span class="n">r</span> <span class="n">requirements</span><span class="o">.</span><span class="n">txt</span>
</pre></div>
</div>
<p>Once you have installed the dependencies, you may load <code class="docutils literal notranslate"><span class="pre">omniperf</span></code> using commands like:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Your Python environment should be active by this point</span>
<span class="n">module</span> <span class="n">load</span> <span class="n">amd</span><span class="o">-</span><span class="n">mixed</span>
<span class="n">module</span> <span class="n">load</span> <span class="n">omniperf</span>
</pre></div>
</div>
<p>Again, it is <strong>critically</strong> important that if you load a different version of ROCm that you re-load <code class="docutils literal notranslate"><span class="pre">omniperf</span></code>.</p>
</section>
</section>
</section>
<hr class="docutils" />
<section id="notable-differences-between-summit-and-crusher">
<h2>Notable Differences between Summit and Crusher<a class="headerlink" href="#notable-differences-between-summit-and-crusher" title="Link to this heading"></a></h2>
<p>This section details tips and tricks and information of interest to users when porting from Summit to Crusher.</p>
<section id="using-reduced-precision-fp16-and-bf16-datatypes">
<h3>Using reduced precision (FP16 and BF16 datatypes)<a class="headerlink" href="#using-reduced-precision-fp16-and-bf16-datatypes" title="Link to this heading"></a></h3>
<p>Users leveraging BF16 and FP16 datatypes for applications such as ML/AI training and low-precision matrix multiplication should be aware that the AMD MI250X GPU has different denormal handling than the V100 GPUs on Summit. On the MI250X, the V_DOT2 and the matrix instructions for FP16 and BF16 flush input and output denormal values to zero.FP32 and FP64 MFMA instructions do not flush input and output denormal values to zero.</p>
<p>When training deep learning models using FP16 precision, some models may fail to converge with FP16 denorms flushed to zero. This occurs in operations encountering denormal values, and so is more likely to occur in FP16 because of a small dynamic range. BF16 numbers have a larger dynamic range than FP16 numbers and are less likely to encounter denormal values.</p>
<p>AMD has provided a solution in ROCm 5.0 which modifies the behavior of Tensorflow, PyTorch, and rocBLAS. This modification starts with FP16 input values, casting the intermediate FP16 values to BF16, and then casting back to FP16 output after the accumulate FP32 operations. In this way, the input and output types are unchanged. The behavior is enabled by default in machine learning frameworks. This behavior requires user action in rocBLAS, via a special enum type. For more information, see the rocBLAS link below.</p>
<p>If you encounter significant differences when running using reduced precision, explore replacing non-converging models in FP16 with BF16, because of the greater dynamic range in BF16. We recommend using BF16 for ML models in general. If you have further questions or encounter issues, contact <a class="reference external" href="mailto:help&#37;&#52;&#48;olcf&#46;ornl&#46;gov" target="_blank">help<span>&#64;</span>olcf<span>&#46;</span>ornl<span>&#46;</span>gov</a>.</p>
<dl class="simple">
<dt>Additional information on MI250X reduced precision can be found at:</dt><dd><ul class="simple">
<li><p>The MI250X ISA specification details the flush to zero denorm behavior at: <a class="reference external" href="https://developer.amd.com/wp-content/resources/CDNA2_Shader_ISA_18November2021.pdf" target="_blank">https://developer.amd.com/wp-content/resources/CDNA2_Shader_ISA_18November2021.pdf</a> (See page 41 and 46)</p></li>
<li><p>AMD rocBLAS library reference guide details this behavior at: <a class="reference external" href="https://rocblas.readthedocs.io/en/master/API_Reference_Guide.html#mi200-gfx90a-considerations" target="_blank">https://rocblas.readthedocs.io/en/master/API_Reference_Guide.html#mi200-gfx90a-considerations</a></p></li>
</ul>
</dd>
</dl>
</section>
<section id="enabling-gpu-page-migration">
<h3>Enabling GPU Page Migration<a class="headerlink" href="#enabling-gpu-page-migration" title="Link to this heading"></a></h3>
<p>The AMD MI250X and operating system on Crusher supports unified virtual addressing across the entire host and device memory, and automatic page migration between CPU and GPU memory. Migratable, universally addressable memory is sometimes called managed or unified memory, but neither of these terms fully describes how memory may behave on Crusher. In the following section well discuss how the heterogenous memory space on a Crusher node is surfaced within your application.</p>
<p>The accessibility of memory from GPU kernels and whether pages may migrate depends three factors: how the memory was allocated; the XNACK operating mode of the GPU; whether the kernel was compiled to support page migration. The latter two factors are intrinsically linked, as the MI250X GPU operating mode restricts the types of kernels which may run.</p>
<p>XNACK (pronounced X-knack) refers to the AMD GPUs ability to retry memory accesses that fail due to a page fault. The XNACK mode of an MI250X can be changed by setting the environment variable <code class="docutils literal notranslate"><span class="pre">HSA_XNACK</span></code> before starting a process that uses the GPU. Valid values are 0 (disabled) and 1 (enabled), and all processes connected to a GPU must use the same XNACK setting. The default MI250X on Crusher is <code class="docutils literal notranslate"><span class="pre">HSA_XNACK=0</span></code>.</p>
<p>If <code class="docutils literal notranslate"><span class="pre">HSA_XNACK=0</span></code>, page faults in GPU kernels are not handled and will terminate the kernel. Therefore all memory locations accessed by the GPU must either be resident in the GPU HBM or mapped by the HIP runtime. Memory regions may be migrated between the host DDR4 and GPU HBM using explicit HIP library functions such as <code class="docutils literal notranslate"><span class="pre">hipMemAdvise</span></code> and <code class="docutils literal notranslate"><span class="pre">hipPrefetchAsync</span></code>, but memory will not be automatically migrated based on access patterns alone.</p>
<p>If <code class="docutils literal notranslate"><span class="pre">HSA_XNACK=1</span></code>, page faults in GPU kernels will trigger a page table lookup. If the memory location can be made accessible to the GPU, either by being migrated to GPU HBM or being mapped for remote access, the appropriate action will occur and the access will be replayed. Page migration  will happen between CPU DDR4 and GPU HBM according to page touch. The exceptions are if the programmer uses a HIP library call such as <code class="docutils literal notranslate"><span class="pre">hipPrefetchAsync</span></code> to request migration, or if a preferred location is set via <code class="docutils literal notranslate"><span class="pre">hipMemAdvise</span></code>, or if GPU HBM becomes full and the page must forcibly be evicted back to CPU DDR4 to make room for other data.</p>
<section id="migration-of-memory-by-allocator-and-xnack-mode">
<h4>Migration of Memory by Allocator and XNACK Mode<a class="headerlink" href="#migration-of-memory-by-allocator-and-xnack-mode" title="Link to this heading"></a></h4>
<p>Most applications that use managed or unified memory on other platforms will want to enable XNACK to take advantage of automatic page migration on Crusher. The following table shows how common allocators currently behave with XNACK enabled. The behavior of a specific memory region may vary from the default if the programmer uses certain API calls.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The page migration behavior summarized by the following tables represents the current, observable behavior. Said behavior will likely change in the near future.</p>
<blockquote>
<div></div></blockquote>
</div>
<p><code class="docutils literal notranslate"><span class="pre">HSA_XNACK=1</span></code> <strong>Automatic Page Migration Enabled</strong></p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Allocator</p></th>
<th class="head"><p>Initial Physical Location</p></th>
<th class="head"><p>CPU Access after GPU First Touch</p></th>
<th class="head"><p>Default Behavior for GPU Access</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>System Allocator (malloc,new,allocate, etc)</p></td>
<td><p>CPU DDR4</p></td>
<td><p>Migrate to CPU DDR4 on touch</p></td>
<td><p>Migrate to GPU HBM on touch</p></td>
</tr>
<tr class="row-odd"><td><p>hipMallocManaged</p></td>
<td><p>CPU DDR4</p></td>
<td><p>Migrate to CPU DDR4 on touch</p></td>
<td><p>Migrate to GPU HBM on touch</p></td>
</tr>
<tr class="row-even"><td><p>hipHostMalloc</p></td>
<td><p>CPU DDR4</p></td>
<td><p>Local read/write</p></td>
<td><p>Zero copy read/write over Infinity Fabric</p></td>
</tr>
<tr class="row-odd"><td><p>hipMalloc</p></td>
<td><p>GPU HBM</p></td>
<td><p>Zero copy read/write over Inifinity Fabric</p></td>
<td><p>Local read/write</p></td>
</tr>
</tbody>
</table>
<p>Disabling XNACK will not necessarily result in an application failure, as most types of memory can still be accessed by the AMD Optimized 3rd Gen EPYC CPU and AMD MI250X GPU. In most cases, however, the access will occur in a zero-copy fashion over the Infinity Fabric. The exception is memory allocated through standard system allocators such as <code class="docutils literal notranslate"><span class="pre">malloc</span></code>, which cannot be accessed directly from GPU kernels without previously being registered via a HIP runtime call such as <code class="docutils literal notranslate"><span class="pre">hipHostRegister</span></code>. Access to malloced and unregistered memory from GPU kernels will result in fatal unhandled page faults. The table below shows how common allocators behave with XNACK disabled.</p>
<p><code class="docutils literal notranslate"><span class="pre">HSA_XNACK=0</span></code> <strong>Automatic Page Migration Disabled</strong></p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Allocator</p></th>
<th class="head"><p>Initial Physical Location</p></th>
<th class="head"><p>Default Behavior for CPU Access</p></th>
<th class="head"><p>Default Behavior for GPU Access</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>System Allocator (malloc,new,allocate, etc)</p></td>
<td><p>CPU DDR4</p></td>
<td><p>Local read/write</p></td>
<td><p>Fatal Unhandled Page Fault</p></td>
</tr>
<tr class="row-odd"><td><p>hipMallocManaged</p></td>
<td><p>CPU DDR4</p></td>
<td><p>Local read/write</p></td>
<td><p>Zero copy read/write over Infinity Fabric</p></td>
</tr>
<tr class="row-even"><td><p>hipHostMalloc</p></td>
<td><p>CPU DDR4</p></td>
<td><p>Local read/write</p></td>
<td><p>Zero copy read/write over Infinity Fabric</p></td>
</tr>
<tr class="row-odd"><td><p>hipMalloc</p></td>
<td><p>GPU HBM</p></td>
<td><p>Zero copy read/write over Inifinity Fabric</p></td>
<td><p>Local read/write</p></td>
</tr>
</tbody>
</table>
</section>
<section id="compiling-hip-kernels-for-specific-xnack-modes">
<h4>Compiling HIP kernels for specific XNACK modes<a class="headerlink" href="#compiling-hip-kernels-for-specific-xnack-modes" title="Link to this heading"></a></h4>
<p>Although XNACK is a capability of the MI250X GPU, it does require that kernels be able to recover from page faults. Both the ROCm and CCE HIP compilers will default to generating code that runs correctly with both XNACK enabled and disabled. Some applications may benefit from using the following compilation options to target specific XNACK modes.</p>
<div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">hipcc</span> <span class="pre">--amdgpu-target=gfx90a</span></code> or <code class="docutils literal notranslate"><span class="pre">CC</span> <span class="pre">--offload-arch=gfx90a</span> <span class="pre">-x</span> <span class="pre">hip</span></code></div>
<div class="line-block">
<div class="line">Kernels are compiled to a single xnack any binary, which will run correctly with both XNACK enabled and XNACK disabled.</div>
</div>
</div>
<div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">hipcc</span> <span class="pre">--amdgpu-target=gfx90a:xnack+</span></code> or <code class="docutils literal notranslate"><span class="pre">CC</span> <span class="pre">--offload-arch=gfx90a:xnack+</span> <span class="pre">-x</span> <span class="pre">hip</span></code></div>
<div class="line-block">
<div class="line">Kernels are compiled in xnack plus mode and will <em>only</em> be able to run on GPUs with <code class="docutils literal notranslate"><span class="pre">HSA_XNACK=1</span></code> to enable XNACK. Performance may be better than xnack any, but attempts to run with XNACK disabled will fail.</div>
</div>
</div>
<div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">hipcc</span> <span class="pre">--amdgpu-target=gfx90a:xnack-</span></code> or <code class="docutils literal notranslate"><span class="pre">CC</span> <span class="pre">--offload-arch=gfx90a:xnack-</span> <span class="pre">-x</span> <span class="pre">hip</span></code></div>
<div class="line-block">
<div class="line">Kernels are compiled in xnack minus mode and will <em>only</em> be able to run on GPUs with <code class="docutils literal notranslate"><span class="pre">HSA_XNACK=0</span></code> and XNACK disabled. Performance may be better than xnack any, but attempts to run with XNACK enabled will fail.</div>
</div>
</div>
<div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">hipcc</span> <span class="pre">--amdgpu-target=gfx90a:xnack-</span> <span class="pre">--amdgpu-target=gfx90a:xnack+</span> <span class="pre">-x</span> <span class="pre">hip</span></code> or <code class="docutils literal notranslate"><span class="pre">CC</span> <span class="pre">--offload-arch=gfx90a:xnack-</span> <span class="pre">--offload-arch=gfx90a:xnack+</span> <span class="pre">-x</span> <span class="pre">hip</span></code></div>
<div class="line-block">
<div class="line">Two versions of each kernel will be generated, one that runs with XNACK disabled and one that runs if XNACK is enabled. This is different from xnack any in that two versions of each kernel are compiled and HIP picks the appropriate one at runtime, rather than there being a single version compatible with both. A fat binary compiled in this way will have the same performance of xnack+ with <code class="docutils literal notranslate"><span class="pre">HSA_XNACK=1</span></code> and as xnack- with <code class="docutils literal notranslate"><span class="pre">HSA_XNACK=0</span></code>, but the final executable will be larger since it contains two copies of every kernel.</div>
</div>
</div>
<p>If the HIP runtime cannot find a kernel image that matches the XNACK mode of the device, it will fail with <code class="docutils literal notranslate"><span class="pre">hipErrorNoBinaryForGpu</span></code>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ HSA_XNACK=0 srun -n 1 -N 1 -t 1 ./xnack_plus.exe
&quot;hipErrorNoBinaryForGpu: Unable to find code object for all current devices!&quot;
srun: error: crusher002: task 0: Aborted
srun: launch/slurm: _step_signal: Terminating StepId=74100.0
</pre></div>
</div>
<p>One way to diagnose <code class="docutils literal notranslate"><span class="pre">hipErrorNoBinaryForGpu</span></code> messages is to set the environment variable <code class="docutils literal notranslate"><span class="pre">AMD_LOG_LEVEL</span></code> to 1 or greater:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ AMD_LOG_LEVEL=1 HSA_XNACK=0 srun -n 1 -N 1 -t 1 ./xnack_plus.exe
:1:rocdevice.cpp            :1573: 43966598070 us: HSA_AMD_AGENT_INFO_SVM_DIRECT_HOST_ACCESS query failed.
:1:rocdevice.cpp            :1573: 43966598762 us: HSA_AMD_AGENT_INFO_SVM_DIRECT_HOST_ACCESS query failed.
:1:rocdevice.cpp            :1573: 43966599392 us: HSA_AMD_AGENT_INFO_SVM_DIRECT_HOST_ACCESS query failed.
:1:rocdevice.cpp            :1573: 43966599970 us: HSA_AMD_AGENT_INFO_SVM_DIRECT_HOST_ACCESS query failed.
:1:rocdevice.cpp            :1573: 43966600550 us: HSA_AMD_AGENT_INFO_SVM_DIRECT_HOST_ACCESS query failed.
:1:rocdevice.cpp            :1573: 43966601109 us: HSA_AMD_AGENT_INFO_SVM_DIRECT_HOST_ACCESS query failed.
:1:rocdevice.cpp            :1573: 43966601673 us: HSA_AMD_AGENT_INFO_SVM_DIRECT_HOST_ACCESS query failed.
:1:rocdevice.cpp            :1573: 43966602248 us: HSA_AMD_AGENT_INFO_SVM_DIRECT_HOST_ACCESS query failed.
:1:hip_code_object.cpp      :460 : 43966602806 us: hipErrorNoBinaryForGpu: Unable to find code object for all current devices!
:1:hip_code_object.cpp      :461 : 43966602810 us:   Devices:
:1:hip_code_object.cpp      :464 : 43966602811 us:     amdgcn-amd-amdhsa--gfx90a:sramecc+:xnack- - [Not Found]
:1:hip_code_object.cpp      :464 : 43966602811 us:     amdgcn-amd-amdhsa--gfx90a:sramecc+:xnack- - [Not Found]
:1:hip_code_object.cpp      :464 : 43966602812 us:     amdgcn-amd-amdhsa--gfx90a:sramecc+:xnack- - [Not Found]
:1:hip_code_object.cpp      :464 : 43966602813 us:     amdgcn-amd-amdhsa--gfx90a:sramecc+:xnack- - [Not Found]
:1:hip_code_object.cpp      :464 : 43966602813 us:     amdgcn-amd-amdhsa--gfx90a:sramecc+:xnack- - [Not Found]
:1:hip_code_object.cpp      :464 : 43966602814 us:     amdgcn-amd-amdhsa--gfx90a:sramecc+:xnack- - [Not Found]
:1:hip_code_object.cpp      :464 : 43966602814 us:     amdgcn-amd-amdhsa--gfx90a:sramecc+:xnack- - [Not Found]
:1:hip_code_object.cpp      :464 : 43966602815 us:     amdgcn-amd-amdhsa--gfx90a:sramecc+:xnack- - [Not Found]
:1:hip_code_object.cpp      :468 : 43966602816 us:   Bundled Code Objects:
:1:hip_code_object.cpp      :485 : 43966602817 us:     host-x86_64-unknown-linux - [Unsupported]
:1:hip_code_object.cpp      :483 : 43966602818 us:     hipv4-amdgcn-amd-amdhsa--gfx90a:xnack+ - [code object v4 is amdgcn-amd-amdhsa--gfx90a:xnack+]
&quot;hipErrorNoBinaryForGpu: Unable to find code object for all current devices!&quot;
srun: error: crusher129: task 0: Aborted
srun: launch/slurm: _step_signal: Terminating StepId=74102.0
</pre></div>
</div>
<p>The above log messages indicate the type of image required by each device, given its current mode (<code class="docutils literal notranslate"><span class="pre">amdgcn-amd-amdhsa--gfx90a:sramecc+:xnack-</span></code>) and the images found in the binary (<code class="docutils literal notranslate"><span class="pre">hipv4-amdgcn-amd-amdhsa--gfx90a:xnack+</span></code>).</p>
</section>
</section>
<hr class="docutils" />
<section id="floating-point-fp-atomic-operations-and-coarse-fine-grained-memory-allocations">
<h3>Floating-Point (FP) Atomic Operations and Coarse/Fine Grained Memory Allocations<a class="headerlink" href="#floating-point-fp-atomic-operations-and-coarse-fine-grained-memory-allocations" title="Link to this heading"></a></h3>
<p>The Crusher system, equipped with CDNA2-based architecture MI250X cards, offers a coherent host interface that enables advanced memory and unique cache coherency capabilities.
The AMD driver leverages the Heterogeneous Memory Management (HMM) support in the Linux kernel to perform seamless page migrations to/from CPU/GPUs.
This new capability comes with a memory model that needs to be understood completely to avoid unexpected behavior in real applications. For more details, please visit the previous section.</p>
<p>AMD GPUs can allocate two different types of memory locations: 1) Coarse grained and 2) Fine grained.</p>
<p><strong>Coarse grained</strong> memory is only guaranteed to be coherent outside of GPU kernels that modify it, enabling higher performance memory operations. Changes applied to coarse-grained memory by a GPU kernel are  only visible to the rest of the system (CPU or other GPUs) when the kernel has completed. A GPU kernel is only guaranteed to see changes applied to coarse grained memory by the rest of the system (CPU or other GPUs) if those changes were made before the kernel launched.</p>
<p><strong>Fine grained</strong> memory allows CPUs and GPUs to synchronize (via atomics) and coherently communicate with each other while the GPU kernel is running, allowing more advanced programming patterns. The additional visibility impacts the performance of fine grained allocated memory.</p>
<p>The fast hardware-based Floating point (FP) atomic operations available on MI250X are assumed to be working on coarse grained memory regions; when these instructions are applied to a fine-grained memory region, they will silently produce a no-op. To avoid returning incorrect results, the compiler never emits hardware-based FP atomics instructions by default, even when applied to coarse grained memory regions. Currently, users can use the <cite>-munsafe-fp-atomics</cite> flag to force the compiler to emit hardware-based FP atomics.
Using hardware-based FP atomics translates in a substantial performance improvement over the default choice.</p>
<p>Users applying floating point atomic operations (e.g., atomicAdd) on memory regions allocated via regular hipMalloc() can safely apply the <cite>-munsafe-fp-atomics</cite> flags to their codes to get the best possible performance and leverage hardware supported floating point atomics.
Atomic operations supported in hardware on non-FP datatypes  (e.g., INT32) will work correctly regardless of the nature of the memory region used.</p>
<p>In ROCm-5.1 and earlier versions, the flag <cite>-munsafe-fp-atomics</cite> is interpreted as a suggestion by the compiler, whereas from ROCm-5.2 the flag will always enforce the use of fast hardware-based FP atomics.</p>
<p>The following tables summarize the result granularity of various combinations of allocators, flags and arguments.</p>
<p>For <code class="docutils literal notranslate"><span class="pre">hipHostMalloc()</span></code>, the following table shows the nature of the memory returned based on the flag passed as argument.</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>API</p></th>
<th class="head"><p>Flag</p></th>
<th class="head"><p>Results</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>hipHostMalloc()</p></td>
<td><p>hipHostMallocDefault</p></td>
<td><p>Fine grained</p></td>
</tr>
<tr class="row-odd"><td><p>hipHostMalloc()</p></td>
<td><p>hipHostMallocNonCoherent</p></td>
<td><p>Coarse grained</p></td>
</tr>
</tbody>
</table>
<p>The following table shows the nature of the memory returned based on the flag passed as argument to <code class="docutils literal notranslate"><span class="pre">hipExtMallocWithFlags()</span></code>.</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>API</p></th>
<th class="head"><p>Flag</p></th>
<th class="head"><p>Result</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>hipExtMallocWithFlags()</p></td>
<td><p>hipDeviceMallocDefault</p></td>
<td><p>Coarse grained</p></td>
</tr>
<tr class="row-odd"><td><p>hipExtMallocWithFlags()</p></td>
<td><p>hipDeviceMallocFinegrained</p></td>
<td><p>Fine grained</p></td>
</tr>
</tbody>
</table>
<p>Finally, the following table summarizes the nature of the memory returned based on the flag passed as argument to <code class="docutils literal notranslate"><span class="pre">hipMallocManaged()</span></code> and the use of CPU regular <code class="docutils literal notranslate"><span class="pre">malloc()</span></code> routine with the possible use of <code class="docutils literal notranslate"><span class="pre">hipMemAdvise()</span></code>.</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>API</p></th>
<th class="head"><p>MemAdvice</p></th>
<th class="head"><p>Result</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>hipMallocManaged()</p></td>
<td></td>
<td><p>Fine grained</p></td>
</tr>
<tr class="row-odd"><td><p>hipMallocManaged()</p></td>
<td><p>hipMemAdvise (hipMemAdviseSetCoarseGrain)</p></td>
<td><p>Coarse grained</p></td>
</tr>
<tr class="row-even"><td><p>malloc()</p></td>
<td></td>
<td><p>Fine grained</p></td>
</tr>
<tr class="row-odd"><td><p>malloc()</p></td>
<td><p>hipMemAdvise (hipMemAdviseSetCoarseGrain)</p></td>
<td><p>Coarse grained</p></td>
</tr>
</tbody>
</table>
</section>
<section id="performance-considerations-for-lds-fp-atomicadd">
<h3>Performance considerations for LDS FP atomicAdd()<a class="headerlink" href="#performance-considerations-for-lds-fp-atomicadd" title="Link to this heading"></a></h3>
<p>Hardware FP atomic operations performed in LDS memory are usually always faster than an equivalent CAS loop, in particular when contention on LDS memory locations is high.
Because of a hardware design choice, FP32 LDS atomicAdd() operations can be slower than equivalent FP64 LDS atomicAdd(), in particular when contention on memory locations is low (e.g. random access pattern).
The aforementioned behavior is only true for FP atomicAdd() operations. Hardware atomic operations for CAS/Min/Max on FP32 are usually faster than the FP64 counterparts.
In cases when contention is very low, a FP32 CAS loop implementing an atomicAdd() operation could be faster than an hardware FP32 LDS atomicAdd().
Applications using single precision FP atomicAdd() are encouraged to experiment with the use of double precision to evaluate the trade-off between high atomicAdd() performance vs. potential lower occupancy due to higher LDS usage.</p>
</section>
</section>
<hr class="docutils" />
<section id="system-updates">
<h2>System Updates<a class="headerlink" href="#system-updates" title="Link to this heading"></a></h2>
<section id="id6">
<h3>2024-03-19<a class="headerlink" href="#id6" title="Link to this heading"></a></h3>
<p>On Tuesday, March 19, 2024, Frontiers system software was upgraded to Slingshot 2.1.1 and Slingshot Host Software 2.1.2. If you encounter any issues or have questions, please contact <a class="reference external" href="mailto:help&#37;&#52;&#48;olcf&#46;ornl&#46;gov" target="_blank">help<span>&#64;</span>olcf<span>&#46;</span>ornl<span>&#46;</span>gov</a>.</p>
</section>
<section id="id7">
<h3>2024-01-23<a class="headerlink" href="#id7" title="Link to this heading"></a></h3>
<p>On Tuesday, January 23, 2024, Crushers system software was upgraded. The following changes took place:</p>
<ul class="simple">
<li><p>ROCm 6.0.0 is now available via the <code class="docutils literal notranslate"><span class="pre">rocm/6.0.0</span></code> modulefile.</p></li>
<li><p>HPE/Cray Programming Environment (PE) 23.12 is now available via the <code class="docutils literal notranslate"><span class="pre">cpe/23.12</span></code> modulefile.</p></li>
<li><p>ROCm 5.3.0 and HPE/Cray PE 22.12 remain as default.</p></li>
<li><p>The system was upgraded to AMD GPU 6.3.6 device driver (ROCm 6.0.0 release).</p></li>
</ul>
<p>Please note that target default versions will be updated to PE 23.12 and ROCm 5.7.1 in the near future. Users are encouraged to try both and report any issues to <a class="reference external" href="mailto:help&#37;&#52;&#48;olcf&#46;ornl&#46;gov" target="_blank">help<span>&#64;</span>olcf<span>&#46;</span>ornl<span>&#46;</span>gov</a>.</p>
</section>
<section id="id8">
<h3>2023-12-05<a class="headerlink" href="#id8" title="Link to this heading"></a></h3>
<p>On Tuesday, December 5, 2023, Crushers system software was upgraded. The following changes took place:</p>
<ul class="simple">
<li><p>ROCm 5.7.1 is now available via the <code class="docutils literal notranslate"><span class="pre">rocm/5.7.1</span></code> modulefile.</p></li>
<li><p>Flux 0.56.0 is now available via the <code class="docutils literal notranslate"><span class="pre">flux/0.56.0</span></code> modulefile.</p></li>
</ul>
</section>
<section id="id9">
<h3>2023-10-03<a class="headerlink" href="#id9" title="Link to this heading"></a></h3>
<p>On Tuesday, October 3, 2023, Crushers system software was upgraded. The following changes took place:</p>
<ul class="simple">
<li><p>The system was to the AMD GPU 6.1.5 device driver (ROCm 5.6.1 release).</p></li>
<li><p>Slurm was upgraded to version 23.02.5</p></li>
</ul>
</section>
<section id="id10">
<h3>2023-09-19<a class="headerlink" href="#id10" title="Link to this heading"></a></h3>
<p>On Tuesday, September 19, 2023, Crushers system software was upgraded. The following changes took place:</p>
<ul class="simple">
<li><p>The system was upgraded to Slingshot Host Software 2.1.0.</p></li>
<li><p>ROCm 5.6.0 and 5.7.0 are now available via the <code class="docutils literal notranslate"><span class="pre">rocm/5.6.0</span></code> and <code class="docutils literal notranslate"><span class="pre">rocm/5.7.0</span></code> modulefiles, respectively.</p></li>
<li><p>HPE/Cray Programming Environments (PE) 23.09 is now available via the <code class="docutils literal notranslate"><span class="pre">cpe/23.09</span></code> modulefile.</p></li>
<li><p>ROCm 5.3.0 and HPE/Cray PE 22.12 remain as default.</p></li>
</ul>
</section>
<section id="id11">
<h3>2023-07-18<a class="headerlink" href="#id11" title="Link to this heading"></a></h3>
<p>On Tuesday, July 18, 2023, the Crusher TDS was upgraded to a new version of the system software stack. During the upgrade, the following changes took place:</p>
<ul class="simple">
<li><p>The system was upgraded to Cray OS 2.5, Slingshot Host Software 2.0.2-112, and the AMD GPU 6.0.5 device driver (ROCm 5.5.1 release).</p></li>
<li><p>ROCm 5.5.1 is now available via the <code class="docutils literal notranslate"><span class="pre">rocm/5.5.1</span></code> modulefile.</p></li>
<li><p>HPE/Cray Programming Environments (PE) 23.05 is now available via the <code class="docutils literal notranslate"><span class="pre">cpe/23.05</span></code> modulefile.</p></li>
<li><p>HPE/Cray PE 23.05 introduces support for ROCm 5.5.1. However, due to issues identified during testing, ROCm 5.3.0 and HPE/Cray PE 22.12 remain as default.</p></li>
</ul>
</section>
<section id="id12">
<h3>2023-04-05<a class="headerlink" href="#id12" title="Link to this heading"></a></h3>
<p>On Wednesday, April 5, 2023, the Crusher TDS was upgraded to a new software stack.  A summary of the changes is included below.</p>
<ul class="simple">
<li><p>The operating system was upgraded to Cray OS 2.4 based on SLES 15.4.</p></li>
<li><dl class="simple">
<dt>HPE/Cray Programming Environment (PE):</dt><dd><ul>
<li><dl class="simple">
<dt>PE 22.12 was installed and is now default. This PE includes the following components:</dt><dd><ul>
<li><p>Cray MPICH 8.1.23</p></li>
<li><p>Cray Libsci 22.12.1.1</p></li>
<li><p>CCE 15.0.0</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>PE 23.03 is now also available and includes:</dt><dd><ul>
<li><p>Cray MPICH 8.1.25</p></li>
<li><p>Cray Libsci 23.02.1.1</p></li>
<li><p>CCE 15.0.1</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>ROCm:</dt><dd><ul>
<li><p>ROCm 5.3.0 is now default.</p></li>
<li><p>ROCm 5.4.0 and 5.4.3 are available.</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>File Systems:</dt><dd><ul>
<li><p>The Orion Lustre parallel file system is now available on Crusher.</p></li>
<li><p>The Alpine GPFS file system remains available but will be permanently unmounted from Crusher on Tuesday, April 18, 2023. Please begin moving your data to the Orion file system as soon as possible.</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</section>
<section id="id13">
<h3>2022-12-29<a class="headerlink" href="#id13" title="Link to this heading"></a></h3>
<p>On Thursday, December 29, 2022 the following system configuration settings will be updated on Crusher:</p>
<blockquote>
<div><ul class="simple">
<li><p>Low-Noise Mode will be enabled: as a result, system processes will be constrained to core 0 on every node.</p></li>
<li><p>Slurms core specialization default will change: Slurm <code class="docutils literal notranslate"><span class="pre">--core-spec</span></code> or <code class="docutils literal notranslate"><span class="pre">-S</span></code> value will be set to 8. This will provide a symmetric distribution of cores per GCD to the application and will reserve one core per L3 cache region. After the outage, the default number of cores available to each GCD on a node will be 7. To change from the new default value, you can set <code class="docutils literal notranslate"><span class="pre">--core-spec</span></code> or <code class="docutils literal notranslate"><span class="pre">-S</span></code> in your job submission.</p></li>
<li><p>The default NIC mapping will be updated to <code class="docutils literal notranslate"><span class="pre">MPICH_OFI_NIC_POLICY=NUMA</span></code> to address known issues described in <a class="reference external" href="https://docs.olcf.ornl.gov/systems/crusher_quick_start_guide.html#olcfdev-1292-crusher-default-nic-binding-is-not-ideal" target="_blank">OLCFDEV-192</a> and <a class="reference external" href="https://docs.olcf.ornl.gov/systems/crusher_quick_start_guide.html#olcfdev-1366-ofi-poll-failed-errors-with-gpu-aware-mpi" target="_blank">OLCFDEV-1366</a>.</p></li>
</ul>
</div></blockquote>
</section>
</section>
<hr class="docutils" />
<section id="getting-help">
<h2>Getting Help<a class="headerlink" href="#getting-help" title="Link to this heading"></a></h2>
<p>If you have problems or need helping running on Crusher, please submit a ticket
by emailing <a class="reference external" href="mailto:help&#37;&#52;&#48;olcf&#46;ornl&#46;gov" target="_blank">help<span>&#64;</span>olcf<span>&#46;</span>ornl<span>&#46;</span>gov</a>.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="spock_quick_start_guide.html" class="btn btn-neutral float-left" title="Spock Quick-Start Guide" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../services_and_applications/index.html" class="btn btn-neutral float-right" title="Services and Applications" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, OLCF.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>  



</body>
</html>