<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Frontier User Guide &mdash; OLCF User Documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=19f00094" />
      <link rel="stylesheet" type="text/css" href="../_static/graphviz.css?v=eafc0fe6" />
      <link rel="stylesheet" type="text/css" href="../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
      <link rel="stylesheet" type="text/css" href="../_static/sphinx-data-viewer/jsonview.bundle.css?v=f6ef2277" />
      <link rel="stylesheet" type="text/css" href="../_static/sphinx-needs/libs/html/datatables.min.css?v=4b4fd840" />
      <link rel="stylesheet" type="text/css" href="../_static/sphinx-needs/common_css/need_style.css?v=678fb11e" />
      <link rel="stylesheet" type="text/css" href="../_static/sphinx-needs/common_css/need_core.css?v=f5b60a78" />
      <link rel="stylesheet" type="text/css" href="../_static/sphinx-needs/common_css/needstable.css?v=5e1b6797" />
      <link rel="stylesheet" type="text/css" href="../_static/sphinx-needs/common_css/need_links.css?v=2150a916" />
      <link rel="stylesheet" type="text/css" href="../_static/sphinx-needs/common_css/need_toggle.css?v=5c6620df" />
      <link rel="stylesheet" type="text/css" href="../_static/sphinx-needs/modern.css?v=803738c0" />
      <link rel="stylesheet" type="text/css" href="https://cdn.datatables.net/1.13.4/css/jquery.dataTables.min.css" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme_overrides.css?v=b034643a" />

  
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="canonical" href="https://docs.olcf.ornl.govsystems/frontier_user_guide.html"/>
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../_static/jquery.js?v=5d32c60e"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="../_static/documentation_options.js?v=5929fcd5"></script>
        <script src="../_static/doctools.js?v=888ff710"></script>
        <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
        <script src="../_static/design-tabs.js?v=36754332"></script>
        <script src="../_static/sphinx-data-viewer/jsonview.bundle.js?v=18cd53c5"></script>
        <script src="../_static/sphinx-data-viewer/jsonview_loader.js?v=f7ff7e7d"></script>
        <script src="../_static/sphinx-needs/libs/html/datatables.min.js?v=8a4aee21"></script>
        <script src="../_static/sphinx-needs/libs/html/datatables_loader.js?v=a2cae175"></script>
        <script src="../_static/sphinx-needs/libs/html/sphinx_needs_collapse.js?v=dca66431"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <script src="https://cdn.datatables.net/1.13.4/js/jquery.dataTables.min.js"></script>
        <script src="../_static/activate_datatables.js?v=e38ccb97"></script>
        <script src="../_static/js/custom.js?v=f5206ae7"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Summit User Guide" href="summit_user_guide.html" />
    <link rel="prev" title="2024 Notable System Changes" href="2024_olcf_system_changes.html" />

</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: #efefef" >

          
          
          <a href="../index.html">
            
              <img src="../_static/olcf_logo.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../support/index.html">Contact &amp; Support</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../support/index.html#olcf-user-assistance-center">OLCF User Assistance Center</a></li>
<li class="toctree-l2"><a class="reference internal" href="../support/index.html#authentication-support">Authentication Support</a></li>
<li class="toctree-l2"><a class="reference internal" href="../support/index.html#olcf-office-hours">OLCF Office Hours</a></li>
<li class="toctree-l2"><a class="reference internal" href="../support/index.html#communication-to-users">Communication to Users</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../accounts/index.html">Accounts and Projects</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../accounts/accounts_and_projects.html">Request a New Allocation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../accounts/accounts_and_projects.html#what-are-the-differences-between-project-types">What are the differences between project types?</a></li>
<li class="toctree-l3"><a class="reference internal" href="../accounts/accounts_and_projects.html#what-happens-after-a-project-request-is-approved">What happens after a project request is approved?</a></li>
<li class="toctree-l3"><a class="reference internal" href="../accounts/accounts_and_projects.html#guidance-on-frontier-allocation-requests">Guidance on Frontier Allocation Requests</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../accounts/accounts_and_projects.html#applying-for-a-user-account">Applying for a user account</a></li>
<li class="toctree-l2"><a class="reference internal" href="../accounts/accounts_and_projects.html#checking-the-status-of-your-application">Checking the status of your application</a></li>
<li class="toctree-l2"><a class="reference internal" href="../accounts/accounts_and_projects.html#get-access-to-additional-projects">Get access to additional projects</a></li>
<li class="toctree-l2"><a class="reference internal" href="../accounts/frequently_asked_questions.html">Frequently Asked Questions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../accounts/frequently_asked_questions.html#how-do-i-apply-for-an-account">How do I apply for an account?</a></li>
<li class="toctree-l3"><a class="reference internal" href="../accounts/frequently_asked_questions.html#what-is-the-status-of-my-application">What is the status of my application?</a></li>
<li class="toctree-l3"><a class="reference internal" href="../accounts/frequently_asked_questions.html#how-should-i-acknowledge-the-olcf-in-my-publications-and-presentations">How should I acknowledge the OLCF in my publications and presentations?</a></li>
<li class="toctree-l3"><a class="reference internal" href="../accounts/frequently_asked_questions.html#what-is-a-subproject">What is a subproject?</a></li>
<li class="toctree-l3"><a class="reference internal" href="../accounts/frequently_asked_questions.html#i-no-longer-need-my-account-who-should-i-inform-and-what-should-i-do-with-my-olcf-issued-rsa-securid-token">I no longer need my account. Who should I inform and what should I do with my OLCF issued RSA SecurID token?</a></li>
<li class="toctree-l3"><a class="reference internal" href="../accounts/frequently_asked_questions.html#my-securid-token-is-broken-expired-what-should-i-do">My SecurID token is broken/expired. What should I do?</a></li>
<li class="toctree-l3"><a class="reference internal" href="../accounts/frequently_asked_questions.html#getting-help">Getting Help</a></li>
<li class="toctree-l3"><a class="reference internal" href="../accounts/frequently_asked_questions.html#additional-resources">Additional Resources</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../accounts/documents_and_forms.html">Documents and Forms</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../accounts/documents_and_forms.html#forms-for-requesting-a-project-allocation">Forms for Requesting a Project Allocation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../accounts/documents_and_forms.html#forms-for-requesting-an-account">Forms for Requesting an Account</a></li>
<li class="toctree-l3"><a class="reference internal" href="../accounts/documents_and_forms.html#forms-to-request-changes-to-computers-jobs-or-accounts">Forms to Request Changes to Computers, Jobs, or Accounts</a></li>
<li class="toctree-l3"><a class="reference internal" href="../accounts/documents_and_forms.html#report-templates">Report Templates</a></li>
<li class="toctree-l3"><a class="reference internal" href="../accounts/documents_and_forms.html#miscellaneous-forms">Miscellaneous Forms</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../accounts/olcf_policy_guide.html">OLCF Policy Guides</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../accounts/olcf_policy_guide.html#olcf-acknowledgement">OLCF Acknowledgement</a></li>
<li class="toctree-l3"><a class="reference internal" href="../accounts/olcf_policy_guide.html#software-requests">Software Requests</a></li>
<li class="toctree-l3"><a class="reference internal" href="../accounts/olcf_policy_guide.html#special-requests-and-policy-exemptions">Special Requests and Policy Exemptions</a></li>
<li class="toctree-l3"><a class="reference internal" href="../accounts/olcf_policy_guide.html#computing-policy">Computing Policy</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../accounts/olcf_policy_guide.html#computer-use">Computer Use</a></li>
<li class="toctree-l4"><a class="reference internal" href="../accounts/olcf_policy_guide.html#data-use">Data Use</a></li>
<li class="toctree-l4"><a class="reference internal" href="../accounts/olcf_policy_guide.html#software-use">Software Use</a></li>
<li class="toctree-l4"><a class="reference internal" href="../accounts/olcf_policy_guide.html#user-accountability">User Accountability</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../accounts/olcf_policy_guide.html#data-management-policy">Data Management Policy</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../accounts/olcf_policy_guide.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../accounts/olcf_policy_guide.html#data-retention-purge-quotas">Data Retention, Purge, &amp; Quotas</a></li>
<li class="toctree-l4"><a class="reference internal" href="../accounts/olcf_policy_guide.html#data-prohibitions-safeguards">Data Prohibitions &amp; Safeguards</a></li>
<li class="toctree-l4"><a class="reference internal" href="../accounts/olcf_policy_guide.html#software">Software</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../accounts/olcf_policy_guide.html#security-policy">Security Policy</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../accounts/olcf_policy_guide.html#scope">Scope</a></li>
<li class="toctree-l4"><a class="reference internal" href="../accounts/olcf_policy_guide.html#personal-use">Personal Use</a></li>
<li class="toctree-l4"><a class="reference internal" href="../accounts/olcf_policy_guide.html#accessing-olcf-computational-resources">Accessing OLCF Computational Resources</a></li>
<li class="toctree-l4"><a class="reference internal" href="../accounts/olcf_policy_guide.html#data-management">Data Management</a></li>
<li class="toctree-l4"><a class="reference internal" href="../accounts/olcf_policy_guide.html#sensitive-data">Sensitive Data</a></li>
<li class="toctree-l4"><a class="reference internal" href="../accounts/olcf_policy_guide.html#data-transfer">Data Transfer</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../accounts/olcf_policy_guide.html#incite-allocation-under-utilization-policy">INCITE Allocation Under-utilization Policy</a></li>
<li class="toctree-l3"><a class="reference internal" href="../accounts/olcf_policy_guide.html#project-reporting-policy">Project Reporting Policy</a></li>
<li class="toctree-l3"><a class="reference internal" href="../accounts/olcf_policy_guide.html#non-proprietary-institutional-user-agreement-policy">Non-proprietary Institutional User Agreement Policy</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../accounts/olcf_policy_guide.html#access">Access</a></li>
<li class="toctree-l4"><a class="reference internal" href="../accounts/olcf_policy_guide.html#rules-and-regulations">Rules and Regulations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../accounts/olcf_policy_guide.html#safety-and-health">Safety and Health</a></li>
<li class="toctree-l4"><a class="reference internal" href="../accounts/olcf_policy_guide.html#intent-to-publish">Intent to Publish</a></li>
<li class="toctree-l4"><a class="reference internal" href="../accounts/olcf_policy_guide.html#export-control">Export Control</a></li>
<li class="toctree-l4"><a class="reference internal" href="../accounts/olcf_policy_guide.html#intellectual-property">Intellectual Property</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../accounts/olcf_policy_guide.html#hipaa-itar-project-rules-of-behavior-policy">HIPAA/ITAR Project Rules of Behavior Policy</a></li>
<li class="toctree-l3"><a class="reference internal" href="../accounts/olcf_policy_guide.html#user-managed-software-ums-policy">User-Managed Software (UMS) Policy</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../accounts/olcf_policy_guide.html#purpose">Purpose</a></li>
<li class="toctree-l4"><a class="reference internal" href="../accounts/olcf_policy_guide.html#policies">Policies</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../accounts/glossary.html">Glossary</a></li>
<li class="toctree-l2"><a class="reference internal" href="../accounts/index.html#additional-resources">Additional Resources</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../connecting/index.html">Connecting</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../connecting/index.html#connecting-for-the-first-time">Connecting for the first time</a></li>
<li class="toctree-l2"><a class="reference internal" href="../connecting/index.html#activating-a-new-securid-fob">Activating a new SecurID fob</a></li>
<li class="toctree-l2"><a class="reference internal" href="../connecting/index.html#pins-passcodes-and-tokencodes">PINs, Passcodes, and Tokencodes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../connecting/index.html#x11-forwarding">X11 Forwarding</a></li>
<li class="toctree-l2"><a class="reference internal" href="../connecting/index.html#id2">Systems Available to All Projects</a></li>
<li class="toctree-l2"><a class="reference internal" href="../connecting/index.html#olcf-system-hostnames">OLCF System Hostnames</a></li>
<li class="toctree-l2"><a class="reference internal" href="../connecting/index.html#starting-a-tmux-session">Starting a Tmux Session</a></li>
<li class="toctree-l2"><a class="reference internal" href="../connecting/index.html#checking-system-availability">Checking System Availability</a></li>
</ul>
</li>
</ul>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Systems</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="2024_olcf_system_changes.html">2024 Notable System Changes</a><ul>
<li class="toctree-l3"><a class="reference internal" href="2024_olcf_system_changes.html#hpss-decommission-and-kronos-availability">HPSS Decommission and Kronos Availability</a><ul>
<li class="toctree-l4"><a class="reference internal" href="2024_olcf_system_changes.html#late-july-2024-kronos-available">Late July 2024 - Kronos available</a></li>
<li class="toctree-l4"><a class="reference internal" href="2024_olcf_system_changes.html#august-30-2024-hpss-becomes-read-only">August 30, 2024 - HPSS becomes read-only</a></li>
<li class="toctree-l4"><a class="reference internal" href="2024_olcf_system_changes.html#january-31-2025-hpss-decommissioned">January 31, 2025 - HPSS decommissioned</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="2024_olcf_system_changes.html#summit-and-alpine2-decommissions">Summit and Alpine2 Decommissions</a><ul>
<li class="toctree-l4"><a class="reference internal" href="2024_olcf_system_changes.html#november-15-2024-summit-decommissioned">November 15, 2024 - Summit decommissioned</a></li>
<li class="toctree-l4"><a class="reference internal" href="2024_olcf_system_changes.html#november-19-2024-alpine2-read-only">November 19, 2024 - Alpine2 read-only</a></li>
<li class="toctree-l4"><a class="reference internal" href="2024_olcf_system_changes.html#january-31-2025-alpine2-decommissioned">January 31, 2025 - Alpine2 decommissioned</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Frontier User Guide</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#system-overview">System Overview</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#frontier-compute-nodes">Frontier Compute Nodes</a></li>
<li class="toctree-l4"><a class="reference internal" href="#node-types">Node Types</a></li>
<li class="toctree-l4"><a class="reference internal" href="#system-interconnect">System Interconnect</a></li>
<li class="toctree-l4"><a class="reference internal" href="#file-systems">File Systems</a></li>
<li class="toctree-l4"><a class="reference internal" href="#operating-system">Operating System</a></li>
<li class="toctree-l4"><a class="reference internal" href="#gpus">GPUs</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#connecting">Connecting</a></li>
<li class="toctree-l3"><a class="reference internal" href="#data-and-storage">Data and Storage</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#transition-from-alpine-to-orion">Transition from Alpine to Orion</a></li>
<li class="toctree-l4"><a class="reference internal" href="#lfs-setstripe-wrapper">LFS setstripe wrapper</a></li>
<li class="toctree-l4"><a class="reference internal" href="#nfs-filesystem">NFS Filesystem</a></li>
<li class="toctree-l4"><a class="reference internal" href="#lustre-filesystem">Lustre Filesystem</a></li>
<li class="toctree-l4"><a class="reference internal" href="#kronos-archival-storage">Kronos Archival Storage</a></li>
<li class="toctree-l4"><a class="reference internal" href="#nvme">NVMe</a></li>
<li class="toctree-l4"><a class="reference internal" href="#nvme-usage">NVMe Usage</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#using-globus-to-move-data-to-and-from-orion">Using Globus to Move Data to and from Orion</a></li>
<li class="toctree-l3"><a class="reference internal" href="#amd-gpus">AMD GPUs</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#amd-vs-nvidia-terminology">AMD vs NVIDIA Terminology</a></li>
<li class="toctree-l4"><a class="reference internal" href="#blocks-workgroups-threads-work-items-grids-wavefronts">Blocks (workgroups), Threads (work items), Grids, Wavefronts</a></li>
<li class="toctree-l4"><a class="reference internal" href="#the-compute-unit">The Compute Unit</a></li>
<li class="toctree-l4"><a class="reference internal" href="#hip">HIP</a></li>
<li class="toctree-l4"><a class="reference internal" href="#things-to-remember-when-programming-for-amd-gpus">Things To Remember When Programming for AMD GPUs</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#programming-environment">Programming Environment</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#environment-modules-lmod">Environment Modules (Lmod)</a></li>
<li class="toctree-l4"><a class="reference internal" href="#compilers">Compilers</a></li>
<li class="toctree-l4"><a class="reference internal" href="#mpi">MPI</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#compiling">Compiling</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id4">Compilers</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id7">MPI</a></li>
<li class="toctree-l4"><a class="reference internal" href="#openmp">OpenMP</a></li>
<li class="toctree-l4"><a class="reference internal" href="#openmp-gpu-offload">OpenMP GPU Offload</a></li>
<li class="toctree-l4"><a class="reference internal" href="#openacc">OpenACC</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id9">HIP</a></li>
<li class="toctree-l4"><a class="reference internal" href="#hip-openmp-cpu-threading">HIP + OpenMP CPU Threading</a></li>
<li class="toctree-l4"><a class="reference internal" href="#sycl">SYCL</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#running-jobs">Running Jobs</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#login-vs-compute-nodes">Login vs Compute Nodes</a></li>
<li class="toctree-l4"><a class="reference internal" href="#simplified-node-layout">Simplified Node Layout</a></li>
<li class="toctree-l4"><a class="reference internal" href="#slurm">Slurm</a></li>
<li class="toctree-l4"><a class="reference internal" href="#batch-scripts">Batch Scripts</a></li>
<li class="toctree-l4"><a class="reference internal" href="#interactive-jobs">Interactive Jobs</a></li>
<li class="toctree-l4"><a class="reference internal" href="#common-slurm-options">Common Slurm Options</a></li>
<li class="toctree-l4"><a class="reference internal" href="#slurm-environment-variables">Slurm Environment Variables</a></li>
<li class="toctree-l4"><a class="reference internal" href="#job-states">Job States</a></li>
<li class="toctree-l4"><a class="reference internal" href="#job-reason-codes">Job Reason Codes</a></li>
<li class="toctree-l4"><a class="reference internal" href="#scheduling-policy">Scheduling Policy</a></li>
<li class="toctree-l4"><a class="reference internal" href="#job-dependencies">Job Dependencies</a></li>
<li class="toctree-l4"><a class="reference internal" href="#monitoring-and-modifying-batch-jobs">Monitoring and Modifying Batch Jobs</a></li>
<li class="toctree-l4"><a class="reference internal" href="#srun">Srun</a></li>
<li class="toctree-l4"><a class="reference internal" href="#process-and-thread-mapping-examples">Process and Thread Mapping Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="#ensemble-jobs">Ensemble Jobs</a></li>
<li class="toctree-l4"><a class="reference internal" href="#tips-for-launching-at-scale">Tips for Launching at Scale</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#software">Software</a></li>
<li class="toctree-l3"><a class="reference internal" href="#debugging">Debugging</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#linaro-ddt">Linaro DDT</a></li>
<li class="toctree-l4"><a class="reference internal" href="#gdb">GDB</a></li>
<li class="toctree-l4"><a class="reference internal" href="#valgrind4hpc">Valgrind4hpc</a></li>
<li class="toctree-l4"><a class="reference internal" href="#omnitrace">Omnitrace</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#profiling-applications">Profiling Applications</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#getting-started-with-the-hpe-performance-analysis-tools-pat">Getting Started with the HPE Performance Analysis Tools (PAT)</a></li>
<li class="toctree-l4"><a class="reference internal" href="#getting-started-with-hpctoolkit">Getting Started with HPCToolkit</a></li>
<li class="toctree-l4"><a class="reference internal" href="#getting-started-with-the-rocm-profiler">Getting Started with the ROCm Profiler</a></li>
<li class="toctree-l4"><a class="reference internal" href="#roofline-profiling-with-the-rocm-profiler">Roofline Profiling with the ROCm Profiler</a></li>
<li class="toctree-l4"><a class="reference internal" href="#omniperf">Omniperf</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#tips-and-tricks">Tips and Tricks</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#using-reduced-precision-fp16-and-bf16-datatypes">Using reduced precision (FP16 and BF16 datatypes)</a></li>
<li class="toctree-l4"><a class="reference internal" href="#enabling-gpu-page-migration">Enabling GPU Page Migration</a></li>
<li class="toctree-l4"><a class="reference internal" href="#floating-point-fp-atomic-operations-and-coarse-fine-grained-memory-allocations">Floating-Point (FP) Atomic Operations and Coarse/Fine Grained Memory Allocations</a></li>
<li class="toctree-l4"><a class="reference internal" href="#performance-considerations-for-lds-fp-atomicadd">Performance considerations for LDS FP atomicAdd()</a></li>
<li class="toctree-l4"><a class="reference internal" href="#library-considerations-with-atomic-operations">Library considerations with atomic operations</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#system-updates">System Updates</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id17">2024-09-03</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id18">2024-08-20</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id19">2024-07-16</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id20">2024-04-17</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id21">2024-03-19</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id22">2024-01-23</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id23">2023-12-05</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id24">2023-10-03</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id25">2023-09-19</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id26">2023-07-18</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id27">2023-05-09</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#known-issues">Known Issues</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="summit_user_guide.html">Summit User Guide</a><ul>
<li class="toctree-l3"><a class="reference internal" href="summit_user_guide.html#summit-documentation-resources">Summit Documentation Resources</a></li>
<li class="toctree-l3"><a class="reference internal" href="summit_user_guide.html#system-overview">System Overview</a><ul>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#summit-nodes">Summit Nodes</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#node-types">Node Types</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#system-interconnect">System Interconnect</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#file-systems">File Systems</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#operating-system">Operating System</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#hardware-threads">Hardware Threads</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#gpus">GPUs</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="summit_user_guide.html#connecting">Connecting</a></li>
<li class="toctree-l3"><a class="reference internal" href="summit_user_guide.html#data-and-storage">Data and Storage</a></li>
<li class="toctree-l3"><a class="reference internal" href="summit_user_guide.html#software">Software</a></li>
<li class="toctree-l3"><a class="reference internal" href="summit_user_guide.html#shell-programming-environments">Shell &amp; Programming Environments</a><ul>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#default-shell">Default Shell</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#environment-management-with-lmod">Environment Management with Lmod</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="summit_user_guide.html#compiling">Compiling</a><ul>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#compilers">Compilers</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#linking-in-libraries">Linking in Libraries</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="summit_user_guide.html#running-jobs">Running Jobs</a><ul>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#login-launch-and-compute-nodes">Login, Launch, and Compute Nodes</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#batch-scripts">Batch Scripts</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#interactive-jobs">Interactive Jobs</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#common-bsub-options">Common bsub Options</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#batch-environment-variables">Batch Environment Variables</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#job-states">Job States</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#scheduling-policy">Scheduling Policy</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#job-dependencies">Job Dependencies</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#job-launcher-jsrun">Job Launcher (jsrun)</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#launching-a-job-with-jsrun">Launching a Job with jsrun</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#jsrun-examples">Jsrun Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#using-multithreading-in-a-job">Using Multithreading in a Job</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#launching-multiple-jsruns">Launching Multiple Jsruns</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#cuda-aware-mpi">CUDA-Aware MPI</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#monitoring-jobs">Monitoring Jobs</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#interacting-with-jobs">Interacting With Jobs</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#other-lsf-commands">Other LSF Commands</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#pbs-torque-moab-to-lsf-translation">PBS/Torque/MOAB-to-LSF Translation</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#easy-mode-vs-expert-mode">Easy Mode vs. Expert Mode</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#system-service-core-isolation">System Service Core Isolation</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#job-accounting-on-summit">Job Accounting on Summit</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#other-notes">Other Notes</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="summit_user_guide.html#debugging">Debugging</a><ul>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#linaro-ddt">Linaro DDT</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#gdb">GDB</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#valgrind">Valgrind</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="summit_user_guide.html#optimizing-and-profiling">Optimizing and Profiling</a><ul>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#profiling-gpu-code-with-nvidia-developer-tools">Profiling GPU Code with NVIDIA Developer Tools</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#score-p">Score-P</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#vampir">Vampir</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#hpctoolkit">HPCToolkit</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="summit_user_guide.html#nvidia-tesla-v100">NVIDIA V100 GPUs</a><ul>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#nvidia-v100-sm">NVIDIA V100 SM</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#hbm2">HBM2</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#nvidia-nvlink">NVIDIA NVLink</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#volta-multi-process-service">Volta Multi-Process Service</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#unified-memory">Unified Memory</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#independent-thread-scheduling">Independent Thread Scheduling</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#tensor-cores">Tensor Cores</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#tesla-v100-specifications">Tesla V100 Specifications</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#further-reading">Further Reading</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="summit_user_guide.html#burst-buffer">Burst Buffer</a><ul>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#nvme-xfs">NVMe (XFS)</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#current-nvme-usage">Current NVMe Usage</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#interactive-jobs-using-the-nvme">Interactive Jobs Using the NVMe</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#nvme-usage-example">NVMe Usage Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#spectral-library">Spectral Library</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="summit_user_guide.html#known-issues">Known Issues</a><ul>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#open-issues">Open Issues</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#resolved-issues">Resolved Issues</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#cuda-10-1-known-issues">CUDA 10.1 Known Issues</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="summit_user_guide.html#scalable-protected-infrastructure-spi">Scalable Protected Infrastructure (SPI)</a></li>
<li class="toctree-l3"><a class="reference internal" href="summit_user_guide.html#training-system-ascent">Training System (Ascent)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#id27">File Systems</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#obtaining-access-to-ascent">Obtaining Access to Ascent</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#logging-in-to-ascent">Logging In to Ascent</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="summit_user_guide.html#preparing-for-frontier">Preparing For Frontier</a><ul>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#hip">HIP</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#using-hip-on-summit">Using HIP on Summit</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#learning-to-program-with-hip">Learning to Program with HIP</a></li>
<li class="toctree-l4"><a class="reference internal" href="summit_user_guide.html#previous-frontier-training-events">Previous Frontier Training Events</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="citadel_user_guide.html">Citadel User Guide</a><ul>
<li class="toctree-l3"><a class="reference internal" href="citadel_user_guide.html#what-is-citadel">What is Citadel</a><ul>
<li class="toctree-l4"><a class="reference internal" href="citadel_user_guide.html#citadel-spi-documentation">Citadel (SPI) Documentation</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="andes_user_guide.html">Andes User Guide</a><ul>
<li class="toctree-l3"><a class="reference internal" href="andes_user_guide.html#system-overview">System Overview</a><ul>
<li class="toctree-l4"><a class="reference internal" href="andes_user_guide.html#compute-nodes">Compute nodes</a></li>
<li class="toctree-l4"><a class="reference internal" href="andes_user_guide.html#login-nodes">Login nodes</a></li>
<li class="toctree-l4"><a class="reference internal" href="andes_user_guide.html#file-systems">File systems</a></li>
<li class="toctree-l4"><a class="reference internal" href="andes_user_guide.html#lfs-setstripe-wrapper">LFS setstripe wrapper</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="andes_user_guide.html#shell-and-programming-environments">Shell and programming environments</a><ul>
<li class="toctree-l4"><a class="reference internal" href="andes_user_guide.html#default-shell">Default shell</a></li>
<li class="toctree-l4"><a class="reference internal" href="andes_user_guide.html#environment-management-with-lmod">Environment management with lmod</a></li>
<li class="toctree-l4"><a class="reference internal" href="andes_user_guide.html#installed-software">Installed Software</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="andes_user_guide.html#compiling">Compiling</a><ul>
<li class="toctree-l4"><a class="reference internal" href="andes_user_guide.html#available-compilers">Available compilers</a></li>
<li class="toctree-l4"><a class="reference internal" href="andes_user_guide.html#changing-compilers">Changing compilers</a></li>
<li class="toctree-l4"><a class="reference internal" href="andes_user_guide.html#compiler-wrappers">Compiler wrappers</a></li>
<li class="toctree-l4"><a class="reference internal" href="andes_user_guide.html#compiling-threaded-codes">Compiling threaded codes</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="andes_user_guide.html#running-jobs">Running Jobs</a><ul>
<li class="toctree-l4"><a class="reference internal" href="andes_user_guide.html#login-vs-compute-nodes-on-commodity-clusters">Login vs Compute Nodes on Commodity Clusters</a></li>
<li class="toctree-l4"><a class="reference internal" href="andes_user_guide.html#slurm">Slurm</a></li>
<li class="toctree-l4"><a class="reference internal" href="andes_user_guide.html#interactive-batch-jobs-on-commodity-clusters">Interactive Batch Jobs on Commodity Clusters</a></li>
<li class="toctree-l4"><a class="reference internal" href="andes_user_guide.html#common-batch-options-to-slurm">Common Batch Options to Slurm</a></li>
<li class="toctree-l4"><a class="reference internal" href="andes_user_guide.html#batch-environment-variables">Batch Environment Variables</a></li>
<li class="toctree-l4"><a class="reference internal" href="andes_user_guide.html#modifying-batch-jobs">Modifying Batch Jobs</a></li>
<li class="toctree-l4"><a class="reference internal" href="andes_user_guide.html#monitoring-batch-jobs">Monitoring Batch Jobs</a></li>
<li class="toctree-l4"><a class="reference internal" href="andes_user_guide.html#job-execution">Job Execution</a></li>
<li class="toctree-l4"><a class="reference internal" href="andes_user_guide.html#batch-queues-on-andes">Batch Queues on Andes</a></li>
<li class="toctree-l4"><a class="reference internal" href="andes_user_guide.html#job-accounting-on-andes">Job Accounting on Andes</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="andes_user_guide.html#andes-debugging">Debugging</a><ul>
<li class="toctree-l4"><a class="reference internal" href="andes_user_guide.html#linaro-ddt">Linaro DDT</a></li>
<li class="toctree-l4"><a class="reference internal" href="andes_user_guide.html#gdb">GDB</a></li>
<li class="toctree-l4"><a class="reference internal" href="andes_user_guide.html#valgrind">Valgrind</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="andes_user_guide.html#visualization-tools">Visualization tools</a><ul>
<li class="toctree-l4"><a class="reference internal" href="andes_user_guide.html#paraview">ParaView</a></li>
<li class="toctree-l4"><a class="reference internal" href="andes_user_guide.html#visit">VisIt</a></li>
<li class="toctree-l4"><a class="reference internal" href="andes_user_guide.html#remote-visualization-using-vnc-non-gpu">Remote Visualization using VNC (non-GPU)</a></li>
<li class="toctree-l4"><a class="reference internal" href="andes_user_guide.html#remote-visualization-using-vnc-gpu-nodes">Remote Visualization using VNC (GPU nodes)</a></li>
<li class="toctree-l4"><a class="reference internal" href="andes_user_guide.html#remote-visualization-using-nice-dcv-gpu-nodes-only">Remote Visualization using Nice DCV (GPU nodes only)</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="home_user_guide.html">Home</a><ul>
<li class="toctree-l3"><a class="reference internal" href="home_user_guide.html#system-overview">System Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="home_user_guide.html#access-connecting">Access &amp; Connecting</a></li>
<li class="toctree-l3"><a class="reference internal" href="home_user_guide.html#usage">Usage</a><ul>
<li class="toctree-l4"><a class="reference internal" href="home_user_guide.html#acceptable-tasks">Acceptable Tasks</a></li>
<li class="toctree-l4"><a class="reference internal" href="home_user_guide.html#unacceptable-tasks">Unacceptable Tasks</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="dtn_user_guide.html">Data Transfer Nodes (DTNs)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="dtn_user_guide.html#system-overview">System Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="dtn_user_guide.html#interactive-access">Interactive Access</a></li>
<li class="toctree-l3"><a class="reference internal" href="dtn_user_guide.html#access-from-globus-online">Access From Globus Online</a></li>
<li class="toctree-l3"><a class="reference internal" href="dtn_user_guide.html#batch-queue-slurm">Batch Queue (Slurm)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="dtn_user_guide.html#queue-policy">Queue Policy</a></li>
<li class="toctree-l4"><a class="reference internal" href="dtn_user_guide.html#submitting-jobs-to-frontier">Submitting jobs to Frontier</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="hpss_user_guide.html">High Performance Storage System</a><ul>
<li class="toctree-l3"><a class="reference internal" href="hpss_user_guide.html#system-overview">System Overview</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="odo_user_guide.html">Odo</a><ul>
<li class="toctree-l3"><a class="reference internal" href="odo_user_guide.html#system-overview">System Overview</a><ul>
<li class="toctree-l4"><a class="reference internal" href="odo_user_guide.html#file-systems">File Systems</a></li>
<li class="toctree-l4"><a class="reference internal" href="odo_user_guide.html#obtaining-access-to-odo">Obtaining Access to Odo</a></li>
<li class="toctree-l4"><a class="reference internal" href="odo_user_guide.html#logging-in-to-odo">Logging In to Odo</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="ascent_user_guide.html">Ascent</a><ul>
<li class="toctree-l3"><a class="reference internal" href="ascent_user_guide.html#system-overview">System Overview</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="spock_quick_start_guide.html">Spock Quick-Start Guide</a><ul>
<li class="toctree-l3"><a class="reference internal" href="spock_quick_start_guide.html#system-overview">System Overview</a><ul>
<li class="toctree-l4"><a class="reference internal" href="spock_quick_start_guide.html#spock-compute-nodes">Spock Compute Nodes</a></li>
<li class="toctree-l4"><a class="reference internal" href="spock_quick_start_guide.html#system-interconnect">System Interconnect</a></li>
<li class="toctree-l4"><a class="reference internal" href="spock_quick_start_guide.html#file-systems">File Systems</a></li>
<li class="toctree-l4"><a class="reference internal" href="spock_quick_start_guide.html#gpus">GPUs</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="spock_quick_start_guide.html#connecting">Connecting</a></li>
<li class="toctree-l3"><a class="reference internal" href="spock_quick_start_guide.html#data-and-storage">Data and Storage</a><ul>
<li class="toctree-l4"><a class="reference internal" href="spock_quick_start_guide.html#nfs">NFS</a></li>
<li class="toctree-l4"><a class="reference internal" href="spock_quick_start_guide.html#gpfs">GPFS</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="spock_quick_start_guide.html#programming-environment">Programming Environment</a><ul>
<li class="toctree-l4"><a class="reference internal" href="spock_quick_start_guide.html#environment-modules-lmod">Environment Modules (Lmod)</a></li>
<li class="toctree-l4"><a class="reference internal" href="spock_quick_start_guide.html#compilers">Compilers</a></li>
<li class="toctree-l4"><a class="reference internal" href="spock_quick_start_guide.html#mpi">MPI</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="spock_quick_start_guide.html#compiling">Compiling</a><ul>
<li class="toctree-l4"><a class="reference internal" href="spock_quick_start_guide.html#id3">MPI</a></li>
<li class="toctree-l4"><a class="reference internal" href="spock_quick_start_guide.html#openmp">OpenMP</a></li>
<li class="toctree-l4"><a class="reference internal" href="spock_quick_start_guide.html#openmp-gpu-offload">OpenMP GPU Offload</a></li>
<li class="toctree-l4"><a class="reference internal" href="spock_quick_start_guide.html#hip">HIP</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="spock_quick_start_guide.html#running-jobs">Running Jobs</a><ul>
<li class="toctree-l4"><a class="reference internal" href="spock_quick_start_guide.html#slurm-workload-manager">Slurm Workload Manager</a></li>
<li class="toctree-l4"><a class="reference internal" href="spock_quick_start_guide.html#slurm-compute-node-partitions">Slurm Compute Node Partitions</a></li>
<li class="toctree-l4"><a class="reference internal" href="spock_quick_start_guide.html#process-and-thread-mapping">Process and Thread Mapping</a></li>
<li class="toctree-l4"><a class="reference internal" href="spock_quick_start_guide.html#nvme-usage">NVMe Usage</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="spock_quick_start_guide.html#getting-help">Getting Help</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="crusher_quick_start_guide.html">Crusher Quick-Start Guide</a><ul>
<li class="toctree-l3"><a class="reference internal" href="crusher_quick_start_guide.html#system-overview">System Overview</a><ul>
<li class="toctree-l4"><a class="reference internal" href="crusher_quick_start_guide.html#crusher-compute-nodes">Crusher Compute Nodes</a></li>
<li class="toctree-l4"><a class="reference internal" href="crusher_quick_start_guide.html#system-interconnect">System Interconnect</a></li>
<li class="toctree-l4"><a class="reference internal" href="crusher_quick_start_guide.html#file-systems">File Systems</a></li>
<li class="toctree-l4"><a class="reference internal" href="crusher_quick_start_guide.html#gpus">GPUs</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="crusher_quick_start_guide.html#connecting">Connecting</a></li>
<li class="toctree-l3"><a class="reference internal" href="crusher_quick_start_guide.html#data-and-storage">Data and Storage</a><ul>
<li class="toctree-l4"><a class="reference internal" href="crusher_quick_start_guide.html#nfs-filesystem">NFS Filesystem</a></li>
<li class="toctree-l4"><a class="reference internal" href="crusher_quick_start_guide.html#lustre-filesystem">Lustre Filesystem</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="crusher_quick_start_guide.html#programming-environment">Programming Environment</a><ul>
<li class="toctree-l4"><a class="reference internal" href="crusher_quick_start_guide.html#environment-modules-lmod">Environment Modules (Lmod)</a></li>
<li class="toctree-l4"><a class="reference internal" href="crusher_quick_start_guide.html#compilers">Compilers</a></li>
<li class="toctree-l4"><a class="reference internal" href="crusher_quick_start_guide.html#mpi">MPI</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="crusher_quick_start_guide.html#compiling">Compiling</a><ul>
<li class="toctree-l4"><a class="reference internal" href="crusher_quick_start_guide.html#id3">MPI</a></li>
<li class="toctree-l4"><a class="reference internal" href="crusher_quick_start_guide.html#openmp">OpenMP</a></li>
<li class="toctree-l4"><a class="reference internal" href="crusher_quick_start_guide.html#openmp-gpu-offload">OpenMP GPU Offload</a></li>
<li class="toctree-l4"><a class="reference internal" href="crusher_quick_start_guide.html#hip">HIP</a></li>
<li class="toctree-l4"><a class="reference internal" href="crusher_quick_start_guide.html#hip-openmp-cpu-threading">HIP + OpenMP CPU Threading</a></li>
<li class="toctree-l4"><a class="reference internal" href="crusher_quick_start_guide.html#sycl">SYCL</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="crusher_quick_start_guide.html#running-jobs">Running Jobs</a><ul>
<li class="toctree-l4"><a class="reference internal" href="crusher_quick_start_guide.html#slurm-workload-manager">Slurm Workload Manager</a></li>
<li class="toctree-l4"><a class="reference internal" href="crusher_quick_start_guide.html#slurm-compute-node-partitions">Slurm Compute Node Partitions</a></li>
<li class="toctree-l4"><a class="reference internal" href="crusher_quick_start_guide.html#process-and-thread-mapping">Process and Thread Mapping</a></li>
<li class="toctree-l4"><a class="reference internal" href="crusher_quick_start_guide.html#nvme-usage">NVMe Usage</a></li>
<li class="toctree-l4"><a class="reference internal" href="crusher_quick_start_guide.html#tips-for-launching-at-scale">Tips for Launching at Scale</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="crusher_quick_start_guide.html#profiling-applications">Profiling Applications</a><ul>
<li class="toctree-l4"><a class="reference internal" href="crusher_quick_start_guide.html#getting-started-with-the-hpe-performance-analysis-tools-pat">Getting Started with the HPE Performance Analysis Tools (PAT)</a></li>
<li class="toctree-l4"><a class="reference internal" href="crusher_quick_start_guide.html#getting-started-with-hpctoolkit">Getting Started with HPCToolkit</a></li>
<li class="toctree-l4"><a class="reference internal" href="crusher_quick_start_guide.html#getting-started-with-the-rocm-profiler">Getting Started with the ROCm Profiler</a></li>
<li class="toctree-l4"><a class="reference internal" href="crusher_quick_start_guide.html#roofline-profiling-with-the-rocm-profiler">Roofline Profiling with the ROCm Profiler</a></li>
<li class="toctree-l4"><a class="reference internal" href="crusher_quick_start_guide.html#omnitrace">Omnitrace</a></li>
<li class="toctree-l4"><a class="reference internal" href="crusher_quick_start_guide.html#omniperf">Omniperf</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="crusher_quick_start_guide.html#notable-differences-between-summit-and-crusher">Notable Differences between Summit and Crusher</a><ul>
<li class="toctree-l4"><a class="reference internal" href="crusher_quick_start_guide.html#using-reduced-precision-fp16-and-bf16-datatypes">Using reduced precision (FP16 and BF16 datatypes)</a></li>
<li class="toctree-l4"><a class="reference internal" href="crusher_quick_start_guide.html#enabling-gpu-page-migration">Enabling GPU Page Migration</a></li>
<li class="toctree-l4"><a class="reference internal" href="crusher_quick_start_guide.html#floating-point-fp-atomic-operations-and-coarse-fine-grained-memory-allocations">Floating-Point (FP) Atomic Operations and Coarse/Fine Grained Memory Allocations</a></li>
<li class="toctree-l4"><a class="reference internal" href="crusher_quick_start_guide.html#performance-considerations-for-lds-fp-atomicadd">Performance considerations for LDS FP atomicAdd()</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="crusher_quick_start_guide.html#system-updates">System Updates</a><ul>
<li class="toctree-l4"><a class="reference internal" href="crusher_quick_start_guide.html#id6">2024-03-19</a></li>
<li class="toctree-l4"><a class="reference internal" href="crusher_quick_start_guide.html#id7">2024-01-23</a></li>
<li class="toctree-l4"><a class="reference internal" href="crusher_quick_start_guide.html#id8">2023-12-05</a></li>
<li class="toctree-l4"><a class="reference internal" href="crusher_quick_start_guide.html#id9">2023-10-03</a></li>
<li class="toctree-l4"><a class="reference internal" href="crusher_quick_start_guide.html#id10">2023-09-19</a></li>
<li class="toctree-l4"><a class="reference internal" href="crusher_quick_start_guide.html#id11">2023-07-18</a></li>
<li class="toctree-l4"><a class="reference internal" href="crusher_quick_start_guide.html#id12">2023-04-05</a></li>
<li class="toctree-l4"><a class="reference internal" href="crusher_quick_start_guide.html#id13">2022-12-29</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="crusher_quick_start_guide.html#getting-help">Getting Help</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../services_and_applications/index.html">Services and Applications</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../services_and_applications/slate/index.html">Slate</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../services_and_applications/slate/overview.html">Overview</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/overview.html#what-is-slate">What is Slate?</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/overview.html#what-is-kubernetes">What is Kubernetes?</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/overview.html#what-is-openshift">What is OpenShift?</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../services_and_applications/slate/getting_started.html">Getting Started</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/getting_started.html#requesting-a-slate-project-allocation">Requesting A Slate Project Allocation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/getting_started.html#logging-in">Logging in</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/getting_started.html#slate-namespaces">Slate Namespaces</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/getting_started.html#install-the-oc-tool">Install the OC tool</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/getting_started.html#test-login-with-oc-tool">Test login with OC Tool</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../services_and_applications/slate/guided_tutorial.html">Guided Tutorial</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/guided_tutorial.html#creating-your-project">Creating your project</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/guided_tutorial.html#guided-web-gui-tutorial">Guided Web GUI Tutorial</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../services_and_applications/slate/guided_tutorial_cli.html">Guided Tutorial: CLI</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/guided_tutorial_cli.html#adding-a-pod-to-your-project">Adding a Pod to your Project</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../services_and_applications/slate/image_building.html">Image Building</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/image_building.html#build-types">Build Types</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/image_building.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/image_building.html#logging-into-the-registry-externally">Logging into the registry externally</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../services_and_applications/slate/workloads/index.html">Workloads</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/workloads/pod.html">Pods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/workloads/deployment.html">Deployments</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../services_and_applications/slate/networking/index.html">Networking</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/networking/services.html">Services</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/networking/nodeport.html">NodePorts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/networking/route.html">Routes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/networking/networkpolicy.html">Network Policies</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/networking/port_forwarding.html">Quick Access from Outside Slate</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../services_and_applications/slate/storage.html">Persistent Storage</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/storage.html#creating-a-persistent-volume-claim">Creating A Persistent Volume Claim</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/storage.html#adding-pvc-to-pod">Adding PVC To Pod</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/storage.html#backups">Backups</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../services_and_applications/slate/workflows/index.html">Workflows</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/workflows/overview.html">Workflows Overview</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/workflows/openshift_gitops.html">OpenShift GitOps</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/workflows/openshift_pipelines.html">OpenShift Pipelines</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../services_and_applications/slate/use_cases/index.html">Application Deployment Examples</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/use_cases/simple_website.html">Build and Deploy Simple Website</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/use_cases/mongodb_service.html">Deploy MongoDB</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/use_cases/nginx_hello_world.html">Deploy NGINX with Hello World</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/use_cases/gitlab_runner.html">GitLab Runners</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/use_cases/helm_example.html">Deploy Packages with Helm</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/use_cases/helm_prerequisite.html">Helm Prerequisites</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/use_cases/minio.html">MinIO Object Store (On an NCCS Filesystem)</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../services_and_applications/slate/access_olcf_resources/index.html">Access OLCF Resources From Containers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/access_olcf_resources/job_submit.html">Batch Job Submission</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/access_olcf_resources/mount_fs.html">Mount OLCF Filesystems</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../services_and_applications/slate/other_resources.html">Schedule Other Slate Resources</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/other_resources.html#gpus">GPUs</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../services_and_applications/slate/olcf_provided_applications/index.html">OLCF-Provided Applications on Slate</a><ul class="simple">
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../services_and_applications/slate/troubleshooting/index.html">Troubleshooting</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/troubleshooting/fix-writable-directories.html">Fix Container Image Permissions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/troubleshooting/debugging.html">Debugging</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../services_and_applications/slate/examples.html">YAML Object Quick Reference</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/examples.html#cronjobs">CronJobs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/examples.html#deployments-and-stateful-sets">Deployments and Stateful Sets</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/examples.html#pods">Pods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/examples.html#roles-and-rolebindings">Roles and Rolebindings</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/examples.html#routes-services-and-nodeports">Routes, Services and Nodeports</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/examples.html#persistent-volume-claims">Persistent Volume Claims</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../services_and_applications/slate/glossary.html">Glossary</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../services_and_applications/myolcf/index.html">myOLCF</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../services_and_applications/myolcf/overview.html">Overview</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/myolcf/overview.html#what-is-myolcf">What is myOLCF?</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/myolcf/overview.html#what-can-it-do">What can it do?</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/myolcf/overview.html#can-i-suggest-a-feature">Can I suggest a feature?</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../services_and_applications/myolcf/authenticating.html">Authenticating</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/myolcf/authenticating.html#olcf-moderate-accounts">OLCF Moderate Accounts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/myolcf/authenticating.html#olcf-open-accounts">OLCF Open Accounts</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../services_and_applications/myolcf/project_pages/project_pages.html">Project Pages</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/myolcf/project_pages/project_pages.html#project-context">Project Context</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/myolcf/project_pages/project_pages.html#switching-project-contexts">Switching Project Contexts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/myolcf/project_pages/project_pages.html#available-pages">Available Pages</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../services_and_applications/myolcf/account_pages/account_pages.html">Account Pages</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/myolcf/account_pages/account_pages.html#account-context">Account Context</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/myolcf/account_pages/account_pages.html#available-pages">Available Pages</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../services_and_applications/myolcf/account_pages/processing_membership_requests.html">Processing Project Membership Requests</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../services_and_applications/jupyter/index.html">Jupyter</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../services_and_applications/jupyter/overview.html">Overview</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/jupyter/overview.html#jupyter-at-olcf">Jupyter at OLCF</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/jupyter/overview.html#access">Access</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/jupyter/overview.html#cpu-vs-gpu-jupyterlab-available-resources">CPU vs. GPU JupyterLab (Available Resources)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/jupyter/overview.html#working-within-lustre-and-nfs-launching-a-notebook">Working within Lustre and NFS (Launching a notebook)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/jupyter/overview.html#conda-environments-and-custom-notebooks">Conda environments and custom notebooks</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/jupyter/overview.html#manually-stopping-your-jupyterlab-session">Manually stopping your JupyterLab session</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/jupyter/overview.html#things-to-be-aware-of">Things to be aware of</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/jupyter/overview.html#example-jupyter-notebooks">Example Jupyter Notebooks</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../data/index.html">Data Storage and Transfers</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../data/index.html#summary-of-storage-areas">Summary of Storage Areas</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../data/index.html#notes-on-user-centric-data-storage">Notes on User-Centric Data Storage</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../data/index.html#user-home-directories-nfs">User Home Directories (NFS)</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../data/index.html#notes-on-project-centric-data-storage">Notes on Project-Centric Data Storage</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../data/index.html#project-home-directories-nfs">Project Home Directories (NFS)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../data/index.html#project-work-areas">Project Work Areas</a></li>
<li class="toctree-l4"><a class="reference internal" href="../data/index.html#project-archive-directories">Project Archive Directories</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../data/index.html#data-policies">Data Policies</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../data/index.html#information">Information</a></li>
<li class="toctree-l3"><a class="reference internal" href="../data/index.html#special-requests">Special Requests</a></li>
<li class="toctree-l3"><a class="reference internal" href="../data/index.html#data-retention">Data Retention</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../data/index.html#orion-lustre-hpe-clusterstor-filesystem">Orion Lustre HPE ClusterStor Filesystem</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../data/index.html#orion-performance-tiers-and-file-striping-policy">Orion Performance Tiers and File Striping Policy</a></li>
<li class="toctree-l3"><a class="reference internal" href="../data/index.html#i-o-patterns-that-benefit-from-file-striping">I/O Patterns that Benefit from File Striping</a></li>
<li class="toctree-l3"><a class="reference internal" href="../data/index.html#lfs-setstripe-wrapper">LFS setstripe wrapper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../data/index.html#lustre-file-locking-tips">Lustre File Locking Tips</a></li>
<li class="toctree-l3"><a class="reference internal" href="../data/index.html#darshan-runtime-and-i-o-profiling">Darshan-runtime and I/O Profiling</a></li>
<li class="toctree-l3"><a class="reference internal" href="../data/index.html#purge">Purge</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../data/index.html#alpine2-ibm-spectrum-scale-filesystem">Alpine2 IBM Spectrum Scale Filesystem</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../data/index.html#alpine2-performance-under-non-ideal-workloads">Alpine2 Performance under non-ideal workloads</a></li>
<li class="toctree-l3"><a class="reference internal" href="../data/index.html#tips">Tips</a></li>
<li class="toctree-l3"><a class="reference internal" href="../data/index.html#major-difference-between-lustre-hpe-clusterstor-and-ibm-spectrum-scale">Major difference between Lustre HPE ClusterStor and IBM Spectrum Scale</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../data/index.html#hpss-data-archival-system">HPSS Data Archival System</a></li>
<li class="toctree-l2"><a class="reference internal" href="../data/index.html#kronos-nearline-archival-storage-system">Kronos Nearline Archival Storage System</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../data/index.html#access-data-transfer">Access / Data Transfer</a></li>
<li class="toctree-l3"><a class="reference internal" href="../data/index.html#directory-structure">Directory Structure</a></li>
<li class="toctree-l3"><a class="reference internal" href="../data/index.html#project-quotas">Project Quotas</a></li>
<li class="toctree-l3"><a class="reference internal" href="../data/index.html#kronos-and-hpss-comparison">Kronos and HPSS Comparison</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../data/index.html#transferring-data">Transferring Data</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../data/index.html#data-transferring-data-globus">Globus</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../data/index.html#using-globus-to-move-data-between-collections">Using Globus to Move Data Between Collections</a></li>
<li class="toctree-l4"><a class="reference internal" href="../data/index.html#using-globus-from-your-local-workstation">Using Globus From Your Local Workstation</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../data/index.html#hsi">HSI</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../data/index.html#additional-hsi-documentation">Additional HSI Documentation</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../data/index.html#htar">HTAR</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../data/index.html#htar-limitations">HTAR Limitations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../data/index.html#additional-htar-documentation">Additional HTAR Documentation</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../data/index.html#command-line-terminal-tools">Command-Line/Terminal Tools</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../data/index.html#burst-buffer-and-spectral-library">Burst Buffer and Spectral Library</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../software/index.html">Software</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../software/SWList.html">SWList</a></li>
<li class="toctree-l2"><a class="reference internal" href="../software/SWList_flat_table.html">SWList_Flat_Table</a></li>
<li class="toctree-l2"><a class="reference internal" href="../software/SWList_less_wide.html">SWList_Less_Wide</a></li>
<li class="toctree-l2"><a class="reference internal" href="../software/SWList_condensed.html">SWList_condensed</a></li>
<li class="toctree-l2"><a class="reference internal" href="../software/SWList_expanding.html">SWList_expanding_copy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../software/software-news.html">Software News</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../software/software-news.html#frontier-updated-modules-for-cpe-23-12-october-16-2024">Frontier: Updated modules for cpe/23.12 (October 16 2024)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../software/software-news.html#frontier-core-module-october-15-2024">Frontier: Core module (October 15, 2024)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../software/software-news.html#frontier-system-software-update-july-16-2024">Frontier: System Software Update (July 16, 2024)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../software/software-news.html#frontier-user-environment-changes-july-9-2024">Frontier: User Environment Changes (July 9, 2024)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../software/analytics/index.html">ML/DL &amp; Data Analytics</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../software/analytics/ibm-wml-ce.html">IBM Watson Machine Learning CE -&gt; Open CE</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../software/analytics/ibm-wml-ce.html#getting-started">Getting Started</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/analytics/ibm-wml-ce.html#running-distributed-deep-learning-jobs">Running Distributed Deep Learning Jobs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/analytics/ibm-wml-ce.html#setting-up-custom-environments">Setting up Custom Environments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/analytics/ibm-wml-ce.html#best-distributed-deep-learning-performance">Best Distributed Deep Learning Performance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/analytics/ibm-wml-ce.html#troubleshooting-tips">Troubleshooting Tips</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../software/analytics/pbdR.html">R and pbdR on Summit</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../software/analytics/pbdR.html#loading-r">Loading R</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/analytics/pbdR.html#how-to-run-an-r-script">How to Run an R Script</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/analytics/pbdR.html#r-hello-world-example">R Hello World Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/analytics/pbdR.html#pbdr-hello-world-example">pbdR Hello World Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/analytics/pbdR.html#common-r-packages-for-parallelism">Common R Packages for Parallelism</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/analytics/pbdR.html#gpu-computing-with-r">GPU Computing with R</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/analytics/pbdR.html#more-information">More Information</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../software/analytics/nvidia-rapids.html">NVIDIA RAPIDS</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../software/analytics/nvidia-rapids.html#overview">Overview</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/analytics/nvidia-rapids.html#getting-started">Getting Started</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/analytics/nvidia-rapids.html#rapids-on-jupyter">RAPIDS on Jupyter</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/analytics/nvidia-rapids.html#rapids-on-summit">RAPIDS on Summit</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/analytics/nvidia-rapids.html#setting-up-custom-environments">Setting up Custom Environments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/analytics/nvidia-rapids.html#blazingsql-distributed-execution">BlazingSQL Distributed Execution</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../software/analytics/apache-spark.html">Apache Spark</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../software/analytics/apache-spark.html#overview">Overview</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/analytics/apache-spark.html#getting-started">Getting Started</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../software/python/index.html">Python on OLCF Systems</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../software/python/index.html#overview">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="../software/python/index.html#olcf-python-guides">OLCF Python Guides</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../software/python/conda_basics.html">Conda Basics</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/python/parallel_h5py.html">Installing mpi4py and h5py</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/python/cupy.html">Installing CuPy</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/python/sbcast_conda.html">Sbcast Conda Environments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/python/jupyter_envs.html">Jupyter Visibility</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/python/pytorch_summit.html">PyTorch on Summit</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/python/pytorch_frontier.html">PyTorch on Frontier</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../software/python/index.html#module-usage">Module Usage</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../software/python/index.html#base-environment">Base Environment</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/python/index.html#custom-environments">Custom Environments</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../software/python/index.html#how-to-run">How to Run</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../software/python/index.html#summit">Summit</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/python/index.html#frontier-andes">Frontier / Andes</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../software/python/index.html#best-practices">Best Practices</a></li>
<li class="toctree-l3"><a class="reference internal" href="../software/python/index.html#additional-resources">Additional Resources</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../software/profiling/index.html">Profiling Tools</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../software/profiling/Scorep.html">Score-P</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../software/profiling/Scorep.html#overview">Overview</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/profiling/Scorep.html#usage">Usage</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/profiling/Scorep.html#instrumentation">Instrumentation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/profiling/Scorep.html#measurement">Measurement</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/profiling/Scorep.html#profiling">Profiling</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/profiling/Scorep.html#tracing">Tracing</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/profiling/Scorep.html#manual-instrumentation">Manual Instrumentation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/profiling/Scorep.html#score-p-demo-video">Score-P Demo Video</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../software/profiling/TAU.html">Tuning and Analysis Utilities (TAU)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../software/profiling/TAU.html#run-time-environment-variables">Run-Time Environment Variables</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/profiling/TAU.html#compile-time-environment-variables">Compile-Time Environment Variables</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/profiling/TAU.html#miniweather-example-application">MiniWeather Example Application</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/profiling/TAU.html#cuda-profiling-tools-interface">CUDA Profiling Tools Interface</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/profiling/TAU.html#tracing">Tracing</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/profiling/TAU.html#selective-instrumentation">Selective Instrumentation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/profiling/TAU.html#dynamic-phase">Dynamic Phase</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/profiling/TAU.html#static-phase">Static Phase</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/profiling/TAU.html#openmp-offload">OpenMP Offload</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../software/profiling/Vampir.html">Vampir</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../software/profiling/Vampir.html#overview">Overview</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/profiling/Vampir.html#usage">Usage</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/profiling/Vampir.html#vampir-on-a-login-node">Vampir on a Login Node</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/profiling/Vampir.html#vampir-using-vampirserver">Vampir Using VampirServer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/profiling/Vampir.html#vampir-tunneling-to-vampirserver">Vampir Tunneling to VampirServer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/profiling/Vampir.html#vampir-gui-demo">Vampir GUI Demo</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../software/UMS/index.html">User-Managed Software</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../software/UMS/index.html#introduction">Introduction</a></li>
<li class="toctree-l3"><a class="reference internal" href="../software/UMS/index.html#currently-available-user-managed-software">Currently Available User-Managed Software</a></li>
<li class="toctree-l3"><a class="reference internal" href="../software/UMS/index.html#usage">Usage</a></li>
<li class="toctree-l3"><a class="reference internal" href="../software/UMS/index.html#policies">Policies</a></li>
<li class="toctree-l3"><a class="reference internal" href="../software/UMS/index.html#writing-ums-modulefiles">Writing UMS Modulefiles</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../software/workflows/index.html">Workflows</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../software/workflows/index.html#running-workflows-on-olcf-resources">Running Workflows on OLCF Resources</a></li>
<li class="toctree-l3"><a class="reference internal" href="../software/workflows/index.html#workflow-systems">Workflow Systems</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../software/workflows/entk.html">Ensemble Toolkit (EnTK)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/workflows/fireworks.html">FireWorks</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/workflows/mlflow.html">MLflow</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/workflows/parsl.html">Parsl</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/workflows/pmake.html">pmake</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/workflows/swift_t.html">Swift/T</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/workflows/libensemble.html">libEnsemble</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../software/e4s.html">E4S Software Stack</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../software/e4s.html#summit">Summit</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../software/e4s.html#access-via-modulefiles">Access via modulefiles</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/e4s.html#e4s-21-08-packages">E4S 21.08 Packages</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/e4s.html#e4s-21-05-packages">E4S 21.05 Packages</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../software/e4s.html#spock">Spock</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../software/e4s.html#id1">Access via modulefiles</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/e4s.html#id2">E4S 21.08 Packages</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/e4s.html#id3">E4S 21.05 Packages</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../software/spack_environments.html">Spack Environments</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../software/spack_environments.html#purpose">Purpose</a></li>
<li class="toctree-l3"><a class="reference internal" href="../software/spack_environments.html#getting-started">Getting Started</a></li>
<li class="toctree-l3"><a class="reference internal" href="../software/spack_environments.html#add-dependencies-to-the-environment">Add Dependencies to the environment</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../software/spack_environments.html#adding-olcf-modulefiles-as-external-packages">Adding OLCF Modulefiles as External Packages</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/spack_environments.html#adding-user-defined-dependencies-to-the-environment">Adding User-Defined Dependencies to the environment</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../software/spack_environments.html#installing-the-environment">Installing the Environment</a></li>
<li class="toctree-l3"><a class="reference internal" href="../software/spack_environments.html#more-details">More Details</a></li>
<li class="toctree-l3"><a class="reference internal" href="../software/spack_environments.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../software/viz_tools/index.html">Visualization Tools</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../software/viz_tools/visit.html">VisIt</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../software/viz_tools/visit.html#overview">Overview</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/viz_tools/visit.html#installing-and-setting-up-visit">Installing and Setting Up Visit</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/viz_tools/visit.html#remote-gui-usage">Remote GUI Usage</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/viz_tools/visit.html#command-line-example">Command Line Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/viz_tools/visit.html#troubleshooting">Troubleshooting</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/viz_tools/visit.html#additional-resources">Additional Resources</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../software/viz_tools/paraview.html">ParaView</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../software/viz_tools/paraview.html#overview">Overview</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/viz_tools/paraview.html#installing-and-setting-up-paraview">Installing and Setting Up ParaView</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/viz_tools/paraview.html#remote-gui-usage">Remote GUI Usage</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/viz_tools/paraview.html#command-line-example">Command Line Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/viz_tools/paraview.html#troubleshooting">Troubleshooting</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/viz_tools/paraview.html#additional-resources">Additional Resources</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../software/containers_on_summit.html">Containers on Summit</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../software/containers_on_summit.html#basic-information">Basic Information</a></li>
<li class="toctree-l3"><a class="reference internal" href="../software/containers_on_summit.html#setup-before-building">Setup before Building</a></li>
<li class="toctree-l3"><a class="reference internal" href="../software/containers_on_summit.html#build-and-run-workflow">Build and Run Workflow</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../software/containers_on_summit.html#building-a-simple-image">Building a Simple Image</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/containers_on_summit.html#using-a-container-registry-to-build-and-save-your-images">Using a Container Registry to Build and Save your Images</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/containers_on_summit.html#running-a-simple-container-in-a-batch-job">Running a Simple Container in a Batch Job</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/containers_on_summit.html#running-an-mpi-program-with-the-olcf-mpi-base-image">Running an MPI program with the OLCF MPI base image</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/containers_on_summit.html#running-a-single-node-gpu-program-with-the-olcf-mpi-base-image">Running a single node GPU program with the OLCF MPI base image</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/containers_on_summit.html#running-a-cuda-aware-mpi-program-with-the-olcf-mpi-base-image">Running a CUDA-Aware MPI program with the OLCF MPI base image</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../software/containers_on_summit.html#tips-tricks-and-things-to-watch-out-for">Tips, Tricks, and Things to Watch Out For</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../software/containers_on_frontier.html">Containers on Frontier</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../software/containers_on_frontier.html#examples-for-building-and-running-containers">Examples for Building and Running Containers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../software/containers_on_frontier.html#building-and-running-a-container-image-from-a-base-linux-distribution-for-mpi">Building and running a container image from a base Linux distribution for MPI</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/containers_on_frontier.html#pushing-your-apptainer-image-to-an-oci-registry-supporting-oras-e-g-dockerhub">Pushing your Apptainer image to an OCI Registry supporting ORAS (e.g. DockerHub)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/containers_on_frontier.html#building-an-image-on-top-of-an-existing-image-local-docker-image-oci-artifact">Building an image on top of an existing image (local, docker image, OCI artifact)</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../software/containers_on_frontier.html#olcf-base-images-apptainer-modules">OLCF Base Images &amp; Apptainer Modules</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../software/containers_on_frontier.html#base-images">Base Images</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/containers_on_frontier.html#apptainer-modules">Apptainer Modules</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/containers_on_frontier.html#example-workflow">Example Workflow</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../software/containers_on_frontier.html#some-restrictions-and-tips">Some Restrictions and Tips</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../software/debugging/index.html">Debugging</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../software/debugging/index.html#linaro-forge-ddt">Linaro Forge DDT</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../software/debugging/index.html#client-setup-and-usage">Client Setup and Usage</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/debugging/index.html#download">Download</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/debugging/index.html#installation">Installation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/debugging/index.html#configuration">Configuration</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../software/debugging/index.html#gnu-gdb">GNU GDB</a></li>
<li class="toctree-l3"><a class="reference internal" href="../software/debugging/index.html#valgrind">Valgrind</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../training/index.html">Training</a><ul>
<li class="toctree-l2"><a class="reference external" href="https://www.olcf.ornl.gov/for-users/training/training-calendar" target="_blank">OLCF Training Calendar</a></li>
<li class="toctree-l2"><a class="reference external" href="https://github.com/olcf-tutorials" target="_blank">OLCF Tutorials</a></li>
<li class="toctree-l2"><a class="reference internal" href="../training/training_archive.html">OLCF Training Archive</a></li>
<li class="toctree-l2"><a class="reference internal" href="../training/olcf_gpu_hackathons.html">OLCF GPU Hackathons</a></li>
<li class="toctree-l2"><a class="reference internal" href="../training/olcf_gpu_hackathons.html#facultyhack">FacultyHack</a></li>
<li class="toctree-l2"><a class="reference external" href="https://vimeo.com/channels/olcftraining" target="_blank">OLCF Vimeo Channel</a></li>
<li class="toctree-l2"><a class="reference internal" href="../training/index.html#new-user-quick-start">New User Quick Start</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quantum/index.html">Quantum</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../quantum/quantum_access.html">Quantum Computing User Program (QCUP) Access</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../quantum/quantum_access.html#qcup-priorities">QCUP Priorities</a></li>
<li class="toctree-l3"><a class="reference internal" href="../quantum/quantum_access.html#project-allocations">Project Allocations</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../quantum/quantum_access.html#what-happens-after-a-project-request-is-approved">What happens after a project request is approved?</a></li>
<li class="toctree-l4"><a class="reference internal" href="../quantum/quantum_access.html#project-renewals">Project Renewals</a></li>
<li class="toctree-l4"><a class="reference internal" href="../quantum/quantum_access.html#closeout-and-quarterly-reports">Closeout and Quarterly Reports</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../quantum/quantum_access.html#user-accounts">User Accounts</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../quantum/quantum_access.html#checking-the-status-of-your-application">Checking the status of your application</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../quantum/quantum_access.html#accessing-quantum-resources">Accessing Quantum Resources</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../quantum/quantum_access.html#ibm-quantum-computing">IBM Quantum Computing</a></li>
<li class="toctree-l4"><a class="reference internal" href="../quantum/quantum_access.html#quantinuum">Quantinuum</a></li>
<li class="toctree-l4"><a class="reference internal" href="../quantum/quantum_access.html#rigetti">Rigetti</a></li>
<li class="toctree-l4"><a class="reference internal" href="../quantum/quantum_access.html#ionq">IonQ</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../quantum/quantum_access.html#publication-citations">Publication Citations</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../quantum/quantum_systems/index.html">Quantum Systems</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../quantum/quantum_systems/ibm_quantum.html">IBM Quantum</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../quantum/quantum_systems/ibm_quantum.html#overview">Overview</a></li>
<li class="toctree-l4"><a class="reference internal" href="../quantum/quantum_systems/ibm_quantum.html#connecting">Connecting</a></li>
<li class="toctree-l4"><a class="reference internal" href="../quantum/quantum_systems/ibm_quantum.html#running-jobs-queue-policies">Running Jobs &amp; Queue Policies</a></li>
<li class="toctree-l4"><a class="reference internal" href="../quantum/quantum_systems/ibm_quantum.html#checking-system-availability-capability">Checking System Availability &amp; Capability</a></li>
<li class="toctree-l4"><a class="reference internal" href="../quantum/quantum_systems/ibm_quantum.html#software">Software</a></li>
<li class="toctree-l4"><a class="reference internal" href="../quantum/quantum_systems/ibm_quantum.html#additional-resources">Additional Resources</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../quantum/quantum_systems/rigetti.html">Rigetti</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../quantum/quantum_systems/rigetti.html#overview">Overview</a></li>
<li class="toctree-l4"><a class="reference internal" href="../quantum/quantum_systems/rigetti.html#connecting">Connecting</a></li>
<li class="toctree-l4"><a class="reference internal" href="../quantum/quantum_systems/rigetti.html#running-jobs">Running Jobs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../quantum/quantum_systems/rigetti.html#allocations-credit-usage">Allocations &amp; Credit Usage</a></li>
<li class="toctree-l4"><a class="reference internal" href="../quantum/quantum_systems/rigetti.html#data-storage-policies">Data Storage Policies</a></li>
<li class="toctree-l4"><a class="reference internal" href="../quantum/quantum_systems/rigetti.html#software">Software</a></li>
<li class="toctree-l4"><a class="reference internal" href="../quantum/quantum_systems/rigetti.html#additional-resources">Additional Resources</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../quantum/quantum_systems/quantinuum.html">Quantinuum</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../quantum/quantum_systems/quantinuum.html#overview">Overview</a></li>
<li class="toctree-l4"><a class="reference internal" href="../quantum/quantum_systems/quantinuum.html#connecting">Connecting</a></li>
<li class="toctree-l4"><a class="reference internal" href="../quantum/quantum_systems/quantinuum.html#running-jobs-queue-policies">Running Jobs &amp; Queue Policies</a></li>
<li class="toctree-l4"><a class="reference internal" href="../quantum/quantum_systems/quantinuum.html#allocations-credit-usage">Allocations &amp; Credit Usage</a></li>
<li class="toctree-l4"><a class="reference internal" href="../quantum/quantum_systems/quantinuum.html#software">Software</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../quantum/quantum_systems/ionq.html">IonQ</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../quantum/quantum_systems/ionq.html#overview">Overview</a></li>
<li class="toctree-l4"><a class="reference internal" href="../quantum/quantum_systems/ionq.html#ionq-systems">IonQ systems</a></li>
<li class="toctree-l4"><a class="reference internal" href="../quantum/quantum_systems/ionq.html#connecting">Connecting</a></li>
<li class="toctree-l4"><a class="reference internal" href="../quantum/quantum_systems/ionq.html#running-jobs-queue-policies">Running Jobs &amp; Queue Policies</a></li>
<li class="toctree-l4"><a class="reference internal" href="../quantum/quantum_systems/ionq.html#allocations-credit-usage">Allocations &amp; Credit Usage</a></li>
<li class="toctree-l4"><a class="reference internal" href="../quantum/quantum_systems/ionq.html#checking-system-availability-capability">Checking System Availability &amp; Capability</a></li>
<li class="toctree-l4"><a class="reference internal" href="../quantum/quantum_systems/ionq.html#additional-resources">Additional Resources</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../quantum/quantum_software/index.html">Quantum Software</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../quantum/quantum_software/hybrid_hpc.html">Quantum Software on HPC Systems</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../quantum/quantum_software/hybrid_hpc.html#overview">Overview</a></li>
<li class="toctree-l4"><a class="reference internal" href="../quantum/quantum_software/hybrid_hpc.html#qiskit">Qiskit</a></li>
<li class="toctree-l4"><a class="reference internal" href="../quantum/quantum_software/hybrid_hpc.html#pyquil-forest-sdk-rigetti">PyQuil/Forest SDK (Rigetti)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../quantum/quantum_software/hybrid_hpc.html#pennylane">PennyLane</a></li>
<li class="toctree-l4"><a class="reference internal" href="../quantum/quantum_software/hybrid_hpc.html#pytket">Pytket</a></li>
<li class="toctree-l4"><a class="reference internal" href="../quantum/quantum_software/hybrid_hpc.html#cuda-q">CUDA-Q</a></li>
<li class="toctree-l4"><a class="reference internal" href="../quantum/quantum_software/hybrid_hpc.html#batch-jobs">Batch Jobs</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../quantum/hello_qcup.html">Hello QCUP Scripts</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../quantum/hello_qcup.html#overview">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="../quantum/hello_qcup.html#ibm-quantum">IBM Quantum</a></li>
<li class="toctree-l3"><a class="reference internal" href="../quantum/hello_qcup.html#quantinuum">Quantinuum</a></li>
<li class="toctree-l3"><a class="reference internal" href="../quantum/hello_qcup.html#ionq">IonQ</a></li>
<li class="toctree-l3"><a class="reference internal" href="../quantum/hello_qcup.html#rigetti">Rigetti</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../quantum/quantum_faq.html">Frequently Asked Questions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../quantum/quantum_faq.html#how-do-quantum-computers-differ-from-classical-computers">How do quantum computers differ from classical computers?</a></li>
<li class="toctree-l3"><a class="reference internal" href="../quantum/quantum_faq.html#what-is-a-qubit">What is a qubit?</a></li>
<li class="toctree-l3"><a class="reference internal" href="../quantum/quantum_faq.html#how-do-i-access-the-olcf-quantum-computing-resources">How do I access the OLCF quantum computing resources?</a></li>
<li class="toctree-l3"><a class="reference internal" href="../quantum/quantum_faq.html#what-happens-after-i-apply-for-access-to-qcup">What happens after I apply for access to QCUP?</a></li>
<li class="toctree-l3"><a class="reference internal" href="../quantum/quantum_faq.html#i-formerly-had-access-to-quantum-resources-but-my-backends-lattices-etc-have-disappeared-what-do-i-do">I formerly had access to quantum resources, but my backends/lattices/etc. have disappeared, what do I do?</a></li>
<li class="toctree-l3"><a class="reference internal" href="../quantum/quantum_faq.html#i-applied-to-a-quantum-computing-resource-via-the-vendor-website-but-dont-have-access-what-do-i-do">I applied to a quantum computing resource via the vendor website, but dont have access; what do I do?</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../spi/index.html">Scalable Protected Infrastructure (SPI)</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../spi/index.html#what-is-spi">What is SPI</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../spi/index.html#what-is-citadel">What is Citadel</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../spi/index.html#new-user-quickstart">New User QuickStart</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../spi/index.html#notable-differences">Notable Differences</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../spi/index.html#allocations-and-user-accounts">Allocations and User Accounts</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../spi/index.html#allocations-projects">Allocations (Projects)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../spi/index.html#requesting-a-new-allocation-project">Requesting a New Allocation (Project)</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../spi/index.html#user-accounts">User Accounts</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../spi/index.html#requesting-a-new-user-account">Requesting a New User Account</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../spi/index.html#available-resources">Available Resources</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../spi/index.html#compute"><span class="xref std std-ref">Compute</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../spi/index.html#file-systems"><span class="xref std std-ref">File Systems</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../spi/index.html#data-transfer"><span class="xref std std-ref">Data Transfer</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../spi/index.html#ip-whitelisting">IP Whitelisting</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../spi/index.html#whitelisting-an-ip-or-range">Whitelisting an IP or range</a></li>
<li class="toctree-l3"><a class="reference internal" href="../spi/index.html#finding-your-ip">Finding your IP</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../spi/index.html#citadel">Citadel</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../spi/index.html#login-nodes">Login Nodes</a></li>
<li class="toctree-l3"><a class="reference internal" href="../spi/index.html#connecting">Connecting</a></li>
<li class="toctree-l3"><a class="reference internal" href="../spi/index.html#building-software">Building Software</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../spi/index.html#external-repositories">External Repositories</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../spi/index.html#running-batch-jobs">Running Batch Jobs</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../spi/index.html#spi-file-systems">File Systems</a></li>
<li class="toctree-l2"><a class="reference internal" href="../spi/index.html#spi-data-transfer">Data Transfer</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../ace_testbed/index.html">Advanced Computing Ecosystem Testbed (ACE)</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../ace_testbed/defiant_quick_start_guide.html">Defiant Quick-Start Guide</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../ace_testbed/defiant_quick_start_guide.html#system-overview">System Overview</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../ace_testbed/defiant_quick_start_guide.html#defiant-compute-nodes">Defiant Compute Nodes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../ace_testbed/defiant_quick_start_guide.html#system-interconnect">System Interconnect</a></li>
<li class="toctree-l4"><a class="reference internal" href="../ace_testbed/defiant_quick_start_guide.html#file-systems">File Systems</a></li>
<li class="toctree-l4"><a class="reference internal" href="../ace_testbed/defiant_quick_start_guide.html#gpus">GPUs</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../ace_testbed/defiant_quick_start_guide.html#connecting">Connecting</a></li>
<li class="toctree-l3"><a class="reference internal" href="../ace_testbed/defiant_quick_start_guide.html#data-and-storage">Data and Storage</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../ace_testbed/defiant_quick_start_guide.html#nfs-filesystem">NFS Filesystem</a></li>
<li class="toctree-l4"><a class="reference internal" href="../ace_testbed/defiant_quick_start_guide.html#lustre-filesystem-polis">Lustre Filesystem (Polis)</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../ace_testbed/defiant_quick_start_guide.html#programming-environment">Programming Environment</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../ace_testbed/defiant_quick_start_guide.html#environment-modules-lmod">Environment Modules (Lmod)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../ace_testbed/defiant_quick_start_guide.html#compilers">Compilers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../ace_testbed/defiant_quick_start_guide.html#mpi">MPI</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../ace_testbed/defiant_quick_start_guide.html#compiling">Compiling</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../ace_testbed/defiant_quick_start_guide.html#id3">MPI</a></li>
<li class="toctree-l4"><a class="reference internal" href="../ace_testbed/defiant_quick_start_guide.html#openmp">OpenMP</a></li>
<li class="toctree-l4"><a class="reference internal" href="../ace_testbed/defiant_quick_start_guide.html#openmp-gpu-offload">OpenMP GPU Offload</a></li>
<li class="toctree-l4"><a class="reference internal" href="../ace_testbed/defiant_quick_start_guide.html#hip">HIP</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../ace_testbed/defiant_quick_start_guide.html#running-jobs">Running Jobs</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../ace_testbed/defiant_quick_start_guide.html#slurm-workload-manager">Slurm Workload Manager</a></li>
<li class="toctree-l4"><a class="reference internal" href="../ace_testbed/defiant_quick_start_guide.html#slurm-compute-node-partitions">Slurm Compute Node Partitions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../ace_testbed/defiant_quick_start_guide.html#process-and-thread-mapping">Process and Thread Mapping</a></li>
<li class="toctree-l4"><a class="reference internal" href="../ace_testbed/defiant_quick_start_guide.html#nvme-usage">NVMe Usage</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../ace_testbed/defiant_quick_start_guide.html#container-usage">Container Usage</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../ace_testbed/defiant_quick_start_guide.html#setup-before-building">Setup before Building</a></li>
<li class="toctree-l4"><a class="reference internal" href="../ace_testbed/defiant_quick_start_guide.html#build-and-run-workflow">Build and Run Workflow</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../ace_testbed/defiant_quick_start_guide.html#getting-help">Getting Help</a></li>
<li class="toctree-l3"><a class="reference internal" href="../ace_testbed/defiant_quick_start_guide.html#known-issues">Known Issues</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../contributing/index.html">Contributing to these docs</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../contributing/index.html#submitting-suggestions">Submitting suggestions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../contributing/index.html#authoring-content">Authoring content</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../contributing/index.html#setup-authoring-environment">Setup authoring environment</a></li>
<li class="toctree-l3"><a class="reference internal" href="../contributing/index.html#edit-the-docs">Edit the docs</a></li>
<li class="toctree-l3"><a class="reference internal" href="../contributing/index.html#resources">Resources</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../contributing/index.html#github-guidelines">GitHub Guidelines</a></li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu"  style="background: #efefef" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">OLCF User Documentation</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content style-external-links">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="index.html">Systems</a></li>
      <li class="breadcrumb-item active">Frontier User Guide</li>
      <li class="wy-breadcrumbs-aside">
              <a href="https://github.com/olcf/olcf-user-docs/blob/master/systems/frontier_user_guide.rst" class="fa fa-github"> Edit on GitHub</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="frontier-user-guide">
<span id="id1"></span><h1>Frontier User Guide<a class="headerlink" href="#frontier-user-guide" title="Link to this heading"></a></h1>
<section id="system-overview">
<span id="id2"></span><h2>System Overview<a class="headerlink" href="#system-overview" title="Link to this heading"></a></h2>
<p>Frontier is a HPE Cray EX supercomputer located at the Oak Ridge Leadership Computing Facility. With a theoretical peak double-precision performance of approximately 2 exaflops (2 quintillion calculations per second), it is the fastest system in the world for a wide range of traditional computational science applications. The system has 74 Olympus rack HPE cabinets, each with 128 AMD compute nodes, and a total of 9,408 AMD compute nodes.</p>
<section id="frontier-compute-nodes">
<span id="frontier-nodes"></span><h3>Frontier Compute Nodes<a class="headerlink" href="#frontier-compute-nodes" title="Link to this heading"></a></h3>
<p>Each Frontier compute node consists of [1x] 64-core AMD Optimized 3rd Gen EPYC CPU (with 2 hardware threads per physical core) with access to 512 GB of DDR4 memory. Each node also contains [4x] AMD MI250X, each with 2 Graphics Compute Dies (GCDs) for a total of 8 GCDs per node. The programmer can think of the 8 GCDs as 8 separate GPUs, each having 64 GB of high-bandwidth memory (HBM2E). The CPU is connected to each GCD via Infinity Fabric CPU-GPU, allowing a peak host-to-device (H2D) and device-to-host (D2H) bandwidth of 36+36 GB/s. The 2 GCDs on the same MI250X are connected with Infinity Fabric GPU-GPU with a peak bandwidth of 200 GB/s. The GCDs on different MI250X are connected with Infinity Fabric GPU-GPU in the arrangement shown in the Frontier Node Diagram below, where the peak bandwidth ranges from 50-100 GB/s based on the number of Infinity Fabric connections between individual GCDs.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>TERMINOLOGY:</strong></p>
<p>The 8 GCDs contained in the 4 MI250X will show as 8 separate GPUs according to Slurm, <code class="docutils literal notranslate"><span class="pre">ROCR_VISIBLE_DEVICES</span></code>, and the ROCr runtime, so from this point forward in the quick-start guide, we will simply refer to the GCDs as GPUs.</p>
</div>
<a class="reference internal image-reference" href="../_images/Frontier_Node_Diagram.jpg"><img alt="Frontier node architecture diagram" class="align-center" src="../_images/Frontier_Node_Diagram.jpg" style="width: 100%;" /></a>
<div class="admonition note" id="numa-note">
<p class="admonition-title">Note</p>
<p>There are [4x] NUMA domains per node and [2x] L3 cache regions per NUMA for a total of [8x] L3 cache regions. The 8 GPUs are each associated with one of the L3 regions as follows:</p>
<p>NUMA 0:</p>
<ul class="simple">
<li><p>hardware threads 000-007, 064-071 | GPU 4</p></li>
<li><p>hardware threads 008-015, 072-079 | GPU 5</p></li>
</ul>
<p>NUMA 1:</p>
<ul class="simple">
<li><p>hardware threads 016-023, 080-087 | GPU 2</p></li>
<li><p>hardware threads 024-031, 088-095 | GPU 3</p></li>
</ul>
<p>NUMA 2:</p>
<ul class="simple">
<li><p>hardware threads 032-039, 096-103 | GPU 6</p></li>
<li><p>hardware threads 040-047, 104-111 | GPU 7</p></li>
</ul>
<p>NUMA 3:</p>
<ul class="simple">
<li><p>hardware threads 048-055, 112-119 | GPU 0</p></li>
<li><p>hardware threads 056-063, 120-127 | GPU 1</p></li>
</ul>
</div>
<p>By default, Frontier reserves the first core in each L3 cache region. Frontier uses low-noise mode,
which constrains all system processes to core 0. Low-noise mode cannot be disabled by users.
In addition, Frontier uses SLURM core specialization (<code class="docutils literal notranslate"><span class="pre">-S</span> <span class="pre">8</span></code> flag at job allocation time, e.g., <code class="docutils literal notranslate"><span class="pre">sbatch</span></code>)
to reserve one core from each L3 cache region, leaving 56 allocatable cores. Set <code class="docutils literal notranslate"><span class="pre">-S</span> <span class="pre">0</span></code> at job allocation to override this setting.</p>
</section>
<section id="node-types">
<h3>Node Types<a class="headerlink" href="#node-types" title="Link to this heading"></a></h3>
<p>On Frontier, there are two major types of nodes you will encounter: Login and Compute. While these are
similar in terms of hardware (see: <a class="reference internal" href="#frontier-nodes"><span class="std std-ref">Frontier Compute Nodes</span></a>), they differ considerably in their intended
use.</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Node Type</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Login</p></td>
<td><p>When you connect to Frontier, youre placed on a login node. This
is the place to write/edit/compile your code, manage data, submit jobs, etc. You
should never launch parallel jobs from a login node nor should you run threaded
jobs on a login node. Login nodes are shared resources that are in use by many
users simultaneously.</p></td>
</tr>
<tr class="row-odd"><td><p>Compute</p></td>
<td><p>Most of the nodes on Frontier are compute nodes. These are where
your parallel job executes. Theyre accessed via the <code class="docutils literal notranslate"><span class="pre">srun</span></code> command.</p></td>
</tr>
</tbody>
</table>
</section>
<section id="system-interconnect">
<h3>System Interconnect<a class="headerlink" href="#system-interconnect" title="Link to this heading"></a></h3>
<p>The Frontier nodes are connected with [4x] HPE Slingshot 200 Gbps (25 GB/s) NICs providing a node-injection bandwidth of 800 Gbps (100 GB/s).</p>
</section>
<section id="file-systems">
<h3>File Systems<a class="headerlink" href="#file-systems" title="Link to this heading"></a></h3>
<p>Frontier is connected to Orion, a parallel filesystem based on Lustre and HPE ClusterStor, with a 679 PB usable
namespace (<code class="docutils literal notranslate"><span class="pre">/lustre/orion/</span></code>). In addition to Frontier, Orion is available on the OLCFs data transfer nodes and on the Andes cluster.
Orion is not available from Summit and Frontier does not mount Summits Alpine2 filesystem.
Frontier also has access to the center-wide NFS-based filesystem (which provides user and project home areas).
Each compute node has two 1.92TB Non-Volatile Memory storage devices. See <a class="reference internal" href="#frontier-data-storage"><span class="std std-ref">Data and Storage</span></a> for more information.</p>
<p>Projects with a Frontier allocation also receive an archival storage area on Kronos. For more information on using Kronos, see the <a class="reference internal" href="../data/index.html#kronos"><span class="std std-ref">Kronos Nearline Archival Storage System</span></a> seciton.</p>
</section>
<section id="operating-system">
<h3>Operating System<a class="headerlink" href="#operating-system" title="Link to this heading"></a></h3>
<p>Frontier is running Cray OS 2.4 based on SUSE Linux Enterprise Server (SLES) version 15.4.</p>
</section>
<section id="gpus">
<h3>GPUs<a class="headerlink" href="#gpus" title="Link to this heading"></a></h3>
<p>Each Frontier compute node contains 4 AMD MI250X. The AMD MI250X has a peak performance of 47.8 TFLOPS in vector-based double-precision for modeling and simulation. Each MI250X contains 2 GPUs, where each GPU has a peak performance of 23.9 TFLOPS (vector-based double-precision), 110 compute units, and 64 GB of high-bandwidth memory (HBM2) which can be accessed at a peak of 1.6 TB/s. The 2 GPUs on an MI250X are connected with Infinity Fabric with a bandwidth of 200 GB/s (in each direction simultaneously).</p>
</section>
</section>
<section id="connecting">
<h2>Connecting<a class="headerlink" href="#connecting" title="Link to this heading"></a></h2>
<p>To connect to Frontier, <code class="docutils literal notranslate"><span class="pre">ssh</span></code> to <code class="docutils literal notranslate"><span class="pre">frontier.olcf.ornl.gov</span></code>. For example:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>ssh<span class="w"> </span>&lt;username&gt;@frontier.olcf.ornl.gov
</pre></div>
</div>
<p>For more information on connecting to OLCF resources, see <a class="reference internal" href="../connecting/index.html#connecting-to-olcf"><span class="std std-ref">Connecting for the first time</span></a>.</p>
<p>By default, connecting to Frontier will automatically place the user on a random login node. If you need to access a specific login node, you will <code class="docutils literal notranslate"><span class="pre">ssh</span></code> to that node after your intial connection to Frontier.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="o">[</span>&lt;username&gt;@login12.frontier<span class="w"> </span>~<span class="o">]</span>$<span class="w"> </span>ssh<span class="w"> </span>&lt;username&gt;@login01.frontier.olcf.ornl.gov
</pre></div>
</div>
<p>Users can connect to any of the 17 Frontier login nodes by replacing <code class="docutils literal notranslate"><span class="pre">login01</span></code> with their login node of choice.</p>
<hr class="docutils" />
</section>
<section id="data-and-storage">
<span id="frontier-data-storage"></span><h2>Data and Storage<a class="headerlink" href="#data-and-storage" title="Link to this heading"></a></h2>
<section id="transition-from-alpine-to-orion">
<h3>Transition from Alpine to Orion<a class="headerlink" href="#transition-from-alpine-to-orion" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p>Frontier mountsOrion, a parallel filesystem basedon Lustre and HPE ClusterStor, with a 679 PB usable namespace (/lustre/orion/). Inaddition to Frontier, Orion isavailable on the OLCFs data transfer nodes. It is notavailablefrom Summit.</p></li>
<li><p>On Alpine, there was no user-exposed concept of file striping, the process of dividing a file between the storage elements of the filesystem. Orion uses a feature called Progressive File Layout (PFL) that changes the striping of files as they grow. Because of this, we ask users not to manually adjust the file striping. If you feel the default striping behavior of Orion is not meeting your needs, please contact <a class="reference external" href="mailto:help&#37;&#52;&#48;olcf&#46;ornl&#46;gov" target="_blank">help<span>&#64;</span>olcf<span>&#46;</span>ornl<span>&#46;</span>gov</a>.</p></li>
<li><p>As with Alpine, files older than 90 days are purged from Orion. Please plan your data management and lifecycle at OLCF before generating the data.</p></li>
</ul>
<p>For more detailed information about center-wide file systems and data archiving available on Frontier, please refer to the pages on <a class="reference internal" href="../data/index.html#data-storage-and-transfers"><span class="std std-ref">Data Storage and Transfers</span></a>. The subsections below give a quick overview of NFS, Lustre, abd archival storage spaces as well as the on node NVMe Burst Buffers (SSDs).</p>
</section>
<section id="lfs-setstripe-wrapper">
<h3>LFS setstripe wrapper<a class="headerlink" href="#lfs-setstripe-wrapper" title="Link to this heading"></a></h3>
<p>The OLCF provides a wrapper for the <code class="docutils literal notranslate"><span class="pre">lfs</span> <span class="pre">setstripe</span></code> command that simplifies the process of striping files. The wrapper will enforce that certain settings are used to ensure that striping is done correctly. This will help to ensure good performance for users as well as prevent filesystem issues that could arise from incorrect striping practices. The wrapper is accessible via the <code class="docutils literal notranslate"><span class="pre">lfs-wrapper</span></code> module and will soon be added to the default environment on Frontier.</p>
<p>Orion is different than other Lustre filesystems that you may have used previously. To make effective use of Orion and to help ensure that the filesystem performs well for all users, it is important that you do the following:</p>
<ul class="simple">
<li><p>Use the <cite>capacity</cite> OST pool tier (e.g. <code class="docutils literal notranslate"><span class="pre">lfs</span> <span class="pre">setstripe</span> <span class="pre">-p</span> <span class="pre">capacity</span></code>)</p></li>
<li><p>Stripe across no more than 450 OSTs (e.g. <code class="docutils literal notranslate"><span class="pre">lfs</span> <span class="pre">setstripe</span> <span class="pre">-c</span></code> &lt;= 450)</p></li>
</ul>
<p>When the module is active in your environment, the wrapper will enforce the above settings. The wrapper will also do the following:</p>
<ul class="simple">
<li><p>If a user provides a stripe count of -1 (e.g. <code class="docutils literal notranslate"><span class="pre">lfs</span> <span class="pre">setstripe</span> <span class="pre">-c</span> <span class="pre">-1</span></code>) the wrapper will set the stripe count to the maximum allowed by the filesystem (currently 450)</p></li>
<li><p>If a user provides a stripe count of 0 (e.g. <code class="docutils literal notranslate"><span class="pre">lfs</span> <span class="pre">setstripe</span> <span class="pre">-c</span> <span class="pre">0</span></code>) the wrapper will use the OLCF default striping command which has been optimized by the OLCF filesystem managers: <code class="docutils literal notranslate"><span class="pre">lfs</span> <span class="pre">setstripe</span> <span class="pre">-E</span> <span class="pre">256K</span> <span class="pre">-L</span> <span class="pre">mdt</span> <span class="pre">-E</span> <span class="pre">8M</span> <span class="pre">-c</span> <span class="pre">1</span> <span class="pre">-S</span> <span class="pre">1M</span> <span class="pre">-p</span> <span class="pre">performance</span> <span class="pre">-z</span> <span class="pre">64M</span> <span class="pre">-E</span> <span class="pre">128G</span> <span class="pre">-c</span> <span class="pre">1</span> <span class="pre">-S</span> <span class="pre">1M</span> <span class="pre">-z</span> <span class="pre">16G</span> <span class="pre">-p</span> <span class="pre">capacity</span> <span class="pre">-E</span> <span class="pre">-1</span> <span class="pre">-z</span> <span class="pre">256G</span> <span class="pre">-c</span> <span class="pre">8</span> <span class="pre">-S</span> <span class="pre">1M</span> <span class="pre">-p</span> <span class="pre">capacity</span></code></p></li>
</ul>
<p>Please contact the OLCF User Assistance Center if you have any questions about using the wrapper or if you encounter any issues.</p>
</section>
<section id="nfs-filesystem">
<h3>NFS Filesystem<a class="headerlink" href="#nfs-filesystem" title="Link to this heading"></a></h3>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Area</p></th>
<th class="head"><p>Path</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Permissions</p></th>
<th class="head"><p>Quota</p></th>
<th class="head"><p>Backups</p></th>
<th class="head"><p>Purged</p></th>
<th class="head"><p>Retention</p></th>
<th class="head"><p>On Compute Nodes</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>User Home</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">/ccs/home/[userid]</span></code></p></td>
<td><p>NFS</p></td>
<td><p>User set</p></td>
<td><p>50 GB</p></td>
<td><p>Yes</p></td>
<td><p>No</p></td>
<td><p>90 days</p></td>
<td><p>Yes</p></td>
</tr>
<tr class="row-odd"><td><p>Project Home</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">/ccs/proj/[projid]</span></code></p></td>
<td><p>NFS</p></td>
<td><p>770</p></td>
<td><p>50 GB</p></td>
<td><p>Yes</p></td>
<td><p>No</p></td>
<td><p>90 days</p></td>
<td><p>Yes</p></td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Though the NFS filesystems User Home and Project Home areas are read/write from Frontiers compute nodes,
we strongly recommend that users launch and run jobs from the Lustre Orion parallel filesystem
instead due to its larger storage capacity and superior performance. Please see below for Lustre
Orion filesystem storage areas and paths.</p>
</div>
</section>
<section id="lustre-filesystem">
<h3>Lustre Filesystem<a class="headerlink" href="#lustre-filesystem" title="Link to this heading"></a></h3>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Area</p></th>
<th class="head"><p>Path</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Permissions</p></th>
<th class="head"><p>Quota</p></th>
<th class="head"><p>Backups</p></th>
<th class="head"><p>Purged</p></th>
<th class="head"><p>Retention</p></th>
<th class="head"><p>On Compute Nodes</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Member Work</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">/lustre/orion/[projid]/scratch/[userid]</span></code></p></td>
<td><p>Lustre HPE ClusterStor</p></td>
<td><p>700</p></td>
<td><p>50 TB</p></td>
<td><p>No</p></td>
<td><p>90 days</p></td>
<td><p>N/A</p></td>
<td><p>Yes</p></td>
</tr>
<tr class="row-odd"><td><p>Project Work</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">/lustre/orion/[projid]/proj-shared</span></code></p></td>
<td><p>Lustre HPE ClusterStor</p></td>
<td><p>770</p></td>
<td><p>50 TB</p></td>
<td><p>No</p></td>
<td><p>90 days</p></td>
<td><p>N/A</p></td>
<td><p>Yes</p></td>
</tr>
<tr class="row-even"><td><p>World Work</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">/lustre/orion/[projid]/world-shared</span></code></p></td>
<td><p>Lustre HPE ClusterStor</p></td>
<td><p>775</p></td>
<td><p>50 TB</p></td>
<td><p>No</p></td>
<td><p>90 days</p></td>
<td><p>N/A</p></td>
<td><p>Yes</p></td>
</tr>
</tbody>
</table>
</section>
<section id="kronos-archival-storage">
<h3>Kronos Archival Storage<a class="headerlink" href="#kronos-archival-storage" title="Link to this heading"></a></h3>
<p>Please note that the Kronos is not mounted directly onto Frontier nodes. There are two main methods for accessing and moving data to/from Kronos, either with standard cli utilities (scp, rsync, etc.) and via Globus using the OLCF Kronos collection. For more information on using Kronos, see the <a class="reference internal" href="../data/index.html#kronos"><span class="std std-ref">Kronos Nearline Archival Storage System</span></a> section.</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Area</p></th>
<th class="head"><p>Path</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Permissions</p></th>
<th class="head"><p>Quota</p></th>
<th class="head"><p>Backups</p></th>
<th class="head"><p>Purged</p></th>
<th class="head"><p>Retention</p></th>
<th class="head"><p>On Compute Nodes</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Member Archive</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">/nl/kronos/olcf/[projid]/users/$USER</span></code></p></td>
<td><p>Nearline</p></td>
<td><p>700</p></td>
<td><p>200 TB*</p></td>
<td><p>No</p></td>
<td><p>No</p></td>
<td><p>90 days</p></td>
<td><p>No</p></td>
</tr>
<tr class="row-odd"><td><p>Project Archive</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">/nl/kronos/olcf/[projid]/proj-shared</span></code></p></td>
<td><p>Nearline</p></td>
<td><p>770</p></td>
<td><p>200 TB*</p></td>
<td><p>No</p></td>
<td><p>No</p></td>
<td><p>90 days</p></td>
<td><p>No</p></td>
</tr>
<tr class="row-even"><td><p>World Archive</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">/nl/kronos/olcf/[projid]/world-shared</span></code></p></td>
<td><p>Nearline</p></td>
<td><p>775</p></td>
<td><p>200 TB*</p></td>
<td><p>No</p></td>
<td><p>No</p></td>
<td><p>90 days</p></td>
<td><p>No</p></td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The three archival storage areas above share a single 200TB per project quota.</p>
</div>
</section>
<section id="nvme">
<h3>NVMe<a class="headerlink" href="#nvme" title="Link to this heading"></a></h3>
<p>Each compute node on Frontier has [2x] 1.92TB<strong>N</strong>on-<strong>V</strong>olatile <strong>Me</strong>mory (NVMe) storage devices (SSDs), colloquially known as a Burst Buffer with a peak sequential performance of 5500 MB/s (read) and 2000 MB/s (write). The purpose of the Burst Buffer system is to bring improved I/O performance to appropriate workloads. Users are not required to use the NVMes. Data can also be written directly to the parallel filesystem.</p>
<figure class="align-center" id="id28">
<img alt="../_images/frontier_nvme_arch.png" src="../_images/frontier_nvme_arch.png" />
<figcaption>
<p><span class="caption-text">The NVMes on Frontier are local to each node.</span><a class="headerlink" href="#id28" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
<section id="nvme-usage">
<h3>NVMe Usage<a class="headerlink" href="#nvme-usage" title="Link to this heading"></a></h3>
<p>To use the NVMe, users must request access during job allocation using the <code class="docutils literal notranslate"><span class="pre">-C</span> <span class="pre">nvme</span></code> option to <code class="docutils literal notranslate"><span class="pre">sbatch</span></code>, <code class="docutils literal notranslate"><span class="pre">salloc</span></code>, or <code class="docutils literal notranslate"><span class="pre">srun</span></code>. Once the devices have been granted to a job, users can access them at <code class="docutils literal notranslate"><span class="pre">/mnt/bb/&lt;userid&gt;</span></code>. <strong>Users are responsible for moving data to/from the NVMe before/after their jobs</strong>. Here is a simple example script:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="c1">#SBATCH -A &lt;projid&gt;</span>
<span class="c1">#SBATCH -J nvme_test</span>
<span class="c1">#SBATCH -o %x-%j.out</span>
<span class="c1">#SBATCH -t 00:05:00</span>
<span class="c1">#SBATCH -p batch</span>
<span class="c1">#SBATCH -N 1</span>
<span class="c1">#SBATCH -C nvme</span>

date

<span class="c1"># Change directory to user scratch space (GPFS)</span>
<span class="nb">cd</span><span class="w"> </span>/gpfs/alpine/&lt;projid&gt;/scratch/&lt;userid&gt;

<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot; &quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;*****ORIGINAL FILE*****&quot;</span>
cat<span class="w"> </span>test.txt
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;***********************&quot;</span>

<span class="c1"># Move file from GPFS to SSD</span>
mv<span class="w"> </span>test.txt<span class="w"> </span>/mnt/bb/&lt;userid&gt;

<span class="c1"># Edit file from compute node</span>
srun<span class="w"> </span>-n1<span class="w"> </span>hostname<span class="w"> </span>&gt;&gt;<span class="w"> </span>/mnt/bb/&lt;userid&gt;/test.txt

<span class="c1"># Move file from SSD back to GPFS</span>
mv<span class="w"> </span>/mnt/bb/&lt;userid&gt;/test.txt<span class="w"> </span>.

<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot; &quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;*****UPDATED FILE******&quot;</span>
cat<span class="w"> </span>test.txt
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;***********************&quot;</span>
</pre></div>
</div>
<p>And here is the output from the script:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>cat<span class="w"> </span>nvme_test-&lt;jobid&gt;.out

*****ORIGINAL<span class="w"> </span>FILE*****
This<span class="w"> </span>is<span class="w"> </span>my<span class="w"> </span>file.<span class="w"> </span>There<span class="w"> </span>are<span class="w"> </span>many<span class="w"> </span>like<span class="w"> </span>it<span class="w"> </span>but<span class="w"> </span>this<span class="w"> </span>one<span class="w"> </span>is<span class="w"> </span>mine.
***********************

*****UPDATED<span class="w"> </span>FILE******
This<span class="w"> </span>is<span class="w"> </span>my<span class="w"> </span>file.<span class="w"> </span>There<span class="w"> </span>are<span class="w"> </span>many<span class="w"> </span>like<span class="w"> </span>it<span class="w"> </span>but<span class="w"> </span>this<span class="w"> </span>one<span class="w"> </span>is<span class="w"> </span>mine.
frontier0123
***********************
</pre></div>
</div>
</section>
</section>
<section id="using-globus-to-move-data-to-and-from-orion">
<h2>Using Globus to Move Data to and from Orion<a class="headerlink" href="#using-globus-to-move-data-to-and-from-orion" title="Link to this heading"></a></h2>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>After January 8, the Globus v4 collections will no longer be supported. Please use the OLCF Kronos and OLCF DTN (Globus 5) collections.</p>
</div>
<p>The following example is intended to help users move data to and from the Orion filesystem.</p>
<p>Below is a summary of the steps for data transfer using Globus:</p>
<blockquote>
<div><p>1. Login to <a class="reference external" href="https://www.globus.org" target="_blank">globus.org</a> using your globus ID and password. If you do not have a globusID, set one up here:
<a class="reference external" href="https://www.globusid.org/create?viewlocale=en_US" target="_blank">Generate a globusID</a>.</p>
<ol class="arabic simple">
<li><p>Once you are logged in, Globus will open the File Manager page. Click the left side Collection text field in the File Manager and         type OLCF DTN (Globus 5).</p></li>
<li><p>When prompted, authenticate into the OLCF DTN (Globus 5) collection using your OLCF username and PIN followed by your RSA passcode.</p></li>
<li><p>Click in the left side Path box in the File Manager and enter the path to your data on Orion. For example,`/lustre/orion/stf007/proj-       shared/my_orion_data`. You should see a list of your files and folders under the left Path Box.</p></li>
<li><p>Click on all files or folders that you want to transfer in the list. This will highlight them.</p></li>
<li><p>Click on the right side Collection box in the File Manager and type the name of a second collection at OLCF or at another institution.        You can transfer data between different paths on the Orion filesystem with this method too; Just use the OLCF DTN (Globus 5) collection again      in the right side Collection box.</p></li>
<li><p>Click in the right side Path box and enter the path where you want to put your data on the second collections filesystem.</p></li>
<li><p>Click the left Start button.</p></li>
<li><p>Click on Activity in the left blue menu bar to monitor your transfer. Globus will send you an email when the transfer is complete.</p></li>
</ol>
</div></blockquote>
<p><strong>Globus Warnings:</strong></p>
<ul class="simple">
<li><p>Globus transfers do not preserve file permissions. Arriving files will have (rw-rr) permissions, meaning arriving files will have <em>user</em> read and write permissions and <em>group</em> and <em>world</em> read permissions. Note that the arriving files will not have any execute permissions, so you will need to use chmod to reset execute permissions before running a Globus-transferred executable.</p></li>
<li><p>Globus will overwrite files at the destination with identically named source files. This is done without warning.</p></li>
<li><p>Globus has restriction of 8 active transfers across all the users. Each user has a limit of 3 active transfers, so it is required to transfer a lot of data on each transfer than less data across many transfers.</p></li>
<li><p>If a folder is constituted with mixed files including thousands of small files (less than 1MB each one), it would be better to tar the smallfiles.  Otherwise, if the files are larger, Globus will handle them.</p></li>
</ul>
</section>
<section id="amd-gpus">
<span id="id3"></span><h2>AMD GPUs<a class="headerlink" href="#amd-gpus" title="Link to this heading"></a></h2>
<p>The AMD Instinct MI200 is built on advanced packaging technologies
enabling two Graphic Compute Dies (GCDs) to be integrated
into a single package in the Open Compute Project (OCP) Accelerator Module (OAM)
in the MI250 and MI250X products.
Each GCD is build on the AMD CDNA 2 architecture.
A single Frontier node contains 4 MI250X OAMs for the total of 8 GCDs.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The Slurm workload manager and the ROCr runtime treat each GCD as a separate GPU
and visibility can be controlled using the <code class="docutils literal notranslate"><span class="pre">ROCR_VISIBLE_DEVICES</span></code> environment variable.
Therefore, from this point on, the Frontier guide simply refers to a GCD as a GPU.</p>
</div>
<p>Each GPU contains 110 Compute Units (CUs) grouped in 4 Compute Engines (CEs).
Physically, each GPU contains 112 CUs, but two are disabled.
A command processor in each GPU receives API commands and transforms them into compute tasks.
Compute tasks are managed by the 4 compute engines, which dispatch wavefronts to compute units.
All wavefronts from a single workgroup are assigned to the same CU.
In CUDA terminology, workgroups are blocks, wavefronts are warps, and work-items are threads.
The terms are often used interchangeably.</p>
<img alt="Block diagram of the AMD Instinct MI200 multi-chip module" class="align-center" src="../_images/amd_instinct_mi250x_oam.png" />
<p>The 110 CUs in each GPU deliver peak performance of 23.9 TFLOPS in double precision, or 47.9 TFLOPS if using the specialized Matrix cores.
Also, each GPU contains 64 GB of high-bandwidth memory (HBM2) accessible at a peak
bandwidth of 1.6 TB/s.
The 2 GPUs in an MI250X are connected with [4x] GPU-to-GPU Infinity Fabric links
providing 200+200 GB/s of bandwidth.
(Consult the diagram in the <a class="reference internal" href="#frontier-nodes"><span class="std std-ref">Frontier Compute Nodes</span></a> section for information
on how the accelerators are connected to each other, to the CPU, and to the network.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The X+X GB/s notation describes bidirectional bandwidth, meaning X GB/s in each direction.</p>
</div>
<section id="amd-vs-nvidia-terminology">
<span id="amd-nvidia-terminology"></span><h3>AMD vs NVIDIA Terminology<a class="headerlink" href="#amd-vs-nvidia-terminology" title="Link to this heading"></a></h3>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>AMD</p></th>
<th class="head"><p>NVIDIA</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Work-items or Threads</p></td>
<td><p>Threads</p></td>
</tr>
<tr class="row-odd"><td><p>Workgroup</p></td>
<td><p>Block</p></td>
</tr>
<tr class="row-even"><td><p>Wavefront</p></td>
<td><p>Warp</p></td>
</tr>
<tr class="row-odd"><td><p>Grid</p></td>
<td><p>Grid</p></td>
</tr>
</tbody>
</table>
<p>We will be using these terms interchangeably as they refer to the same concepts in GPU
programming, with the exception that we will only be using wavefront (which refers to a
unit of 64 threads) instead of warp (which refers to a unit of 32 threads) as they mean
different things.</p>
</section>
<section id="blocks-workgroups-threads-work-items-grids-wavefronts">
<h3>Blocks (workgroups), Threads (work items), Grids, Wavefronts<a class="headerlink" href="#blocks-workgroups-threads-work-items-grids-wavefronts" title="Link to this heading"></a></h3>
<p>When kernels are launched on a GPU, a grid of thread blocks are created, where the
number of thread blocks in the grid and the number of threads within each block are
defined by the programmer. The number of blocks in the grid (grid size) and the number of
threads within each block (block size) can be specified in one, two, or three dimensions
during the kernel launch. Each thread can be identified with a unique id within the
kernel, indexed along the X, Y, and Z dimensions.</p>
<ul class="simple">
<li><p>Number of blocks that can be specified along each dimension in a grid: (2147483647, 65536, 65536)</p></li>
<li><p>Max number of threads that can be specified along each dimension in a block: (1024, 1024, 1024)</p>
<ul>
<li><p>However, the total of number of threads in a block has an upper limit of 1024
[i.e. (size of x dimension * size of y dimension * size of z dimension) cannot exceed
1024].</p></li>
<li><p>And the total number of threads in a kernel launch has an upper limit of 2147483647.</p></li>
</ul>
</li>
</ul>
<p>Each block (or workgroup) of threads is assigned to a single Compute Unit i.e. a single
block wont be split across multiple CUs. The threads in a block are scheduled in units of
64 threads called wavefronts (similar to warps in CUDA, but warps only have 32 threads
instead of 64). When launching a kernel, up to 64KB of block level shared memory called
the Local Data Store (LDS) can be statically or dynamically allocated. This shared memory
between the threads in a block allows the threads to access block local data with much
lower latency compared to using the HBM since the data is in the compute unit itself.</p>
</section>
<section id="the-compute-unit">
<h3>The Compute Unit<a class="headerlink" href="#the-compute-unit" title="Link to this heading"></a></h3>
<img alt="Block diagram of the AMD Instinct CDNA2 Compute Unit" class="align-center" src="../_images/amd_computeunit.png" />
<p>Each CU has 4 Matrix Core Units (the equivalent of NVIDIAs Tensor core units) and 4
16-wide SIMD units. For a vector instruction that uses the SIMD units, each wavefront
(which has 64 threads) is assigned to a single 16-wide SIMD unit such that the wavefront
as a whole executes the instruction over 4 cycles, 16 threads per cycle. Since other
wavefronts occupy the other three SIMD units at the same time, the total throughput still
remains 1 instruction per cycle. Each CU maintains an instructions buffer for 8
wavefronts and also maintains 256 registers where each register is 64 4-byte wide
entries.</p>
</section>
<section id="hip">
<span id="amd-hip"></span><h3>HIP<a class="headerlink" href="#hip" title="Link to this heading"></a></h3>
<p>The Heterogeneous Interface for Portability (HIP) is AMDs dedicated GPU programming
environment for designing high performance kernels on GPU hardware. HIP is a C++ runtime
API and programming language that allows developers to create portable applications on
different platforms, including the AMD MI250X. This means that developers can write their GPU applications and with
very minimal changes be able to run their code in any environment.  The API is very
similar to CUDA, so if youre already familiar with CUDA there is almost no additional
work to learn HIP. See <a class="reference external" href="https://www.olcf.ornl.gov/preparing-for-frontier/" target="_blank">here</a> for a series
of tutorials on programming with HIP and also converting existing CUDA code to HIP with the <a class="reference external" href="https://github.com/ROCm-Developer-Tools/HIPIFY" target="_blank">hipify tools</a> .</p>
</section>
<section id="things-to-remember-when-programming-for-amd-gpus">
<h3>Things To Remember When Programming for AMD GPUs<a class="headerlink" href="#things-to-remember-when-programming-for-amd-gpus" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p>The MI250X has different denormal handling for FP16 and BF16 datatypes, which is relevant for ML training. Prefer using the BF16 over the FP16 datatype for ML models as you are more likely to encounter denormal values with FP16 (which get flushed to zero, causing failure in convergence for some ML models). See more in <a class="reference internal" href="#using-reduced-precision"><span class="std std-ref">Using reduced precision (FP16 and BF16 datatypes)</span></a>.</p></li>
<li><p>Memory can be automatically migrated to GPU from CPU on a page fault if XNACK operating mode is set.  No need to explicitly migrate data or provide managed memory. This is useful if youre migrating code from a programming model that relied on unified or managed memory. See more in <a class="reference internal" href="#enabling-gpu-page-migration"><span class="std std-ref">Enabling GPU Page Migration</span></a>. Information about how memory is accessed based on the allocator used and the XNACK mode can be found in <a class="reference internal" href="#migration-of-memory-allocator-xnack"><span class="std std-ref">Migration of Memory by Allocator and XNACK Mode</span></a>.</p></li>
<li><p>HIP has two kinds of memory allocations, coarse grained and fine grained, with tradeoffs between performance and coherence. Particularly relevant if you want to ues the hardware FP atomic instructions. See more in <a class="reference internal" href="#fp-atomic-ops-coarse-fine-allocations"><span class="std std-ref">Floating-Point (FP) Atomic Operations and Coarse/Fine Grained Memory Allocations</span></a>.</p></li>
<li><p>FP32 atomicAdd operations on Local Data Store (i.e. block shared memory) can be slower than the equivalent FP64 operations. See more in <a class="reference internal" href="#performance-lds-atomicadd"><span class="std std-ref">Performance considerations for LDS FP atomicAdd()</span></a>.</p></li>
</ul>
<p>See the <a class="reference internal" href="#frontier-compilers"><span class="std std-ref">Compiling</span></a> section for information on compiling for AMD GPUs, and
see the <a class="reference internal" href="#tips-and-tricks"><span class="std std-ref">Tips and Tricks</span></a> section for some detailed information to keep in mind
to run more efficiently on AMD GPUs.</p>
</section>
</section>
<section id="programming-environment">
<h2>Programming Environment<a class="headerlink" href="#programming-environment" title="Link to this heading"></a></h2>
<p>Frontier users are provided with many pre-installed software packages and scientific libraries. To facilitate this, environment management tools are used to handle necessary changes to the shell.</p>
<section id="environment-modules-lmod">
<h3>Environment Modules (Lmod)<a class="headerlink" href="#environment-modules-lmod" title="Link to this heading"></a></h3>
<p>Environment modules are provided through <a class="reference external" href="https://lmod.readthedocs.io/en/latest/" target="_blank">Lmod</a>, a Lua-based module system for dynamically altering shell environments. By managing changes to the shells environment variables (such as <code class="docutils literal notranslate"><span class="pre">PATH</span></code>, <code class="docutils literal notranslate"><span class="pre">LD_LIBRARY_PATH</span></code>, and <code class="docutils literal notranslate"><span class="pre">PKG_CONFIG_PATH</span></code>), Lmod allows you to alter the software available in your shell environment without the risk of creating package and version combinations that cannot coexist in a single environment.</p>
<section id="general-usage">
<h4>General Usage<a class="headerlink" href="#general-usage" title="Link to this heading"></a></h4>
<p>The interface to Lmod is provided by the <code class="docutils literal notranslate"><span class="pre">module</span></code> command:</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Command</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">module</span> <span class="pre">-t</span> <span class="pre">list</span></code></p></td>
<td><p>Shows a terse list of the currently loaded modules</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">module</span> <span class="pre">avail</span></code></p></td>
<td><p>Shows a table of the currently available modules</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">module</span> <span class="pre">help</span> <span class="pre">&lt;modulename&gt;</span></code></p></td>
<td><p>Shows help information about <code class="docutils literal notranslate"><span class="pre">&lt;modulename&gt;</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">module</span> <span class="pre">show</span> <span class="pre">&lt;modulename&gt;</span></code></p></td>
<td><p>Shows the environment changes made by the <code class="docutils literal notranslate"><span class="pre">&lt;modulename&gt;</span></code> modulefile</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">module</span> <span class="pre">spider</span> <span class="pre">&lt;string&gt;</span></code></p></td>
<td><p>Searches all possible modules according to <code class="docutils literal notranslate"><span class="pre">&lt;string&gt;</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">module</span> <span class="pre">load</span> <span class="pre">&lt;modulename&gt;</span> <span class="pre">[...]</span></code></p></td>
<td><p>Loads the given <code class="docutils literal notranslate"><span class="pre">&lt;modulename&gt;</span></code>(s) into the current environment</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">module</span> <span class="pre">use</span> <span class="pre">&lt;path&gt;</span></code></p></td>
<td><p>Adds <code class="docutils literal notranslate"><span class="pre">&lt;path&gt;</span></code> to the modulefile search cache and <code class="docutils literal notranslate"><span class="pre">MODULESPATH</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">module</span> <span class="pre">unuse</span> <span class="pre">&lt;path&gt;</span></code></p></td>
<td><p>Removes <code class="docutils literal notranslate"><span class="pre">&lt;path&gt;</span></code> from the modulefile search cache and <code class="docutils literal notranslate"><span class="pre">MODULESPATH</span></code></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">module</span> <span class="pre">purge</span></code></p></td>
<td><p>Unloads all modules</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">module</span> <span class="pre">reset</span></code></p></td>
<td><p>Resets loaded modules to system defaults</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">module</span> <span class="pre">update</span></code></p></td>
<td><p>Reloads all currently loaded modules</p></td>
</tr>
</tbody>
</table>
</section>
<section id="searching-for-modules">
<h4>Searching for Modules<a class="headerlink" href="#searching-for-modules" title="Link to this heading"></a></h4>
<p>Modules with dependencies are only available when the underlying dependencies, such as compiler families, are loaded. Thus, module avail will only display modules that are compatible with the current state of the environment. To search the entire hierarchy across all possible dependencies, the <code class="docutils literal notranslate"><span class="pre">spider</span></code> sub-command can be used as summarized in the following table.</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Command</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">module</span> <span class="pre">spider</span></code></p></td>
<td><p>Shows the entire possible graph of modules</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">module</span> <span class="pre">spider</span> <span class="pre">&lt;modulename&gt;</span></code></p></td>
<td><p>Searches for modules named <code class="docutils literal notranslate"><span class="pre">&lt;modulename&gt;</span></code> in the graph of possible modules</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">module</span> <span class="pre">spider</span> <span class="pre">&lt;modulename&gt;/&lt;version&gt;</span></code></p></td>
<td><p>Searches for a specific version of <code class="docutils literal notranslate"><span class="pre">&lt;modulename&gt;</span></code> in the graph of possible modules</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">module</span> <span class="pre">spider</span> <span class="pre">&lt;string&gt;</span></code></p></td>
<td><p>Searches for modulefiles containing <code class="docutils literal notranslate"><span class="pre">&lt;string&gt;</span></code></p></td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Due to the implementation of the module heirarchy on Frontier, <code class="docutils literal notranslate"><span class="pre">module</span> <span class="pre">spider</span></code> does not currently locate OLCF-provided Spack-built modulefiles in <code class="docutils literal notranslate"><span class="pre">/sw/frontier</span></code>.</p>
</div>
</section>
</section>
<section id="compilers">
<h3>Compilers<a class="headerlink" href="#compilers" title="Link to this heading"></a></h3>
<p>Cray, AMD, and GCC compilers are provided through modules on Frontier. The Cray and AMD compilers are both based on LLVM/Clang. There is also a system/OS versions of GCC available in <code class="docutils literal notranslate"><span class="pre">/usr/bin</span></code>. The table below lists details about each of the module-provided compilers. Please see the following <a class="reference internal" href="#frontier-compilers"><span class="std std-ref">Compiling</span></a> section for more detailed inforation on how to compile using these modules.</p>
<section id="cray-programming-environment-and-compiler-wrappers">
<h4>Cray Programming Environment and Compiler Wrappers<a class="headerlink" href="#cray-programming-environment-and-compiler-wrappers" title="Link to this heading"></a></h4>
<p>Cray provides <code class="docutils literal notranslate"><span class="pre">PrgEnv-&lt;compiler&gt;</span></code> modules (e.g., <code class="docutils literal notranslate"><span class="pre">PrgEnv-cray</span></code>) that load compatible components of a specific compiler toolchain. The components include the specified compiler as well as MPI, LibSci, and other libraries. Loading the <code class="docutils literal notranslate"><span class="pre">PrgEnv-&lt;compiler&gt;</span></code> modules also defines a set of compiler wrappers for that compiler toolchain that automatically add include paths and link in libraries for Cray software. Compiler wrappers are provided for C (<code class="docutils literal notranslate"><span class="pre">cc</span></code>), C++ (<code class="docutils literal notranslate"><span class="pre">CC</span></code>), and Fortran (<code class="docutils literal notranslate"><span class="pre">ftn</span></code>).</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Use the <code class="docutils literal notranslate"><span class="pre">-craype-verbose</span></code> flag to display the full include and link information used by the Cray compiler wrappers. This must be called on a file to see the full output (e.g., <code class="docutils literal notranslate"><span class="pre">CC</span> <span class="pre">-craype-verbose</span> <span class="pre">test.cpp</span></code>).</p>
</div>
</section>
</section>
<section id="mpi">
<h3>MPI<a class="headerlink" href="#mpi" title="Link to this heading"></a></h3>
<p>The MPI implementation available on Frontier is Crays MPICH, which is GPU-aware so GPU buffers can be passed directly to MPI calls.</p>
<hr class="docutils" />
</section>
</section>
<section id="compiling">
<span id="frontier-compilers"></span><h2>Compiling<a class="headerlink" href="#compiling" title="Link to this heading"></a></h2>
<section id="id4">
<h3>Compilers<a class="headerlink" href="#id4" title="Link to this heading"></a></h3>
<p>Cray, AMD, and GCC compilers are provided through modules on Frontier. The Cray and AMD compilers are both based on LLVM/Clang. There is also a system/OS versions of GCC available in <code class="docutils literal notranslate"><span class="pre">/usr/bin</span></code>. The table below lists details about each of the module-provided compilers.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>It is highly recommended to use the Cray compiler wrappers (<code class="docutils literal notranslate"><span class="pre">cc</span></code>, <code class="docutils literal notranslate"><span class="pre">CC</span></code>, and <code class="docutils literal notranslate"><span class="pre">ftn</span></code>) whenever possible. See the next section for more details.</p>
</div>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Vendor</p></th>
<th class="head"><p>Programming Environment</p></th>
<th class="head"><p>Compiler Module</p></th>
<th class="head"><p>Language</p></th>
<th class="head"><p>Compiler Wrapper</p></th>
<th class="head"><p>Compiler</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td rowspan="3"><p>Cray</p></td>
<td rowspan="3"><p><code class="docutils literal notranslate"><span class="pre">PrgEnv-cray</span></code></p></td>
<td rowspan="3"><p><code class="docutils literal notranslate"><span class="pre">cce</span></code></p></td>
<td><p>C</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">cc</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">craycc</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>C++</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">CC</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">craycxx</span></code> or <code class="docutils literal notranslate"><span class="pre">crayCC</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Fortran</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">ftn</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">crayftn</span></code></p></td>
</tr>
<tr class="row-odd"><td rowspan="3"><p>AMD</p></td>
<td rowspan="3"><p><code class="docutils literal notranslate"><span class="pre">PrgEnv-amd</span></code></p></td>
<td rowspan="3"><p><code class="docutils literal notranslate"><span class="pre">amd</span></code></p></td>
<td><p>C</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">cc</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">amdclang</span></code></p></td>
</tr>
<tr class="row-even"><td><p>C++</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">CC</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">amdclang++</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Fortran</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">ftn</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">amdflang</span></code></p></td>
</tr>
<tr class="row-even"><td rowspan="3"><p>GCC</p></td>
<td rowspan="3"><p><code class="docutils literal notranslate"><span class="pre">PrgEnv-gnu</span></code></p></td>
<td rowspan="3"><p><code class="docutils literal notranslate"><span class="pre">gcc-native</span></code>
or
<code class="docutils literal notranslate"><span class="pre">gcc</span></code> (&lt;12.3)</p></td>
<td><p>C</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">cc</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">gcc</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>C++</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">CC</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">g++</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Fortran</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">ftn</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">gfortran</span></code></p></td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The <code class="docutils literal notranslate"><span class="pre">gcc-native</span></code> compiler module was introduced in the December 2023 release of the HPE/Cray Programming Environment (CrayPE) and replaces <code class="docutils literal notranslate"><span class="pre">gcc</span></code>.
<code class="docutils literal notranslate"><span class="pre">gcc</span></code> provides GCC installations that were packaged within CrayPE, while <code class="docutils literal notranslate"><span class="pre">gcc-native</span></code> provides GCC installations outside of CrayPE.</p>
</div>
<section id="id5">
<h4>Cray Programming Environment and Compiler Wrappers<a class="headerlink" href="#id5" title="Link to this heading"></a></h4>
<p>Cray provides <code class="docutils literal notranslate"><span class="pre">PrgEnv-&lt;compiler&gt;</span></code> modules (e.g., <code class="docutils literal notranslate"><span class="pre">PrgEnv-cray</span></code>) that load compatible components of a specific compiler toolchain. The components include the specified compiler as well as MPI, LibSci, and other libraries. Loading the <code class="docutils literal notranslate"><span class="pre">PrgEnv-&lt;compiler&gt;</span></code> modules also defines a set of compiler wrappers for that compiler toolchain that automatically add include paths and link in libraries for Cray software. Compiler wrappers are provided for C (<code class="docutils literal notranslate"><span class="pre">cc</span></code>), C++ (<code class="docutils literal notranslate"><span class="pre">CC</span></code>), and Fortran (<code class="docutils literal notranslate"><span class="pre">ftn</span></code>).</p>
<p>For example, to load the AMD programming environment, do:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>module<span class="w"> </span>load<span class="w"> </span>PrgEnv-amd
</pre></div>
</div>
<p>This module will setup your programming environment with paths to software and libraries that are compatible with AMD host compilers.</p>
<p>When loading non-default versions of Cray-provided components, please see <a class="reference internal" href="#understanding-the-compatibility-of-compilers-rocm-and-cray-mpich"><span class="std std-ref">Understanding the Compatibility of Compilers, ROCm, and Cray MPICH</span></a> for information about loading a set of compatible Cray modules.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Use the <code class="docutils literal notranslate"><span class="pre">-craype-verbose</span></code> flag to display the full include and link information used by the Cray compiler wrappers. This must be called on a file to see the full output (e.g., <code class="docutils literal notranslate"><span class="pre">CC</span> <span class="pre">-craype-verbose</span> <span class="pre">test.cpp</span></code>).</p>
</div>
</section>
<section id="exposing-the-rocm-toolchain-to-your-programming-environment">
<span id="id6"></span><h4>Exposing The ROCm Toolchain to yourProgrammingEnvironment<a class="headerlink" href="#exposing-the-rocm-toolchain-to-your-programming-environment" title="Link to this heading"></a></h4>
<p>If you need to add thetoolsand libraries related to ROCm, the framework for targeting AMD GPUs, to your path, you will need to use a version of ROCm that is compatible with your programming environment.
ROCm can be loaded with: <code class="docutils literal notranslate"><span class="pre">module</span> <span class="pre">load</span> <span class="pre">rocm/X.Y.Z</span></code>, or to load the default ROCm version, <code class="docutils literal notranslate"><span class="pre">module</span> <span class="pre">load</span> <span class="pre">rocm</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Both the CCE and ROCm compilers are Clang-based, so please be sure to use consistent (major) Clang versions when using them together. You can check which version of Clang is being used with CCE and ROCm by giving the <code class="docutils literal notranslate"><span class="pre">--version</span></code> flag to <code class="docutils literal notranslate"><span class="pre">CC</span></code> and <code class="docutils literal notranslate"><span class="pre">amdclang</span></code>, respectively.
Please see <a class="reference internal" href="#understanding-the-compatibility-of-compilers-rocm-and-cray-mpich"><span class="std std-ref">Understanding the Compatibility of Compilers, ROCm, and Cray MPICH</span></a> for information about loading a compatible set of modules.</p>
</div>
</section>
</section>
<section id="id7">
<h3>MPI<a class="headerlink" href="#id7" title="Link to this heading"></a></h3>
<p>The MPI implementation available on Frontier is Crays MPICH, which is GPU-aware so GPU buffers can be passed directly to MPI calls.</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Implementation</p></th>
<th class="head"><p>Module</p></th>
<th class="head"><p>Compiler</p></th>
<th class="head"><p>Header Files &amp; Linking</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td rowspan="2"><p>Cray MPICH</p></td>
<td rowspan="2"><p><code class="docutils literal notranslate"><span class="pre">cray-mpich</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">cc</span></code>, <code class="docutils literal notranslate"><span class="pre">CC</span></code>, <code class="docutils literal notranslate"><span class="pre">ftn</span></code> (Cray compiler wrappers)</p></td>
<td><p>MPI header files and linking is built into the Cray compiler wrappers</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">hipcc</span></code></p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">-L${MPICH_DIR}/lib</span> <span class="pre">-lmpi</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">${CRAY_XPMEM_POST_LINK_OPTS}</span> <span class="pre">-lxpmem</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">${PE_MPICH_GTL_DIR_amd_gfx90a}</span> <span class="pre">${PE_MPICH_GTL_LIBS_amd_gfx90a}</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">-I${MPICH_DIR}/include</span></code></div>
</div>
</td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>hipcc requires the ROCm Toolclain, See <a class="reference internal" href="#exposing-the-rocm-toolchain-to-your-programming-environment"><span class="std std-ref">Exposing The ROCm Toolchain to yourProgrammingEnvironment</span></a></p>
</div>
<section id="gpu-aware-mpi">
<h4>GPU-Aware MPI<a class="headerlink" href="#gpu-aware-mpi" title="Link to this heading"></a></h4>
<p>To use GPU-aware Cray MPICH with Frontiers PrgEnv modules, users must set the following modules and environment variables:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>module<span class="w"> </span>load<span class="w"> </span>craype-accel-amd-gfx90a
module<span class="w"> </span>load<span class="w"> </span>rocm

<span class="nb">export</span><span class="w"> </span><span class="nv">MPICH_GPU_SUPPORT_ENABLED</span><span class="o">=</span><span class="m">1</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>There are extra steps needed to enable GPU-aware MPI on Frontier, which depend on the compiler that is used (see 1. and 2. below).</p>
</div>
<section id="compiling-with-the-cray-compiler-wrappers-cc-or-cc">
<h5>1. Compiling with the Cray compiler wrappers, <code class="docutils literal notranslate"><span class="pre">cc</span></code> or <code class="docutils literal notranslate"><span class="pre">CC</span></code><a class="headerlink" href="#compiling-with-the-cray-compiler-wrappers-cc-or-cc" title="Link to this heading"></a></h5>
<p>When using GPU-aware Cray MPICH with the Cray compiler wrappers, most of the needed libraries are automatically linked through the environment variables.</p>
<p>Though, the following header files and libraries must be included explicitly:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>-I<span class="si">${</span><span class="nv">ROCM_PATH</span><span class="si">}</span>/include
-L<span class="si">${</span><span class="nv">ROCM_PATH</span><span class="si">}</span>/lib<span class="w"> </span>-lamdhip64
</pre></div>
</div>
<p>where the include path implies that <code class="docutils literal notranslate"><span class="pre">#include</span> <span class="pre">&lt;hip/hip_runtime.h&gt;</span></code> is included in the source file.</p>
</section>
<section id="compiling-without-the-cray-compiler-wrappers-e-g-hipcc">
<h5>2. Compiling without the Cray compiler wrappers, e.g. <code class="docutils literal notranslate"><span class="pre">hipcc</span></code><a class="headerlink" href="#compiling-without-the-cray-compiler-wrappers-e-g-hipcc" title="Link to this heading"></a></h5>
<p>To use <code class="docutils literal notranslate"><span class="pre">hipcc</span></code> with GPU-aware Cray MPICH, the following is needed to setup the needed header files and libraries.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>-I<span class="si">${</span><span class="nv">MPICH_DIR</span><span class="si">}</span>/include
-L<span class="si">${</span><span class="nv">MPICH_DIR</span><span class="si">}</span>/lib<span class="w"> </span>-lmpi<span class="w"> </span><span class="se">\</span>
<span class="w">  </span><span class="si">${</span><span class="nv">CRAY_XPMEM_POST_LINK_OPTS</span><span class="si">}</span><span class="w"> </span>-lxpmem<span class="w"> </span><span class="se">\</span>
<span class="w">  </span><span class="si">${</span><span class="nv">PE_MPICH_GTL_DIR_amd_gfx90a</span><span class="si">}</span><span class="w"> </span><span class="si">${</span><span class="nv">PE_MPICH_GTL_LIBS_amd_gfx90a</span><span class="si">}</span>

<span class="nv">HIPFLAGS</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>--offload-arch<span class="o">=</span>gfx90a
</pre></div>
</div>
</section>
</section>
<section id="understanding-the-compatibility-of-compilers-rocm-and-cray-mpich">
<span id="id8"></span><h4>Understanding the Compatibility of Compilers, ROCm, and Cray MPICH<a class="headerlink" href="#understanding-the-compatibility-of-compilers-rocm-and-cray-mpich" title="Link to this heading"></a></h4>
<p>There are three primary sources of compatibility required to successfully build and run on Frontier:</p>
<blockquote>
<div><ol class="arabic simple">
<li><p>Compatible Compiler &amp; ROCm toolchain versions</p></li>
<li><p>Compatible ROCm &amp; Cray MPICH versions</p></li>
<li><p>Compatibility with other CrayPE-provided software</p></li>
</ol>
</div></blockquote>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If using non-default versions of any <code class="docutils literal notranslate"><span class="pre">cray-*</span></code> module, you must <em>prepend</em> <code class="docutils literal notranslate"><span class="pre">${CRAY_LD_LIBRARY_PATH}</span></code> (or the path to <code class="docutils literal notranslate"><span class="pre">lib64</span></code> for your specific <code class="docutils literal notranslate"><span class="pre">cray-*</span></code> component) to your <code class="docutils literal notranslate"><span class="pre">LD_LIBRARY_PATH</span></code> at run time or your executables rpath at build time.</p>
</div>
<section id="compatible-compiler-rocm-toolchain-versions">
<h5>Compatible Compiler &amp; ROCm toolchain versions<a class="headerlink" href="#compatible-compiler-rocm-toolchain-versions" title="Link to this heading"></a></h5>
<p>All compilers in the same HPE/Cray Programming Environment (CrayPE) release are generally ABI-compatible (e.g. code generated by CCE can be linked against code compiled by GCC).
However, the AMD and CCE compilers are both LLVM/Clang-based, and it is recommended to use the same major LLVM version when cross-compiling.
CCEs module version indicates the base LLVM version, but for AMD, you must run <code class="docutils literal notranslate"><span class="pre">amdclang</span> <span class="pre">--version</span></code>.
For example, ROCm/5.3.0 is based on LLVM 15.0.0.
It is strongly discouraged to use ROCm/5.3.0 with CCE/16.0.1, which is based on LLVM 16.
The following table shows the recommended ROCm version for each CCE version, along with the CPE version:</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>CCE</p></th>
<th class="head"><p>CPE</p></th>
<th class="head"><p>Recommended ROCm Version</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>15.0.0</p></td>
<td><p>22.12</p></td>
<td><p>5.3.0</p></td>
</tr>
<tr class="row-odd"><td><p>15.0.1</p></td>
<td><p>23.03</p></td>
<td><p>5.3.0</p></td>
</tr>
<tr class="row-even"><td><p>16.0.0</p></td>
<td><p>23.05</p></td>
<td><p>5.5.1</p></td>
</tr>
<tr class="row-odd"><td><p>16.0.1</p></td>
<td><p>23.09</p></td>
<td><p>5.5.1</p></td>
</tr>
<tr class="row-even"><td><p>17.0.0</p></td>
<td><p>23.12</p></td>
<td><p>5.7.0 or 5.7.1</p></td>
</tr>
<tr class="row-odd"><td><p>17.0.1</p></td>
<td><p>24.03</p></td>
<td><p>6.0.0</p></td>
</tr>
<tr class="row-even"><td><p>18.0.0</p></td>
<td><p>24.07</p></td>
<td><p>6.1.3</p></td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Recall that the CPE module is a meta-module that simple loads the correct version for each Cray-provided module (e.g. CCE, Cray MPICH, Cray Libsci).
This is the best way to load the versions of modules from a specific CrayPE release.</p>
</div>
</section>
<section id="compatible-rocm-cray-mpich-versions">
<h5>Compatible ROCm &amp; Cray MPICH versions<a class="headerlink" href="#compatible-rocm-cray-mpich-versions" title="Link to this heading"></a></h5>
<p>Compatibility between Cray MPICH and ROCm is required in order to use GPU-aware MPI.
Releases of <code class="docutils literal notranslate"><span class="pre">cray-mpich</span></code> are each compiled using a specific version of ROCm, and compatibility across multiple versions is not guaranteed.
OLCF will maintain compatible default modules when possible.
If using non-default modules, you can determine compatibility by reviewing the <em>Product and OS Dependencies</em> section in the <code class="docutils literal notranslate"><span class="pre">cray-mpich</span></code> release notes.
This can be displayed by running <code class="docutils literal notranslate"><span class="pre">module</span> <span class="pre">show</span> <span class="pre">cray-mpich/&lt;version&gt;</span></code>. If the notes indicate compatibility with <em>AMD ROCM X.Y or later</em>, only use <code class="docutils literal notranslate"><span class="pre">rocm/X.Y.Z</span></code> modules.
If you are loading compatible ROCm and Cray MPICH versions but still getting errors, try setting <code class="docutils literal notranslate"><span class="pre">MPICH_VERSION_DISPLAY=1</span></code> to verify the correct Cray MPICH version is being used at run-time.
If it is not, verify you are prepending <code class="docutils literal notranslate"><span class="pre">LD_LIBRARY_PATH</span></code> with <code class="docutils literal notranslate"><span class="pre">CRAY_LD_LIBRARY_PATH</span></code> or <code class="docutils literal notranslate"><span class="pre">${MPICH_DIR}/lib</span></code>.
The following compatibility table below was determined by testing of the linker and basic GPU-aware MPI functions with all current combinations of <code class="docutils literal notranslate"><span class="pre">cray-mpich</span></code> and ROCm modules on Frontier.
Alongside <code class="docutils literal notranslate"><span class="pre">cray-mpich</span></code>, we load the corresponding <code class="docutils literal notranslate"><span class="pre">cpe</span></code> module, which loads other important modules for MPI such as <code class="docutils literal notranslate"><span class="pre">cray-pmi</span></code> and <code class="docutils literal notranslate"><span class="pre">craype</span></code>.
It is strongly encouraged to load a <code class="docutils literal notranslate"><span class="pre">cpe</span></code> module when using non-default modules.
This ensures that all CrayPE-provided modules are compatible.
An asterisk indicates the latest officially supported version of ROCm for each <code class="docutils literal notranslate"><span class="pre">cray-mpich</span></code> version.</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>cray-mpich</p></th>
<th class="head"><p>cpe</p></th>
<th class="head"><p>ROCm</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>8.1.23</p></td>
<td><p>22.12</p></td>
<td><p>5.4.3, 5.4.0, 5.3.0*</p></td>
</tr>
<tr class="row-odd"><td><p>8.1.25</p></td>
<td><p>23.03</p></td>
<td><p>5.4.3, 5.4.0*, 5.3.0</p></td>
</tr>
<tr class="row-even"><td><p>8.1.26</p></td>
<td><p>23.05</p></td>
<td><p>5.7.1, 5.7.0, 5.6.0, 5.5.1*, 5.4.3, 5.4.0, 5.3.0</p></td>
</tr>
<tr class="row-odd"><td><p>8.1.27</p></td>
<td><p>23.09</p></td>
<td><p>5.7.1, 5.7.0, 5.6.0, 5.5.1*, 5.4.3, 5.4.0, 5.3.0</p></td>
</tr>
<tr class="row-even"><td><p>8.1.28</p></td>
<td><p>23.12</p></td>
<td><p>5.7.1, 5.7.0*, 5.6.0, 5.5.1, 5.4.3, 5.4.0, 5.3.0</p></td>
</tr>
<tr class="row-odd"><td><p>8.1.29</p></td>
<td><p>24.03</p></td>
<td><p>6.1.3, 6.0.0*</p></td>
</tr>
<tr class="row-even"><td><p>8.1.30</p></td>
<td><p>24.07</p></td>
<td><p>6.1.3*, 6.0.0</p></td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>OLCF recommends using the officially supported ROCm version (with asterisk) for each <code class="docutils literal notranslate"><span class="pre">cray-mpich</span></code> version.
Newer versions were tested using a sample of MPI operations and there may be undiscovered incompatibility.</p>
</div>
</section>
<section id="compatibility-with-other-craype-provided-software">
<h5>Compatibility with other CrayPE-provided Software<a class="headerlink" href="#compatibility-with-other-craype-provided-software" title="Link to this heading"></a></h5>
<p>The HPE/Cray Programming Environment (CrayPE) provides many libraries for use on Frontier, including the well-known libraries like Cray MPICH, Cray Libsci, and Cray FFTW.
CrayPE also has many modules that operate in the background and can easily be overlooked.
For example, the <code class="docutils literal notranslate"><span class="pre">craype</span></code> module provides the <code class="docutils literal notranslate"><span class="pre">cc</span></code>, <code class="docutils literal notranslate"><span class="pre">CC</span></code>, and <code class="docutils literal notranslate"><span class="pre">ftn</span></code> Cray compiler drivers.
These drivers are written to link to specific libraries (e.g. the <code class="docutils literal notranslate"><span class="pre">ftn</span></code> wrapper in September 2023 PE links to <code class="docutils literal notranslate"><span class="pre">libtcmalloc_minimal.so</span></code>),
which may not be needed by compiler versions other than the one they were released with.</p>
<p>For the full compatibility of your loaded CrayPE environment, we strongly recommended loading the <code class="docutils literal notranslate"><span class="pre">cpe</span></code> module of your desired CrayPE release (version is the last two digits of the year and the two-digit month, e.g. September 2023 is version 23.09).
For example, to load the September 2023 PE (CCE 16.0.1, Cray MPICH 8.1.27, ROCm 5.5.1 compatibility),
you would run the following commands:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>module<span class="w"> </span>load<span class="w"> </span>PrgEnv-cray
<span class="c1"># Load the cpe module after your desired PE, but before rocm -- sometimes cpe attempts to set a rocm version</span>
module<span class="w"> </span>load<span class="w"> </span>cpe/23.09
module<span class="w"> </span>load<span class="w"> </span>rocm/5.5.1

<span class="c1"># Since these modules are not default, make sure to prepend CRAY_LD_LIBRARY_PATH to LD_LIBRARY_PATH</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="si">${</span><span class="nv">CRAY_LD_LIBRARY_PATH</span><span class="si">}</span>:<span class="si">${</span><span class="nv">LD_LIBRARY_PATH</span><span class="si">}</span>
</pre></div>
</div>
</section>
</section>
</section>
<section id="openmp">
<h3>OpenMP<a class="headerlink" href="#openmp" title="Link to this heading"></a></h3>
<p>This section shows how to compile with OpenMP using the different compilers covered above.</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Vendor</p></th>
<th class="head"><p>Module</p></th>
<th class="head"><p>Language</p></th>
<th class="head"><p>Compiler</p></th>
<th class="head"><p>OpenMP flag (CPU thread)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td rowspan="2"><p>Cray</p></td>
<td rowspan="2"><p><code class="docutils literal notranslate"><span class="pre">cce</span></code></p></td>
<td><p>C, C++</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">cc</span></code> (wraps <code class="docutils literal notranslate"><span class="pre">craycc</span></code>)</div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">CC</span></code> (wraps <code class="docutils literal notranslate"><span class="pre">crayCC</span></code>)</div>
</div>
</td>
<td><p><code class="docutils literal notranslate"><span class="pre">-fopenmp</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Fortran</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">ftn</span></code> (wraps <code class="docutils literal notranslate"><span class="pre">crayftn</span></code>)</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">-homp</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">-fopenmp</span></code> (alias)</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>AMD</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">amd</span></code></p></td>
<td><div class="line-block">
<div class="line">C</div>
<div class="line">C++</div>
<div class="line">Fortran</div>
</div>
</td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">cc</span></code> (wraps <code class="docutils literal notranslate"><span class="pre">amdclang</span></code>)</div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">CC</span></code> (wraps <code class="docutils literal notranslate"><span class="pre">amdclang++</span></code>)</div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">ftn</span></code> (wraps <code class="docutils literal notranslate"><span class="pre">amdflang</span></code>)</div>
</div>
</td>
<td><p><code class="docutils literal notranslate"><span class="pre">-fopenmp</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>GCC</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">gcc</span></code></p></td>
<td><div class="line-block">
<div class="line">C</div>
<div class="line">C++</div>
<div class="line">Fortran</div>
</div>
</td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">cc</span></code> (wraps <code class="docutils literal notranslate"><span class="pre">$GCC_PATH/bin/gcc</span></code>)</div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">CC</span></code> (wraps <code class="docutils literal notranslate"><span class="pre">$GCC_PATH/bin/g++</span></code>)</div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">ftn</span></code> (wraps <code class="docutils literal notranslate"><span class="pre">$GCC_PATH/bin/gfortran</span></code>)</div>
</div>
</td>
<td><p><code class="docutils literal notranslate"><span class="pre">-fopenmp</span></code></p></td>
</tr>
</tbody>
</table>
</section>
<section id="openmp-gpu-offload">
<h3>OpenMP GPU Offload<a class="headerlink" href="#openmp-gpu-offload" title="Link to this heading"></a></h3>
<p>This section shows how to compile with OpenMP Offload using the different compilers covered above.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Make sure the <code class="docutils literal notranslate"><span class="pre">craype-accel-amd-gfx90a</span></code> module is loaded when using OpenMP offload.</p>
</div>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Vendor</p></th>
<th class="head"><p>Module</p></th>
<th class="head"><p>Language</p></th>
<th class="head"><p>Compiler</p></th>
<th class="head"><p>OpenMP flag (GPU)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td rowspan="2"><p>Cray</p></td>
<td rowspan="2"><p><code class="docutils literal notranslate"><span class="pre">cce</span></code></p></td>
<td><p>C
C++</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">cc</span></code> (wraps <code class="docutils literal notranslate"><span class="pre">craycc</span></code>)</div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">CC</span></code> (wraps <code class="docutils literal notranslate"><span class="pre">crayCC</span></code>)</div>
</div>
</td>
<td><p><code class="docutils literal notranslate"><span class="pre">-fopenmp</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Fortran</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">ftn</span></code> (wraps <code class="docutils literal notranslate"><span class="pre">crayftn</span></code>)</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">-homp</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">-fopenmp</span></code> (alias)</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>AMD</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">amd</span></code></p></td>
<td><div class="line-block">
<div class="line">C</div>
<div class="line">C++</div>
<div class="line">Fortran</div>
</div>
</td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">cc</span></code> (wraps <code class="docutils literal notranslate"><span class="pre">amdclang</span></code>)</div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">CC</span></code> (wraps <code class="docutils literal notranslate"><span class="pre">amdclang++</span></code>)</div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">ftn</span></code> (wraps <code class="docutils literal notranslate"><span class="pre">amdflang</span></code>)</div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">hipcc</span></code> (requires flags below)</div>
</div>
</td>
<td><p><code class="docutils literal notranslate"><span class="pre">-fopenmp</span></code></p></td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If invoking <code class="docutils literal notranslate"><span class="pre">amdclang</span></code>, <code class="docutils literal notranslate"><span class="pre">amdclang++</span></code>, or <code class="docutils literal notranslate"><span class="pre">amdflang</span></code> directly for <code class="docutils literal notranslate"><span class="pre">openmp</span> <span class="pre">offload</span></code>, or using <code class="docutils literal notranslate"><span class="pre">hipcc</span></code> you will need to add:</p>
<p><code class="docutils literal notranslate"><span class="pre">-fopenmp</span> <span class="pre">-fopenmp-targets=amdgcn-amd-amdhsa</span> <span class="pre">-Xopenmp-target=amdgcn-amd-amdhsa</span> <span class="pre">-march=gfx90a</span></code>.</p>
</div>
</section>
<section id="openacc">
<h3>OpenACC<a class="headerlink" href="#openacc" title="Link to this heading"></a></h3>
<p>This section shows how to compile code with OpenACC. Currently only the Cray compiler supports OpenACC for Fortran. The AMD and
GNU programming environments do not support OpenACC at all.
C and C++ support for OpenACC is provided by <a class="reference external" href="https://csmd.ornl.gov/project/clacc" target="_blank">clacc</a> which maintains a fork of the LLVM
compiler with added support for OpenACC. It can be obtained by loading the UMS modules
<code class="docutils literal notranslate"><span class="pre">ums</span></code>, <code class="docutils literal notranslate"><span class="pre">ums025</span></code>, and <code class="docutils literal notranslate"><span class="pre">clacc</span></code>.</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Vendor</p></th>
<th class="head"><p>Module</p></th>
<th class="head"><p>Language</p></th>
<th class="head"><p>Compiler</p></th>
<th class="head"><p>Flags</p></th>
<th class="head"><p>Support</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td rowspan="2"><p>Cray</p></td>
<td rowspan="2"><p><code class="docutils literal notranslate"><span class="pre">cce</span></code></p></td>
<td><p>C, C++</p></td>
<td><p>No support</p></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>Fortran</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">ftn</span></code> (wraps <code class="docutils literal notranslate"><span class="pre">crayftn</span></code>)</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">-h</span> <span class="pre">acc</span></code></div>
</div>
</td>
<td><p>Full support for OpenACC 2.0
Partial support for OpenACC 2.x/3.x</p></td>
</tr>
<tr class="row-even"><td rowspan="2"><p>UMS
module</p></td>
<td rowspan="2"><p><code class="docutils literal notranslate"><span class="pre">PrgEnv-cray</span></code>
<code class="docutils literal notranslate"><span class="pre">ums</span></code>
<code class="docutils literal notranslate"><span class="pre">um025</span></code>
<code class="docutils literal notranslate"><span class="pre">clacc</span></code></p></td>
<td><p>C, C++</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">clang</span></code></p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">-fopenacc</span></code></div>
</div>
</td>
<td><dl class="simple">
<dt>Experimental. Contact</dt><dd><p>Joel Denny <a class="reference external" href="mailto:dennyje&#37;&#52;&#48;ornl&#46;gov" target="_blank">dennyje<span>&#64;</span>ornl<span>&#46;</span>gov</a></p>
</dd>
</dl>
</td>
</tr>
<tr class="row-odd"><td><p>Fortran</p></td>
<td><p>No support</p></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</section>
<section id="id9">
<h3>HIP<a class="headerlink" href="#id9" title="Link to this heading"></a></h3>
<p>This section shows how to compile HIP codes using the Cray compiler wrappers and <code class="docutils literal notranslate"><span class="pre">hipcc</span></code> compiler driver.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Make sure the <code class="docutils literal notranslate"><span class="pre">craype-accel-amd-gfx90a</span></code> module is loaded when compiling HIP with the Cray compiler wrappers.</p>
</div>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Compiler</p></th>
<th class="head"><p>Compile/Link Flags, Header Files, and Libraries</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">CC</span></code></div>
<div class="line">Only with</div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">PrgEnv-cray</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">PrgEnv-amd</span></code></div>
</div>
</td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">CFLAGS</span> <span class="pre">=</span> <span class="pre">-std=c++11</span> <span class="pre">-D__HIP_ROCclr__</span> <span class="pre">-D__HIP_ARCH_GFX90A__=1</span> <span class="pre">--rocm-path=${ROCM_PATH}</span> <span class="pre">--offload-arch=gfx90a</span> <span class="pre">-x</span> <span class="pre">hip</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">LFLAGS</span> <span class="pre">=</span> <span class="pre">--rocm-path=${ROCM_PATH}</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">-L${ROCM_PATH}/lib</span> <span class="pre">-lamdhip64</span></code></div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">hipcc</span></code></p></td>
<td><div class="line-block">
<div class="line">Can be used directly to compile HIP source files.</div>
<div class="line">To see what is being invoked within this compiler driver, issue the command, <code class="docutils literal notranslate"><span class="pre">hipcc</span> <span class="pre">--verbose</span></code></div>
<div class="line">To explicitly target AMD MI250X, use <code class="docutils literal notranslate"><span class="pre">--offload-arch=gfx90a</span></code></div>
</div>
</td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>hipcc requires the ROCm Toolclain, See <a class="reference internal" href="#exposing-the-rocm-toolchain-to-your-programming-environment"><span class="std std-ref">Exposing The ROCm Toolchain to yourProgrammingEnvironment</span></a></p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Information about compiling code for different XNACK modes (which control page migration between GPU and CPU memory) can be found in the <a class="reference internal" href="#compiling-hip-kernels-for-xnack-modes"><span class="std std-ref">Compiling HIP kernels for specific XNACK modes</span></a> section.</p>
</div>
</section>
<section id="hip-openmp-cpu-threading">
<h3>HIP + OpenMP CPU Threading<a class="headerlink" href="#hip-openmp-cpu-threading" title="Link to this heading"></a></h3>
<p>This section shows how to compile HIP + OpenMP CPU threading hybrid codes.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Make sure the <code class="docutils literal notranslate"><span class="pre">craype-accel-amd-gfx90a</span></code> module is loaded when compiling HIP with the Cray compiler wrappers.</p>
</div>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Vendor</p></th>
<th class="head"><p>Compiler</p></th>
<th class="head"><p>Compile/Link Flags, Header Files, and Libraries</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td rowspan="2"><p>AMD/Cray</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">CC</span></code></p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">CFLAGS</span> <span class="pre">=</span> <span class="pre">-std=c++11</span> <span class="pre">-D__HIP_ROCclr__</span> <span class="pre">-D__HIP_ARCH_GFX90A__=1</span> <span class="pre">--rocm-path=${ROCM_PATH}</span> <span class="pre">--offload-arch=gfx90a</span> <span class="pre">-x</span> <span class="pre">hip</span> <span class="pre">-fopenmp</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">LFLAGS</span> <span class="pre">=</span> <span class="pre">--rocm-path=${ROCM_PATH}</span> <span class="pre">-fopenmp</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">-L${ROCM_PATH}/lib</span> <span class="pre">-lamdhip64</span></code></div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">hipcc</span></code></p></td>
<td><div class="line-block">
<div class="line">Can be used to directly compile HIP source files, add <code class="docutils literal notranslate"><span class="pre">-fopenmp</span></code> flag to enable OpenMP threading</div>
<div class="line">To explicitly target AMD MI250X, use <code class="docutils literal notranslate"><span class="pre">--offload-arch=gfx90a</span></code></div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>GNU</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">CC</span></code></p></td>
<td><div class="line-block">
<div class="line">The GNU compilers cannot be used to compile HIP code, so all HIP kernels must be separated from CPU code.</div>
<div class="line">During compilation, all non-HIP files must be compiled with <code class="docutils literal notranslate"><span class="pre">CC</span></code> while HIP kernels must be compiled with <code class="docutils literal notranslate"><span class="pre">hipcc</span></code>.</div>
<div class="line">Then linking must be performed with the <code class="docutils literal notranslate"><span class="pre">CC</span></code> wrapper.</div>
<div class="line">NOTE: When using <code class="docutils literal notranslate"><span class="pre">cmake</span></code>, HIP code must currently be compiled using <code class="docutils literal notranslate"><span class="pre">amdclang++</span></code> instead of <code class="docutils literal notranslate"><span class="pre">hipcc</span></code>.</div>
</div>
</td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>hipcc requires the ROCm Toolclain, See <a class="reference internal" href="#exposing-the-rocm-toolchain-to-your-programming-environment"><span class="std std-ref">Exposing The ROCm Toolchain to yourProgrammingEnvironment</span></a></p>
</div>
</section>
<section id="sycl">
<h3>SYCL<a class="headerlink" href="#sycl" title="Link to this heading"></a></h3>
<p>This section shows how to compile SYCL codes using the oneAPI DPC++ compiler.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Setup and load the oneAPI and ROCm modules:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">module</span> <span class="n">use</span> <span class="o">/</span><span class="n">sw</span><span class="o">/</span><span class="n">frontier</span><span class="o">/</span><span class="n">ums</span><span class="o">/</span><span class="n">ums015</span><span class="o">/</span><span class="n">modulefiles</span>
<span class="n">module</span> <span class="n">load</span> <span class="n">oneapi</span><span class="o">/</span><span class="n">tbb</span> <span class="n">oneapi</span><span class="o">/</span><span class="n">oclfpga</span> <span class="n">oneapi</span><span class="o">/</span><span class="n">compiler</span><span class="o">-</span><span class="n">rt</span> <span class="n">oneapi</span><span class="o">/</span><span class="n">compiler</span>
<span class="n">module</span> <span class="n">load</span> <span class="n">rocm</span><span class="o">/</span><span class="mf">5.4.3</span>
</pre></div>
</div>
</div>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Compiler</p></th>
<th class="head"><p>Compile/Link Flags, Header Files, and Libraries</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">icpx</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">CFLAGS</span> <span class="pre">=</span> <span class="pre">-fsycl</span> <span class="pre">-fsycl-targets=amdgcn-amd-amdhsa</span> <span class="pre">-Xsycl-target-backend</span> <span class="pre">--offload-arch=gfx90a</span></code>, or
<code class="docutils literal notranslate"><span class="pre">CFLAGS</span> <span class="pre">=</span> <span class="pre">-fsycl</span> <span class="pre">-fsycl-targets=amd_gpu_gfx90a</span></code></p></td>
</tr>
</tbody>
</table>
<p>Additional documentation on the DPC++ support for AMD can be found on
<a class="reference external" href="https://developer.codeplay.com/products/oneapi/amd/2024.1.0/guides/" target="_blank">Codeplays developer website</a>, in
particular the pages covering <a class="reference external" href="https://developer.codeplay.com/products/oneapi/amd/2024.1.0/guides/performance/common-optimizations" target="_blank">common optimizations</a>
or <a class="reference external" href="https://developer.codeplay.com/products/oneapi/amd/2024.1.0/guides/troubleshooting" target="_blank">troubleshooting</a>
can be helpful.</p>
<hr class="docutils" />
</section>
</section>
<section id="running-jobs">
<span id="frontier-running"></span><h2>Running Jobs<a class="headerlink" href="#running-jobs" title="Link to this heading"></a></h2>
<p>Computational work on Frontier is performed by <em>jobs</em>. Jobs typically consist of several componenets:</p>
<ul class="simple">
<li><p>A batch submission script</p></li>
<li><p>A binary executable</p></li>
<li><p>A set of input files for the executable</p></li>
<li><p>A set of output files created by the executable</p></li>
</ul>
<p>In general, the process for running a job is to:</p>
<ol class="arabic simple">
<li><p>Prepare executables and input files.</p></li>
<li><p>Write a batch script.</p></li>
<li><p>Submit the batch script to the batch scheduler.</p></li>
<li><p>Optionally monitor the job before and during execution.</p></li>
</ol>
<p>The following sections describe in detail how to create, submit, and manage jobs for execution on Frontier. Frontier uses SchedMDs Slurm Workload Manager as the batch scheduling system.</p>
<section id="login-vs-compute-nodes">
<h3>Login vs Compute Nodes<a class="headerlink" href="#login-vs-compute-nodes" title="Link to this heading"></a></h3>
<p>Recall from the System Overview that Frontier contains two node types: Login and Compute. When you connect to the system, you are placed on a <em>login</em> node. Login nodes are used for tasks such as code editing, compiling, etc. They are shared among all users of the system, so it is not appropriate to run tasks that are long/computationally intensive on login nodes. Users should also limit the number of simultaneous tasks on login nodes (e.g. concurrent tar commands, parallel make</p>
<p>Compute nodes are the appropriate place for long-running, computationally-intensive tasks. When you start a batch job, your batch script (or interactive shell for batch-interactive jobs) runs on one of your allocated compute nodes.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Compute-intensive, memory-intensive, or other disruptive processes running on login nodes may be killed without warning.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Unlike Summit and Titan, there are no launch/batch nodes on Frontier. This means your batch script runs on a node allocated to you rather than a shared node. You still must use the job launcher (<code class="docutils literal notranslate"><span class="pre">srun</span></code>) to run parallel jobs across all of your nodes, but serial tasks need not be launched with <code class="docutils literal notranslate"><span class="pre">srun</span></code>.</p>
</div>
</section>
<section id="simplified-node-layout">
<span id="frontier-simple"></span><h3>Simplified Node Layout<a class="headerlink" href="#simplified-node-layout" title="Link to this heading"></a></h3>
<p>To easily visualize job examples (see <a class="reference internal" href="#frontier-mapping"><span class="std std-ref">Process and Thread Mapping Examples</span></a> further below), the
compute node diagram has been simplified to the picture shown below.</p>
<a class="reference internal image-reference" href="../_images/Frontier_Node_Diagram_Simple.png"><img alt="Simplified Frontier node architecture diagram" class="align-center" src="../_images/Frontier_Node_Diagram_Simple.png" style="width: 100%;" /></a>
<p>In the diagram, each <strong>physical</strong> core on a Frontier compute node is composed
of two <strong>logical</strong> cores that are represented by a pair of blue and grey boxes.
For a given physical core, the blue box represents the logical core of the
first hardware thread, where the grey box represents the logical core of the
second hardware thread.</p>
<section id="low-noise-mode-layout">
<span id="frontier-lownoise"></span><h4>Low-noise Mode Layout<a class="headerlink" href="#low-noise-mode-layout" title="Link to this heading"></a></h4>
<p>Frontier uses low-noise mode and core specialization (<code class="docutils literal notranslate"><span class="pre">-S</span></code> flag at job
allocation, e.g., <code class="docutils literal notranslate"><span class="pre">sbatch</span></code>).  Low-noise mode constrains all system processes
to core 0.  Core specialization (by default, <code class="docutils literal notranslate"><span class="pre">-S</span> <span class="pre">8</span></code>) reserves the first core
in each L3 region.  This prevents the user running on the core that system
processes are constrained to.  This also means that there are only 56
allocatable cores by default instead of 64. Therefore, this modifies the
simplified node layout to:</p>
<a class="reference internal image-reference" href="../_images/Frontier_Node_Diagram_Simple_lownoise.png"><img alt="Simplified Frontier node architecture diagram (low-noise mode)" class="align-center" src="../_images/Frontier_Node_Diagram_Simple_lownoise.png" style="width: 100%;" /></a>
<p>To override this default layout (not recommended), set <code class="docutils literal notranslate"><span class="pre">-S</span> <span class="pre">0</span></code> at job allocation.</p>
</section>
</section>
<section id="slurm">
<span id="frontier-slurm"></span><h3>Slurm<a class="headerlink" href="#slurm" title="Link to this heading"></a></h3>
<p>Frontier uses SchedMDs Slurm Workload Manager for scheduling and managing jobs. Slurm maintains similar functionality to other schedulers such as IBMs LSF, but provides unique control of Frontiers resources through custom commands and options specific to Slurm. A few important commands can be found in the conversion table below, but please visit SchedMDs <a class="reference external" href="https://slurm.schedmd.com/rosetta.pdf" target="_blank">Rosetta Stone of Workload Managers</a> for a more complete conversion reference.</p>
<p>Slurm documentation for each command is available via the <code class="docutils literal notranslate"><span class="pre">man</span></code> utility, and on the web at <a class="reference external" href="https://slurm.schedmd.com/man_index.html" target="_blank">https://slurm.schedmd.com/man_index.html</a>. Additional documentation is available at <a class="reference external" href="https://slurm.schedmd.com/documentation.html" target="_blank">https://slurm.schedmd.com/documentation.html</a>.</p>
<p>Some common Slurm commands are summarized in the table below. More complete examples are given in the Monitoring and Modifying Batch Jobs section of this guide.</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Command</p></th>
<th class="head"><p>Action/Task</p></th>
<th class="head"><p>LSF Equivalent</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">squeue</span></code></p></td>
<td><p>Show the current queue</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">bjobs</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">sbatch</span></code></p></td>
<td><p>Submit a batch script</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">bsub</span></code></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">salloc</span></code></p></td>
<td><p>Submit an interactive job</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">bsub</span> <span class="pre">-Is</span> <span class="pre">$SHELL</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">srun</span></code></p></td>
<td><p>Launch a parallel job</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">jsrun</span></code></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">sinfo</span></code></p></td>
<td><p>Show node/partition info</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">bqueues</span></code> or <code class="docutils literal notranslate"><span class="pre">bhosts</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">sacct</span></code></p></td>
<td><p>View accounting information for jobs/job steps</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">bacct</span></code></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">scancel</span></code></p></td>
<td><p>Cancel a job or job step</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">bkill</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">scontrol</span></code></p></td>
<td><p>View or modify job configuration.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">bstop</span></code>, <code class="docutils literal notranslate"><span class="pre">bresume</span></code>, <code class="docutils literal notranslate"><span class="pre">bmod</span></code></p></td>
</tr>
</tbody>
</table>
</section>
<section id="batch-scripts">
<h3>Batch Scripts<a class="headerlink" href="#batch-scripts" title="Link to this heading"></a></h3>
<p>The most common way to interact with the batch system is via batch scripts. A batch script is simply a shell script with added directives to request various resoruces from or provide certain information to the scheduling system.  Aside from these directives, the batch script is simply the series of commands needed to set up and run your job.</p>
<p>To submit a batch script, use the command <code class="docutils literal notranslate"><span class="pre">sbatch</span> <span class="pre">myjob.sl</span></code></p>
<p>Consider the following batch script:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="ch">#!/bin/bash</span>
<span class="linenos"> 2</span><span class="c1">#SBATCH -A ABC123</span>
<span class="linenos"> 3</span><span class="c1">#SBATCH -J RunSim123</span>
<span class="linenos"> 4</span><span class="c1">#SBATCH -o %x-%j.out</span>
<span class="linenos"> 5</span><span class="c1">#SBATCH -t 1:00:00</span>
<span class="linenos"> 6</span><span class="c1">#SBATCH -p batch</span>
<span class="linenos"> 7</span><span class="c1">#SBATCH -N 1024</span>
<span class="linenos"> 8</span>
<span class="linenos"> 9</span><span class="nb">cd</span><span class="w"> </span><span class="nv">$MEMBERWORK</span>/abc123/Run.456
<span class="linenos">10</span>cp<span class="w"> </span><span class="nv">$PROJWORK</span>/abc123/RunData/Input.456<span class="w"> </span>./Input.456
<span class="linenos">11</span>srun<span class="w"> </span>...
<span class="linenos">12</span>cp<span class="w"> </span>my_output_file<span class="w"> </span><span class="nv">$PROJWORK</span>/abc123/RunData/Output.456
</pre></div>
</div>
<p>In the script, Slurm directives are preceded by <code class="docutils literal notranslate"><span class="pre">#SBATCH</span></code>, making them appear as comments to the shell. Slurm looks for these directives through the first non-comment, non-whitespace line. Options after that will be ignored by Slurm (and the shell).</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Line</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1</p></td>
<td><p>Shell interpreter line</p></td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td><p>OLCF project to charge</p></td>
</tr>
<tr class="row-even"><td><p>3</p></td>
<td><p>Job name</p></td>
</tr>
<tr class="row-odd"><td><p>4</p></td>
<td><p>Job standard output file (<code class="docutils literal notranslate"><span class="pre">%x</span></code> will be replaced with the job name and <code class="docutils literal notranslate"><span class="pre">%j</span></code> with the Job ID)</p></td>
</tr>
<tr class="row-even"><td><p>5</p></td>
<td><p>Walltime requested (in <code class="docutils literal notranslate"><span class="pre">HH:MM:SS</span></code> format). See the table below for other formats.</p></td>
</tr>
<tr class="row-odd"><td><p>6</p></td>
<td><p>Partition (queue) to use</p></td>
</tr>
<tr class="row-even"><td><p>7</p></td>
<td><p>Number of compute nodes requested</p></td>
</tr>
<tr class="row-odd"><td><p>8</p></td>
<td><p>Blank line</p></td>
</tr>
<tr class="row-even"><td><p>9</p></td>
<td><p>Change into the run directory</p></td>
</tr>
<tr class="row-odd"><td><p>10</p></td>
<td><p>Copy the input file into place</p></td>
</tr>
<tr class="row-even"><td><p>11</p></td>
<td><p>Run the job ( add layout details )</p></td>
</tr>
<tr class="row-odd"><td><p>12</p></td>
<td><p>Copy the output file to an appropriate location.</p></td>
</tr>
</tbody>
</table>
</section>
<section id="interactive-jobs">
<span id="frontier-interactive"></span><h3>Interactive Jobs<a class="headerlink" href="#interactive-jobs" title="Link to this heading"></a></h3>
<p>Most users will find batch jobs an easy way to use the system, as they allow you to hand off a job to the scheduler, allowing them to focus on other tasks while their job waits in the queue and eventually runs. Occasionally, it is necessary to run interactively, especially when developing, testing, modifying or debugging a code.</p>
<p>Since all compute resources are managed and scheduled by Slurm, it is not possible to simply log into the system and immediately begin running parallel codes interactively. Rather, you must request the appropriate resources from Slurm and, if necessary, wait for them to become available. This is done through an interactive batch job. Interactive batch jobs are submitted with the <code class="docutils literal notranslate"><span class="pre">salloc</span></code> command. Resources are requested via the same options that are passed via <code class="docutils literal notranslate"><span class="pre">#SBATCH</span></code> in a regular batch script (but without the <code class="docutils literal notranslate"><span class="pre">#SBATCH</span></code> prefix). For example, to request an interactive batch job with the same resources that the batch script above requests, you would use <code class="docutils literal notranslate"><span class="pre">salloc</span> <span class="pre">-A</span> <span class="pre">ABC123</span> <span class="pre">-J</span> <span class="pre">RunSim123</span> <span class="pre">-t</span> <span class="pre">1:00:00</span> <span class="pre">-p</span> <span class="pre">batch</span> <span class="pre">-N</span> <span class="pre">1024</span></code>. Note there is no option for an output fileyou are running interactively, so standard output and standard error will be displayed to the terminal.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Indicating your shell in your <code class="docutils literal notranslate"><span class="pre">salloc</span></code> command is NOT recommended (e.g., <code class="docutils literal notranslate"><span class="pre">salloc</span> <span class="pre">...</span> <span class="pre">/bin/bash</span></code>). Doing so causes your compute job to start on a login node by default rather than automatically moving you to a compute node.</p>
</div>
</section>
<section id="common-slurm-options">
<span id="id10"></span><h3>Common Slurm Options<a class="headerlink" href="#common-slurm-options" title="Link to this heading"></a></h3>
<p>The table below summarizes options for submitted jobs. Unless otherwise noted, they can be used for either batch scripts or interactive batch jobs. For scripts, they can be added on the <code class="docutils literal notranslate"><span class="pre">sbatch</span></code> command line or as a <code class="docutils literal notranslate"><span class="pre">#BSUB</span></code> directive in the batch script. (If theyre specified in both places, the command line takes precedence.) This is only a subset of all available options. Check the <a class="reference external" href="https://slurm.schedmd.com/man_index.html" target="_blank">Slurm Man Pages</a> for a more complete list.</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 15.0%" />
<col style="width: 28.0%" />
<col style="width: 57.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Option</p></th>
<th class="head"><p>Example Usage</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">-A</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">-A</span> <span class="pre">ABC123</span></code></p></td>
<td><p>Specifies the project to which the job should be charged</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">-N</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">-N</span> <span class="pre">1024</span></code></p></td>
<td><p>Request 1024 nodes for the job</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">-t</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">-t</span> <span class="pre">4:00:00</span></code></p></td>
<td><p>Request a walltime of 4 hours.
Walltime requests can be specified as minutes, hours:minutes, hours:minuts:seconds
days-hours, days-hours:minutes, or days-hours:minutes:seconds</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">--threads-per-core</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">--threads-per-core=2</span></code></p></td>
<td><div class="line-block">
<div class="line">Number of active hardware threads per core. Can be 1 or 2 (1 is default)</div>
<div class="line"><strong>Must</strong> be used if using <code class="docutils literal notranslate"><span class="pre">--threads-per-core=2</span></code> in your <code class="docutils literal notranslate"><span class="pre">srun</span></code> command.</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">-d</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">-d</span> <span class="pre">afterok:12345</span></code></p></td>
<td><p>Specify job dependency (in this example, this job cannot start until job 12345 exits
with an exit code of 0. See the Job Dependency section for more information</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">-C</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">-C</span> <span class="pre">nvme</span></code></p></td>
<td><p>Request the burst buffer/NVMe on each node be made available for your job. See
the Burst Buffers section for more information on using them.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">-J</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">-J</span> <span class="pre">MyJob123</span></code></p></td>
<td><p>Specify the job name (this will show up in queue listings)</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">-o</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">-o</span> <span class="pre">jobout.%j</span></code></p></td>
<td><p>File where job STDOUT will be directed (%j will be replaced with the job ID).
If no <cite>-e</cite> option is specified, job STDERR will be placed in this file, too.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">-e</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">-e</span> <span class="pre">joberr.%j</span></code></p></td>
<td><p>File where job STDERR will be directed (%j will be replaced with the job ID).
If no <cite>-o</cite> option is specified, job STDOUT will be placed in this file, too.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">--mail-type</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">--mail-type=END</span></code></p></td>
<td><p>Send email for certain job actions. Can be a comma-separated list. Actions include
BEGIN, END, FAIL, REQUEUE, INVALID_DEPEND, STAGE_OUT, ALL, and more.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--mail-user</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">--mail-user=user&#64;somewhere.com</span></code></p></td>
<td><p>Email address to be used for notifications.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">--reservation</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">--reservation=MyReservation.1</span></code></p></td>
<td><p>Instructs Slurm to run a job on nodes that are part of the specified reservation.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">-S</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">-S</span> <span class="pre">8</span></code></p></td>
<td><p>Instructs Slurm to reserve a specific number of cores per node (default is 8).
Reserved cores cannot be used by the application.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">--signal</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">--signal=USR1&#64;300</span></code></p></td>
<td><div class="line-block">
<div class="line">Send the given signal to a job the specified time (in seconds) seconds before the
job reaches its walltime. The signal can be by name or by number (i.e. both 10 and
USR1 would send SIGUSR1).</div>
<div class="line"><br /></div>
<div class="line">Signaling a job can be used, for example, to force a job to write a checkpoint just
before Slurm kills the job (note that this option only sends the signal; the user
must still make sure their job script traps the signal and handles it in the desired
manner).</div>
<div class="line"><br /></div>
<div class="line">When used with <code class="docutils literal notranslate"><span class="pre">sbatch</span></code>, the signal can be prefixed by B:
(e.g. <code class="docutils literal notranslate"><span class="pre">--signal=B:USR1&#64;300</span></code>) to tell Slurm to signal only the batch shell;
otherwise all processes will be signaled.</div>
</div>
</td>
</tr>
</tbody>
</table>
</section>
<section id="slurm-environment-variables">
<h3>Slurm Environment Variables<a class="headerlink" href="#slurm-environment-variables" title="Link to this heading"></a></h3>
<p>Slurm reads a number of environment variables, many of which can provide the same information as the job options noted above. We recommend using the job options rather than environment variables to specify job options, as it allows you to have everything self-contained within the job submission script (rather than having to remember what options you set for a given job).</p>
<p>Slurm also provides a number of environment variables within your running job. The following table summarizes those that may be particularly useful within your job (e.g. for naming output log files):</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Variable</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">$SLURM_SUBMIT_DIR</span></code></p></td>
<td><p>The directory from which the batch job was submitted. By default, a new job starts
in your home directory. You can get back to the directory of job submission with
<code class="docutils literal notranslate"><span class="pre">cd</span> <span class="pre">$SLURM_SUBMIT_DIR</span></code>. Note that this is not necessarily the same directory in which
the batch script resides.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">$SLURM_JOBID</span></code></p></td>
<td><p>The jobs full identifier. A common use for <code class="docutils literal notranslate"><span class="pre">$SLURM_JOBID</span></code> is to append the jobs ID
to the standard output and error files.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">$SLURM_JOB_NUM_NODES</span></code></p></td>
<td><p>The number of nodes requested.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">$SLURM_JOB_NAME</span></code></p></td>
<td><p>The job name supplied by the user.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">$SLURM_NODELIST</span></code></p></td>
<td><p>The list of nodes assigned to the job.</p></td>
</tr>
</tbody>
</table>
</section>
<section id="job-states">
<h3>Job States<a class="headerlink" href="#job-states" title="Link to this heading"></a></h3>
<p>A job will transition through several states during its lifetime. Common ones include:</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>State
Code</p></th>
<th class="head"><p>State</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>CA</p></td>
<td><p>Canceled</p></td>
<td><p>The job was canceled (couldve been by the user or an administrator)</p></td>
</tr>
<tr class="row-odd"><td><p>CD</p></td>
<td><p>Completed</p></td>
<td><p>The job completed successfully (exit code 0)</p></td>
</tr>
<tr class="row-even"><td><p>CG</p></td>
<td><p>Completing</p></td>
<td><p>The job is in the process of completing (some processes may still be running)</p></td>
</tr>
<tr class="row-odd"><td><p>PD</p></td>
<td><p>Pending</p></td>
<td><p>The job is waiting for resources to be allocated</p></td>
</tr>
<tr class="row-even"><td><p>R</p></td>
<td><p>Running</p></td>
<td><p>The job is currently running</p></td>
</tr>
</tbody>
</table>
</section>
<section id="job-reason-codes">
<h3>Job Reason Codes<a class="headerlink" href="#job-reason-codes" title="Link to this heading"></a></h3>
<p>In addition to state codes, jobs that are pending will have a reason code to explain why the job is pending. Completed jobs will have a reason describing how the job ended. Some codes you might see include:</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Reason</p></th>
<th class="head"><p>Meaning</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Dependency</p></td>
<td><p>Job has dependencies that have not been met</p></td>
</tr>
<tr class="row-odd"><td><p>JobHeldUser</p></td>
<td><p>Job is held at users request</p></td>
</tr>
<tr class="row-even"><td><p>JobHeldAdmin</p></td>
<td><p>Job is held at system administrators request</p></td>
</tr>
<tr class="row-odd"><td><p>Priority</p></td>
<td><p>Other jobs with higher priority exist for the partition/reservation</p></td>
</tr>
<tr class="row-even"><td><p>Reservation</p></td>
<td><p>The job is waiting for its reservation to become available</p></td>
</tr>
<tr class="row-odd"><td><p>AssocMaxJobsLimit</p></td>
<td><p>The job is being held because the user/project has hit the limit on running jobs</p></td>
</tr>
<tr class="row-even"><td><p>ReqNodeNotAvail</p></td>
<td><p>The requested a particular node, but its currently unavailable (its in use, reserved, down, draining, etc.)</p></td>
</tr>
<tr class="row-odd"><td><p>JobLaunchFailure</p></td>
<td><p>Job failed to launch (could due to system problems, invalid program name, etc.)</p></td>
</tr>
<tr class="row-even"><td><p>NonZeroExitCode</p></td>
<td><p>The job exited with some code other than 0</p></td>
</tr>
</tbody>
</table>
<p>Many other states and job reason codes exist. For a more complete description, see the <code class="docutils literal notranslate"><span class="pre">squeue</span></code> man page (either on the system or online).</p>
</section>
<section id="scheduling-policy">
<h3>Scheduling Policy<a class="headerlink" href="#scheduling-policy" title="Link to this heading"></a></h3>
<p>In a simple batch queue system, jobs run in a first-in, first-out (FIFO) order. This can lead to inefficient use of the system. If a large job is the next to run, a strict FIFO queue can cause nodes to sit idle while waiting for the large job to start. <em>Backfilling</em> would allow smaller, shorter jobs to use those resources that would otherwise remain idle until the large job starts. With the proper algorithm, they would do so without impacting the start time of the large job. While this does make more efficient use of the system, it encourages the submission of smaller jobs.</p>
<section id="the-doe-leadership-class-job-mandate">
<h4>The DOE Leadership-Class Job Mandate<a class="headerlink" href="#the-doe-leadership-class-job-mandate" title="Link to this heading"></a></h4>
<p>As a DOE Leadership Computing Facility, OLCF has a mandate that a large portion of Frontiers usage come from large, <em>leadership-class</em> (a.k.a. <em>capability</em>) jobs. To ensure that OLCF complies with this directive, we strongly encourage users to run jobs on Frontier that are as large as their code will allow. To that end, OLCF implements queue policies that enable large jobs to run in a timely fashion.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The OLCF implements queue policies that encourage the submission and timely execution of large, leadership-class jobs on Frontier.</p>
</div>
<p>The basic priority mechanism for jobs waiting in the queue is the time the job has been waiting in the queue. If your jobs require resources outside these policies such as higher priority or longer walltimes, please contact <a class="reference external" href="mailto:help&#37;&#52;&#48;olcf&#46;ornl&#46;gov" target="_blank">help<span>&#64;</span>olcf<span>&#46;</span>ornl<span>&#46;</span>gov</a></p>
</section>
<section id="job-priority-by-node-count">
<h4>Job Priority by Node Count<a class="headerlink" href="#job-priority-by-node-count" title="Link to this heading"></a></h4>
<p>Jobs are <em>aged</em> according to the jobs requested node count (older
age equals higher queue priority). Each jobs requested node count
places it into a specific <em>bin</em>. Each bin has a different aging
parameter, which all jobs in the bin receive.</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Bin</p></th>
<th class="head"><p>Min Nodes</p></th>
<th class="head"><p>Max Nodes</p></th>
<th class="head"><p>Max Walltime (Hours)</p></th>
<th class="head"><p>Aging Boost (Days)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1</p></td>
<td><p>5,645</p></td>
<td><p>9,408</p></td>
<td><p>12.0</p></td>
<td><p>8</p></td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td><p>1,882</p></td>
<td><p>5,644</p></td>
<td><p>12.0</p></td>
<td><p>4</p></td>
</tr>
<tr class="row-even"><td><p>3</p></td>
<td><p>184</p></td>
<td><p>1,881</p></td>
<td><p>12.0</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>4</p></td>
<td><p>92</p></td>
<td><p>183</p></td>
<td><p>6.0</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-even"><td><p>5</p></td>
<td><p>1</p></td>
<td><p>91</p></td>
<td><p>2.0</p></td>
<td><p>0</p></td>
</tr>
</tbody>
</table>
</section>
<section id="batch-partition-queue-policy">
<h4><code class="docutils literal notranslate"><span class="pre">batch</span></code> Partition (queue) Policy<a class="headerlink" href="#batch-partition-queue-policy" title="Link to this heading"></a></h4>
<p>The <code class="docutils literal notranslate"><span class="pre">batch</span></code> partition (queue) is the default partition for production work on Frontier. Most work on Frontier is handled through this partition. The following policies are enforced for the <code class="docutils literal notranslate"><span class="pre">batch</span></code> partition:</p>
<ul class="simple">
<li><p>Limit of four <em>eligible-to-run</em> jobs per user. (Jobs in excess of this number will be held, but will move to the eligible-to-run state at the appropriate time.)</p></li>
<li><p>Users may have only 100 jobs queued across all partitions at any time (this includes jobs in all states) i.e. jobs submitted in different partitions on Frontier are added up together to check if its within the 100 queued jobs limit. Additional jobs will be rejected at submit time.</p></li>
</ul>
</section>
<section id="extended-partition-queue-policy">
<h4><code class="docutils literal notranslate"><span class="pre">extended</span></code> Partition (queue) Policy<a class="headerlink" href="#extended-partition-queue-policy" title="Link to this heading"></a></h4>
<p>The <code class="docutils literal notranslate"><span class="pre">extended</span></code> partition (queue) is designated for smaller long-running jobs on Frontier. The following policies are enforced for the <code class="docutils literal notranslate"><span class="pre">extended</span></code> partition:</p>
<ul class="simple">
<li><p>24-Hour maximum wall time for each queued job.</p></li>
<li><p>64-Node maximum job size for each queued job.</p></li>
<li><p>Each user will be allowed 1 running job and 1 <em>eligible-to-run</em> job at a given time. Any additional queued jobs will be held in an ineligible state until the previous job runs.</p></li>
<li><p>Users may have only 100 jobs queued across all partitions at any time (this includes jobs in all states) i.e. jobs submitted in different partitions on Frontier are added up together to check if its within the 100 queued jobs limit. Additional jobs will be rejected at submit time.</p></li>
</ul>
</section>
<section id="debug-quality-of-service-class">
<h4><code class="docutils literal notranslate"><span class="pre">debug</span></code> Quality of Service Class<a class="headerlink" href="#debug-quality-of-service-class" title="Link to this heading"></a></h4>
<p>The <code class="docutils literal notranslate"><span class="pre">debug</span></code> quality of service (QOS) class can be used to access Frontiers compute resources for short non-production debug tasks. The QOS provides a higher priority compare to jobs of the same job size bin in production partitions. Production work and job chaining using the <code class="docutils literal notranslate"><span class="pre">debug</span></code> QOS is prohibited. Each user is limited to one job in any state at any one point. Attempts to submit multiple jobs to this QOS will be rejected upon job submission.</p>
<p>To submit a job to the <code class="docutils literal notranslate"><span class="pre">debug</span></code> QOS, add the <cite>-q debug</cite> option to your <code class="docutils literal notranslate"><span class="pre">sbatch</span></code> or <code class="docutils literal notranslate"><span class="pre">salloc</span></code> command or <code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">-q</span> <span class="pre">debug</span></code> to your job script.</p>
</section>
<section id="allocation-overuse-policy">
<h4>Allocation Overuse Policy<a class="headerlink" href="#allocation-overuse-policy" title="Link to this heading"></a></h4>
<p>Projects that overrun their allocation are still allowed to run on OLCF systems, although at a reduced priority. Like the adjustment for the number of processors requested above, this is an adjustment to the apparent submit time of the job. However, this adjustment has the effect of making jobs appear much younger than jobs submitted under projects that have not exceeded their allocation. In addition to the priority change, these jobs are also limited in the amount of wall time that can be used. For example, consider that <cite>job1</cite> is submitted at the same time as <cite>job2</cite>. The project associated with <cite>job1</cite> is over its allocation, while the project for <cite>job2</cite> is not. The batch system will consider <cite>job2</cite> to have been waiting for a longer time than <cite>job1</cite>. Additionally, projects that are at 125% of their allocated time will be limited to only 3 running jobs at a time. The adjustment to the apparent submit time depends upon the percentage that the project is over its allocation, as shown in the table below:</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>% of Allocation Used</p></th>
<th class="head"><p>Priority
Reduction</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>&lt; 100%</p></td>
<td><p>none</p></td>
</tr>
<tr class="row-odd"><td><p>&gt;=100% but &lt;=125%</p></td>
<td><p>30 days</p></td>
</tr>
<tr class="row-even"><td><p>&gt; 125%</p></td>
<td><p>365 days</p></td>
</tr>
</tbody>
</table>
</section>
<section id="system-reservation-policy">
<h4>System Reservation Policy<a class="headerlink" href="#system-reservation-policy" title="Link to this heading"></a></h4>
<p>Projects may request to reserve a set of nodes for a period of time by contacting <a class="reference external" href="mailto:help&#37;&#52;&#48;olcf&#46;ornl&#46;gov" target="_blank">help<span>&#64;</span>olcf<span>&#46;</span>ornl<span>&#46;</span>gov</a>. If the reservation is granted, the reserved nodes will be blocked from general use for a given period of time. Only users that have been authorized to use the reservation can utilize those resources. Since no other users can access the reserved resources, it is crucial that groups given reservations take care to ensure the utilization on those resources remains high. To prevent reserved resources from remaining idle for an extended period of time, reservations are monitored for inactivity. If activity falls below 50% of the reserved resources for more than (30) minutes, the reservation will be canceled and the system will be returned to normal scheduling. A new reservation must be requested if this occurs.</p>
<p>The requesting projects allocation is charged according to the time window granted, regardless of actual utilization. For example, an 8-hour, 2,000 node reservation on Frontier would be equivalent to using 16,000 Frontier node-hours of a projects allocation.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Reservations should not be confused with priority requests. If quick turnaround is needed for a few jobs or for a period of time, a priority boost should be requested. A reservation should only be requested if users need to guarantee availability of a set of nodes at a given time, such as for a live demonstration at a conference.</p>
</div>
</section>
</section>
<section id="job-dependencies">
<h3>Job Dependencies<a class="headerlink" href="#job-dependencies" title="Link to this heading"></a></h3>
<p>Oftentimes, a job will need data from some other job in the queue, but its nonetheless convenient to submit the second job before the first finishes. Slurm allows you to submit a job with constraints that will keep it from running until these dependencies are met. These are specified with the <code class="docutils literal notranslate"><span class="pre">-d</span></code> option to Slurm. Common dependency flags are summarized below. In each of these examples, only a single jobid is shown but you can specify multiple job IDs as a colon-delimited list (i.e. <code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">-d</span> <span class="pre">afterok:12345:12346:12346</span></code>). For the <code class="docutils literal notranslate"><span class="pre">after</span></code> dependency, you can optionally specify a <code class="docutils literal notranslate"><span class="pre">+time</span></code> value for each jobid.</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 25.0%" />
<col style="width: 75.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Flag</p></th>
<th class="head"><p>Meaning (for the dependent job)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">-d</span> <span class="pre">after:jobid[+time]</span></code></p></td>
<td><p>The job can start after the specified jobs start or are canceled. The optional <code class="docutils literal notranslate"><span class="pre">+time</span></code> argument
is a number of minutes. If specified, the job cannot start until that many minutes have passed
since the listed jobs start/are canceled. If not specified, there is no delay.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">-d</span> <span class="pre">afterany:jobid</span></code></p></td>
<td><p>The job can start after the specified jobs have ended (regardless of exit state)</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">-d</span> <span class="pre">afternotok:jobid</span></code></p></td>
<td><p>The job can start after the specified jobs terminate in a failed (non-zero) state</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">-d</span> <span class="pre">afterok:jobid</span></code></p></td>
<td><p>The job can start after the specified jobs complete successfully (i.e. zero exit code)</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">-d</span> <span class="pre">singleton</span></code></p></td>
<td><p>Job can begin after any previously-launched job with the same name and from the same user
have completed. In other words, serialize the running jobs based on username+jobname pairs.</p></td>
</tr>
</tbody>
</table>
</section>
<section id="monitoring-and-modifying-batch-jobs">
<h3>Monitoring and Modifying Batch Jobs<a class="headerlink" href="#monitoring-and-modifying-batch-jobs" title="Link to this heading"></a></h3>
<section id="scontrol-hold-and-scontrol-release-holding-and-releasing-jobs">
<h4><code class="docutils literal notranslate"><span class="pre">scontrol</span> <span class="pre">hold</span></code> and <code class="docutils literal notranslate"><span class="pre">scontrol</span> <span class="pre">release</span></code>: Holding and Releasing Jobs<a class="headerlink" href="#scontrol-hold-and-scontrol-release-holding-and-releasing-jobs" title="Link to this heading"></a></h4>
<p>Sometimes you may need to place a hold on a job to keep it from starting. For example, you may have submitted it assuming some needed data was in place but later realized that data is not yet available. This can be done with the <code class="docutils literal notranslate"><span class="pre">scontrol</span> <span class="pre">hold</span></code> command. Later, when the data is ready, you can release the job (i.e. tell the system that its now OK to run the job) with the <code class="docutils literal notranslate"><span class="pre">scontrol</span> <span class="pre">release</span></code> command. For example:</p>
<table class="docutils align-default">
<tbody>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">scontrol</span> <span class="pre">hold</span> <span class="pre">12345</span></code></p></td>
<td><p>Place job 12345 on hold</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">scontrol</span> <span class="pre">release</span> <span class="pre">12345</span></code></p></td>
<td><p>Release job 12345 (i.e. tell the system its OK to run it)</p></td>
</tr>
</tbody>
</table>
</section>
<section id="scontrol-update-changing-job-parameters">
<h4><code class="docutils literal notranslate"><span class="pre">scontrol</span> <span class="pre">update</span></code>: Changing Job Parameters<a class="headerlink" href="#scontrol-update-changing-job-parameters" title="Link to this heading"></a></h4>
<p>There may also be occasions where you want to modify a job thats waiting in the queue. For example, perhaps you requested 2,000 nodes but later realized this is a different data set and only needs 1,000 nodes. You can use the <code class="docutils literal notranslate"><span class="pre">scontrol</span> <span class="pre">update</span></code> command for this. For example:</p>
<table class="docutils align-default">
<tbody>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">scontrol</span> <span class="pre">update</span> <span class="pre">NumNodes=1000</span> <span class="pre">JobID=12345</span></code></p></td>
<td><p>Change job 12345s node request to 1000 nodes</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">scontrol</span> <span class="pre">update</span> <span class="pre">TimeLimit=4:00:00</span> <span class="pre">JobID=12345</span></code></p></td>
<td><p>Change job 12345s max walltime to 4 hours</p></td>
</tr>
</tbody>
</table>
</section>
<section id="scancel-cancel-or-signal-a-job">
<h4><code class="docutils literal notranslate"><span class="pre">scancel</span></code>: Cancel or Signal a Job<a class="headerlink" href="#scancel-cancel-or-signal-a-job" title="Link to this heading"></a></h4>
<p>In addition to the <code class="docutils literal notranslate"><span class="pre">--signal</span></code> option for the <code class="docutils literal notranslate"><span class="pre">sbatch</span></code>/<code class="docutils literal notranslate"><span class="pre">salloc</span></code> commands described <a class="reference internal" href="#common-slurm-options"><span class="std std-ref">above</span></a>, the <code class="docutils literal notranslate"><span class="pre">scancel</span></code> command can be used to manually signal a job. Typically, this is used to remove a job from the queue. In this use case, you do not need to specify a signal and can simply provide the jobid (i.e. <code class="docutils literal notranslate"><span class="pre">scancel</span> <span class="pre">12345</span></code>). If you want to send some other signal to the job, use <code class="docutils literal notranslate"><span class="pre">scancel</span></code> the with the <code class="docutils literal notranslate"><span class="pre">-s</span></code> option. The <code class="docutils literal notranslate"><span class="pre">-s</span></code> option allows signals to be specified either by number or by name. Thus, if you want to send <code class="docutils literal notranslate"><span class="pre">SIGUSR1</span></code> to a job, you would use <code class="docutils literal notranslate"><span class="pre">scancel</span> <span class="pre">-s</span> <span class="pre">10</span> <span class="pre">12345</span></code> or <code class="docutils literal notranslate"><span class="pre">scancel</span> <span class="pre">-s</span> <span class="pre">USR1</span> <span class="pre">12345</span></code>.</p>
</section>
<section id="squeue-view-the-queue">
<h4><code class="docutils literal notranslate"><span class="pre">squeue</span></code>: View the Queue<a class="headerlink" href="#squeue-view-the-queue" title="Link to this heading"></a></h4>
<p>The <code class="docutils literal notranslate"><span class="pre">squeue</span></code> command is used to show the batch queue. You can filter the level of detail through several command-line options. For example:</p>
<table class="docutils align-default">
<tbody>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">squeue</span> <span class="pre">-l</span></code></p></td>
<td><p>Show all jobs currently in the queue</p></td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">squeue</span> <span class="pre">-l</span> <span class="pre">-u</span> <span class="pre">$USER</span></code></div>
</div>
</td>
<td><p>Show all of <em>your</em> jobs currently in the queue</p></td>
</tr>
</tbody>
</table>
</section>
<section id="sacct-get-job-accounting-information">
<h4><code class="docutils literal notranslate"><span class="pre">sacct</span></code>: Get Job Accounting Information<a class="headerlink" href="#sacct-get-job-accounting-information" title="Link to this heading"></a></h4>
<p>The <code class="docutils literal notranslate"><span class="pre">sacct</span></code> command gives detailed information about jobs currently in the queue and recently-completed jobs. You can also use it to see the various steps within a batch jobs.</p>
<table class="docutils align-default">
<tbody>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">sacct</span> <span class="pre">-a</span> <span class="pre">-X</span></code></p></td>
<td><p>Show all jobs (<code class="docutils literal notranslate"><span class="pre">-a</span></code>) in the queue, but summarize the whole allocation instead of showing individual steps (<code class="docutils literal notranslate"><span class="pre">-X</span></code>)</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">sacct</span> <span class="pre">-u</span> <span class="pre">$USER</span></code></p></td>
<td><p>Show all of your jobs, and show the individual steps (since there was no <code class="docutils literal notranslate"><span class="pre">-X</span></code> option)</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">sacct</span> <span class="pre">-j</span> <span class="pre">12345</span></code></p></td>
<td><p>Show all job steps that are part of job 12345</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">sacct</span> <span class="pre">-u</span> <span class="pre">$USER</span> <span class="pre">-S</span> <span class="pre">2022-07-01T13:00:00</span> <span class="pre">-o</span> <span class="pre">&quot;jobid%5,jobname%25,nodelist%20&quot;</span> <span class="pre">-X</span></code></p></td>
<td><p>Show all of your jobs since 1 PM on July 1, 2022 using a particular output format</p></td>
</tr>
</tbody>
</table>
</section>
<section id="scontrol-show-job-get-detailed-job-information">
<h4><code class="docutils literal notranslate"><span class="pre">scontrol</span> <span class="pre">show</span> <span class="pre">job</span></code>: Get Detailed Job Information<a class="headerlink" href="#scontrol-show-job-get-detailed-job-information" title="Link to this heading"></a></h4>
<p>In addition to holding, releasing, and updating the job, the <code class="docutils literal notranslate"><span class="pre">scontrol</span></code> command can show detailed job information via the <code class="docutils literal notranslate"><span class="pre">show</span> <span class="pre">job</span></code> subcommand. For example, <code class="docutils literal notranslate"><span class="pre">scontrol</span> <span class="pre">show</span> <span class="pre">job</span> <span class="pre">12345</span></code>.</p>
</section>
</section>
<section id="srun">
<span id="frontier-srun"></span><h3>Srun<a class="headerlink" href="#srun" title="Link to this heading"></a></h3>
<p>The default job launcher for Frontier is <a class="reference external" href="https://slurm.schedmd.com/srun.html" target="_blank">srun</a> . The <code class="docutils literal notranslate"><span class="pre">srun</span></code> command is used to execute an MPI binary on one or more compute nodes in parallel.</p>
<section id="srun-format">
<h4>Srun Format<a class="headerlink" href="#srun-format" title="Link to this heading"></a></h4>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">srun</span>  <span class="p">[</span><span class="n">OPTIONS</span><span class="o">...</span> <span class="p">[</span><span class="n">executable</span> <span class="p">[</span><span class="n">args</span><span class="o">...</span><span class="p">]]]</span>
</pre></div>
</div>
<p>Single Command (non-interactive)</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>srun<span class="w"> </span>-A<span class="w"> </span>&lt;project_id&gt;<span class="w"> </span>-t<span class="w"> </span><span class="m">00</span>:05:00<span class="w"> </span>-p<span class="w"> </span>&lt;partition&gt;<span class="w"> </span>-N<span class="w"> </span><span class="m">2</span><span class="w"> </span>-n<span class="w"> </span><span class="m">4</span><span class="w"> </span>--ntasks-per-node<span class="o">=</span><span class="m">2</span><span class="w"> </span>./a.out
&lt;output<span class="w"> </span>printed<span class="w"> </span>to<span class="w"> </span>terminal&gt;
</pre></div>
</div>
<p>The job name and output options have been removed since stdout/stderr are typically desired in the terminal window in this usage mode.</p>
<p><code class="docutils literal notranslate"><span class="pre">srun</span></code> accepts the following common options:</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 30.0%" />
<col style="width: 70.0%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">-N</span></code></p></td>
<td><p>Number of nodes</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">-n</span></code></p></td>
<td><p>Total number of MPI tasks (default is 1)</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">-c,</span> <span class="pre">--cpus-per-task=&lt;ncpus&gt;</span></code></p></td>
<td><div class="line-block">
<div class="line">Logical cores per MPI task (default is 1)</div>
<div class="line">When used with <code class="docutils literal notranslate"><span class="pre">--threads-per-core=1</span></code>: <code class="docutils literal notranslate"><span class="pre">-c</span></code> is equivalent to <em>physical</em> cores per task</div>
<div class="line">By default, when <code class="docutils literal notranslate"><span class="pre">-c</span> <span class="pre">&gt;</span> <span class="pre">1</span></code>, additional cores per task are distributed within one L3 region
first before filling a different L3 region.</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--cpu-bind=threads</span></code></p></td>
<td><div class="line-block">
<div class="line">Bind tasks to CPUs.</div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">threads</span></code> - (default, recommended) Automatically generate masks binding tasks to threads.</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">--threads-per-core=&lt;threads&gt;</span></code></p></td>
<td><div class="line-block">
<div class="line">In task layout, use the specified maximum number of hardware threads per core</div>
<div class="line">(default is 1; there are 2 hardware threads per physical CPU core).</div>
<div class="line">Must also be set in <code class="docutils literal notranslate"><span class="pre">salloc</span></code> or <code class="docutils literal notranslate"><span class="pre">sbatch</span></code> if using <code class="docutils literal notranslate"><span class="pre">--threads-per-core=2</span></code> in your <code class="docutils literal notranslate"><span class="pre">srun</span></code> command.</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">-m,</span> <span class="pre">--distribution=&lt;value&gt;:&lt;value&gt;:&lt;value&gt;</span></code></p></td>
<td><div class="line-block">
<div class="line">Specifies the distribution of MPI ranks across compute nodes, sockets (L3 regions), and cores, respectively.</div>
<div class="line">The default values are <code class="docutils literal notranslate"><span class="pre">block:cyclic:cyclic</span></code>, see <code class="docutils literal notranslate"><span class="pre">man</span> <span class="pre">srun</span></code> for more information.</div>
<div class="line">Currently, the distribution setting for cores (the third &lt;value&gt; entry) has no effect on Frontier</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">--ntasks-per-node=&lt;ntasks&gt;</span></code></p></td>
<td><div class="line-block">
<div class="line">If used without <code class="docutils literal notranslate"><span class="pre">-n</span></code>: requests that a specific number of tasks be invoked on each node.</div>
<div class="line">If used with <code class="docutils literal notranslate"><span class="pre">-n</span></code>: treated as a <em>maximum</em> count of tasks per node.</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--gpus</span></code></p></td>
<td><p>Specify the number of GPUs required for the job (total GPUs across all nodes).</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">--gpus-per-node</span></code></p></td>
<td><p>Specify the number of GPUs per node required for the job.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--gpu-bind=closest</span></code></p></td>
<td><p>Binds each task to the GPU which is on the same NUMA domain as the CPU core the MPI rank is running on.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">--gpu-bind=map_gpu:&lt;list&gt;</span></code></p></td>
<td><p>Bind tasks to specific GPUs by setting GPU masks on tasks (or ranks) as specified where
<code class="docutils literal notranslate"><span class="pre">&lt;list&gt;</span></code> is <code class="docutils literal notranslate"><span class="pre">&lt;gpu_id_for_task_0&gt;,&lt;gpu_id_for_task_1&gt;,...</span></code>. If the number of tasks (or
ranks) exceeds the number of elements in this list, elements in the list will be reused as
needed starting from the beginning of the list. To simplify support for large task
counts, the lists may follow a map with an asterisk and repetition count. (For example
<code class="docutils literal notranslate"><span class="pre">map_gpu:0*4,1*4</span></code>)</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--ntasks-per-gpu=&lt;ntasks&gt;</span></code></p></td>
<td><p>Request that there are ntasks tasks invoked for every GPU.</p></td>
</tr>
</tbody>
</table>
<p>Below is a comparison table between <code class="docutils literal notranslate"><span class="pre">srun</span></code> and <code class="docutils literal notranslate"><span class="pre">jsrun</span></code>.</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Option</p></th>
<th class="head"><p>jsrun (Summit)</p></th>
<th class="head"><p>srun  (Frontier)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Number of nodes</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">-nnodes</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">-N,</span> <span class="pre">--nnodes</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Number of tasks</p></td>
<td><p>defined with resource set</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">-n,</span> <span class="pre">--ntasks</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Number of tasks per node</p></td>
<td><p>defined with resource set</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">--ntasks-per-node</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Number of CPUs per task</p></td>
<td><p>defined with resource set</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">-c,</span> <span class="pre">--cpus-per-task</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Number of resource sets</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">-n,</span> <span class="pre">--nrs</span></code></p></td>
<td><p>N/A</p></td>
</tr>
<tr class="row-odd"><td><p>Number of resource sets per host</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">-r,</span> <span class="pre">--rs_per_host</span></code></p></td>
<td><p>N/A</p></td>
</tr>
<tr class="row-even"><td><p>Number of tasks per resource set</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">-a,</span> <span class="pre">--tasks_per_rs</span></code></p></td>
<td><p>N/A</p></td>
</tr>
<tr class="row-odd"><td><p>Number of CPUs per resource set</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">-c,</span> <span class="pre">--cpus_per_rs</span></code></p></td>
<td><p>N/A</p></td>
</tr>
<tr class="row-even"><td><p>Number of GPUs per resource set</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">-g,</span> <span class="pre">--gpus_per_rs</span></code></p></td>
<td><p>N/A</p></td>
</tr>
<tr class="row-odd"><td><p>Bind tasks to allocated CPUs</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">-b,</span> <span class="pre">--bind</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">--cpu-bind</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Performance binding preference</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">-l,--latency_priority</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">--hint</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Specify the task to resource mapping pattern</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">--launch_distribution</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">-m,</span> <span class="pre">--distribution</span></code></p></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="process-and-thread-mapping-examples">
<span id="frontier-mapping"></span><h3>Process and Thread Mapping Examples<a class="headerlink" href="#process-and-thread-mapping-examples" title="Link to this heading"></a></h3>
<p>This section describes how to map processes (e.g., MPI ranks) and process
threads (e.g., OpenMP threads) to the CPUs, GPUs, and NICs on Frontier.</p>
<p>Users are highly encouraged to use the CPU- and GPU-mapping programs used in
the following sections to check their understanding of the job steps (i.e.,
<code class="docutils literal notranslate"><span class="pre">srun</span></code> commands) they intend to use in their actual jobs.</p>
<ul>
<li><p>For the <a class="reference internal" href="#frontier-cpu-map"><span class="std std-ref">CPU Mapping</span></a> and <a class="reference internal" href="#frontier-multi-map"><span class="std std-ref">Multithreading</span></a> sections:</p>
<p>A simple MPI+OpenMP Hello, World program (<a class="reference external" href="https://code.ornl.gov/olcf/hello_mpi_omp" target="_blank">hello_mpi_omp</a>) will be used to clarify the
mappings.</p>
</li>
<li><p>For the <a class="reference internal" href="#frontier-gpu-map"><span class="std std-ref">GPU Mapping</span></a> section:</p>
<p>An MPI+OpenMP+HIP Hello, World program (<a class="reference external" href="https://code.ornl.gov/olcf/hello_jobstep" target="_blank">hello_jobstep</a>) will be used to clarify the GPU
mappings.</p>
</li>
</ul>
<p>Additionally, it may be helpful to cross reference the
<a class="reference internal" href="#frontier-simple"><span class="std std-ref">simplified Frontier node diagram</span></a>  specifically the
<a class="reference internal" href="#frontier-lownoise"><span class="std std-ref">low-noise mode diagram</span></a>.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Unless specified otherwise, the examples below assume the default low-noise
core specialization setting (<code class="docutils literal notranslate"><span class="pre">-S</span> <span class="pre">8</span></code>).  This means that there are only 56
allocatable cores by default instead of 64.  See the <a class="reference internal" href="#frontier-lownoise"><span class="std std-ref">Low-noise Mode Layout</span></a>
section for more details.  Set <code class="docutils literal notranslate"><span class="pre">-S</span> <span class="pre">0</span></code> at job allocation to override this setting.</p>
</div>
<section id="cpu-mapping">
<span id="frontier-cpu-map"></span><h4>CPU Mapping<a class="headerlink" href="#cpu-mapping" title="Link to this heading"></a></h4>
<p>This subsection covers how to map tasks to the CPU without the presence of
additional threads (i.e., solely MPI tasks  no additional OpenMP threads).</p>
<p>The intent with both of the following examples is to launch 8 MPI ranks across
the node where each rank is assigned its own logical (and, in this case,
physical) core.  Using the <code class="docutils literal notranslate"><span class="pre">-m</span></code> distribution flag, we will cover two common
approaches to assign the MPI ranks  in a round-robin (<code class="docutils literal notranslate"><span class="pre">cyclic</span></code>)
configuration and in a packed (<code class="docutils literal notranslate"><span class="pre">block</span></code>) configuration. Slurms
<a class="reference internal" href="#frontier-interactive"><span class="std std-ref">Interactive Jobs</span></a> method was used to request an allocation of 1
compute node for these examples: <code class="docutils literal notranslate"><span class="pre">salloc</span> <span class="pre">-A</span> <span class="pre">&lt;project_id&gt;</span> <span class="pre">-t</span> <span class="pre">30</span> <span class="pre">-p</span> <span class="pre">&lt;parition&gt;</span>
<span class="pre">-N</span> <span class="pre">1</span></code></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>There are many different ways users might choose to perform these mappings,
so users are encouraged to clone the <code class="docutils literal notranslate"><span class="pre">hello_mpi_omp</span></code> program and test whether
or not processes and threads are running where intended.</p>
</div>
<section id="mpi-ranks-round-robin">
<h5>8 MPI Ranks (round-robin)<a class="headerlink" href="#mpi-ranks-round-robin" title="Link to this heading"></a></h5>
<p>Assigning MPI ranks in a round-robin (<code class="docutils literal notranslate"><span class="pre">cyclic</span></code>) manner across L3 cache
regions (sockets) is the default behavior on Frontier. This mode will assign
consecutive MPI tasks to different sockets before it tries to fill up a
socket.</p>
<p>Recall that the <code class="docutils literal notranslate"><span class="pre">-m</span></code> flag behaves like: <code class="docutils literal notranslate"><span class="pre">-m</span> <span class="pre">&lt;node</span> <span class="pre">distribution&gt;:&lt;socket</span>
<span class="pre">distribution&gt;</span></code>.  Hence, the key setting to achieving the round-robin nature is
the <code class="docutils literal notranslate"><span class="pre">-m</span> <span class="pre">block:cyclic</span></code> flag, specifically the <code class="docutils literal notranslate"><span class="pre">cyclic</span></code> setting provided for
the socket distribution. This ensures that the MPI tasks will be distributed
across sockets in a cyclic (round-robin) manner.</p>
<p>The below <code class="docutils literal notranslate"><span class="pre">srun</span></code> command will achieve the intended 8 MPI round-robin layout:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span><span class="nb">export</span><span class="w"> </span><span class="nv">OMP_NUM_THREADS</span><span class="o">=</span><span class="m">1</span>
$<span class="w"> </span>srun<span class="w"> </span>-N1<span class="w"> </span>-n8<span class="w"> </span>-c1<span class="w"> </span>--cpu-bind<span class="o">=</span>threads<span class="w"> </span>--threads-per-core<span class="o">=</span><span class="m">1</span><span class="w"> </span>-m<span class="w"> </span>block:cyclic<span class="w"> </span>./hello_mpi_omp<span class="w"> </span><span class="p">|</span><span class="w"> </span>sort

MPI<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier00144
MPI<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">009</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier00144
MPI<span class="w"> </span><span class="m">002</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">017</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier00144
MPI<span class="w"> </span><span class="m">003</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">025</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier00144
MPI<span class="w"> </span><span class="m">004</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">033</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier00144
MPI<span class="w"> </span><span class="m">005</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">041</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier00144
MPI<span class="w"> </span><span class="m">006</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">049</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier00144
MPI<span class="w"> </span><span class="m">007</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">057</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier00144
</pre></div>
</div>
<a class="reference internal image-reference" href="../_images/Frontier_Node_Diagram_Simple_lownoise_mpiRR.png"><img alt="../_images/Frontier_Node_Diagram_Simple_lownoise_mpiRR.png" class="align-center" src="../_images/Frontier_Node_Diagram_Simple_lownoise_mpiRR.png" style="width: 100%;" /></a>
<p>Breaking down the <code class="docutils literal notranslate"><span class="pre">srun</span></code> command, we have:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">-N1</span></code>: indicates we are using 1 node</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">-n8</span></code>: indicates we are launching 8 MPI tasks</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">-c1</span></code>: indicates we are assigning 1 logical core per MPI task.
In this case, because of <code class="docutils literal notranslate"><span class="pre">--threads-per-core=1</span></code>, this also means 1 <strong>physical</strong> core per MPI task.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--cpu-bind=threads</span></code>: binds tasks to threads</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--threads-per-core=1</span></code>: use a maximum of 1 hardware thread per physical core (i.e., only use 1 logical core per physical core)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">-m</span> <span class="pre">block:cyclic</span></code>: distribute the tasks in a block layout across nodes (default), and in a <strong>cyclic</strong> (round-robin) layout across L3 sockets</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">./hello_mpi_omp</span></code>: launches the hello_mpi_omp executable</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">|</span> <span class="pre">sort</span></code>: sorts the output</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the above command used the default settings <code class="docutils literal notranslate"><span class="pre">-c1</span></code>,
<code class="docutils literal notranslate"><span class="pre">--cpu-bind=threads</span></code>, <code class="docutils literal notranslate"><span class="pre">--threads-per-core=1</span></code> and <code class="docutils literal notranslate"><span class="pre">-m</span> <span class="pre">block:cyclic</span></code>, it is
always better to be explicit with your <code class="docutils literal notranslate"><span class="pre">srun</span></code> command to have more control
over your node layout. The above command is equivalent to <code class="docutils literal notranslate"><span class="pre">srun</span> <span class="pre">-N1</span> <span class="pre">-n8</span></code>.</p>
</div>
<p>As you can see in the node diagram above, this results in the 8 MPI tasks
(outlined in different colors) being distributed vertically across L3
sockets.</p>
</section>
<section id="mpi-ranks-packed">
<h5>7 MPI Ranks (packed)<a class="headerlink" href="#mpi-ranks-packed" title="Link to this heading"></a></h5>
<p>Instead, you can assign MPI ranks so that the L3 regions are filled in a
packed (<code class="docutils literal notranslate"><span class="pre">block</span></code>) manner.  This mode will assign consecutive MPI tasks to
the same L3 region (socket) until it is filled up or packed before
assigning a task to a different socket.</p>
<p>Recall that the <code class="docutils literal notranslate"><span class="pre">-m</span></code> flag behaves like: <code class="docutils literal notranslate"><span class="pre">-m</span> <span class="pre">&lt;node</span> <span class="pre">distribution&gt;:&lt;socket</span>
<span class="pre">distribution&gt;</span></code>.  Hence, the key setting to achieving the round-robin nature is
the <code class="docutils literal notranslate"><span class="pre">-m</span> <span class="pre">block:block</span></code> flag, specifically the <code class="docutils literal notranslate"><span class="pre">block</span></code> setting provided for
the socket distribution. This ensures that the MPI tasks will be distributed
in a packed manner.</p>
<p>The below <code class="docutils literal notranslate"><span class="pre">srun</span></code> command will achieve the intended 7 MPI packed layout:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span><span class="nb">export</span><span class="w"> </span><span class="nv">OMP_NUM_THREADS</span><span class="o">=</span><span class="m">1</span>
$<span class="w"> </span>srun<span class="w"> </span>-N1<span class="w"> </span>-n7<span class="w"> </span>-c1<span class="w"> </span>--cpu-bind<span class="o">=</span>threads<span class="w"> </span>--threads-per-core<span class="o">=</span><span class="m">1</span><span class="w"> </span>-m<span class="w"> </span>block:block<span class="w"> </span>./hello_mpi_omp<span class="w"> </span><span class="p">|</span><span class="w"> </span>sort

MPI<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier00144
MPI<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">002</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier00144
MPI<span class="w"> </span><span class="m">002</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">003</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier00144
MPI<span class="w"> </span><span class="m">003</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">004</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier00144
MPI<span class="w"> </span><span class="m">004</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">005</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier00144
MPI<span class="w"> </span><span class="m">005</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">006</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier00144
MPI<span class="w"> </span><span class="m">006</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">007</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier00144
</pre></div>
</div>
<a class="reference internal image-reference" href="../_images/Frontier_Node_Diagram_Simple_lownoise_mpiPacked.png"><img alt="../_images/Frontier_Node_Diagram_Simple_lownoise_mpiPacked.png" class="align-center" src="../_images/Frontier_Node_Diagram_Simple_lownoise_mpiPacked.png" style="width: 100%;" /></a>
<p>Breaking down the <code class="docutils literal notranslate"><span class="pre">srun</span></code> command, the only difference than the previous example is:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">-m</span> <span class="pre">block:block</span></code>: distribute the tasks in a block layout across nodes (default), and in a <strong>block</strong> (packed) socket layout</p></li>
</ul>
<p>As you can see in the node diagram above, this results in the 7 MPI tasks
(outlined in different colors) being distributed horizontally <em>within</em> a
socket, rather than being spread across different L3 sockets like with the
previous example. However, if an 8th task was requested it would be assigned
to the next L3 region on core 009.</p>
</section>
</section>
<section id="multithreading">
<span id="frontier-multi-map"></span><h4>Multithreading<a class="headerlink" href="#multithreading" title="Link to this heading"></a></h4>
<p>Because a Frontier compute node has two hardware threads available (2 logical
cores per physical core), this enables the possibility of multithreading your
application (e.g., with OpenMP threads). Although the additional hardware
threads can be assigned to additional MPI tasks, this is not recommended. It is
highly recommended to only use 1 MPI task per physical core and to use OpenMP
threads instead on any additional logical cores gained when using both hardware
threads.</p>
<p>The following examples cover multithreading with hybrid MPI+OpenMP
applications.  In these examples, Slurms <a class="reference internal" href="#frontier-interactive"><span class="std std-ref">Interactive Jobs</span></a> method
was used to request an allocation of 1 compute node:
<code class="docutils literal notranslate"><span class="pre">salloc</span> <span class="pre">-A</span> <span class="pre">&lt;project_id&gt;</span> <span class="pre">-t</span> <span class="pre">30</span> <span class="pre">-p</span> <span class="pre">&lt;parition&gt;</span> <span class="pre">-N</span> <span class="pre">1</span></code></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>There are many different ways users might choose to perform these mappings,
so users are encouraged to clone the <code class="docutils literal notranslate"><span class="pre">hello_mpi_omp</span></code> program and test whether
or not processes and threads are running where intended.</p>
</div>
<section id="mpi-ranks-each-with-2-openmp-threads">
<h5>2 MPI ranks - each with 2 OpenMP threads<a class="headerlink" href="#mpi-ranks-each-with-2-openmp-threads" title="Link to this heading"></a></h5>
<p>In this example, the intent is to launch 2 MPI ranks, each of which spawn 2
OpenMP threads, and have all of the 4 OpenMP threads run on different physical
CPU cores.</p>
<p><strong>First (INCORRECT) attempt</strong></p>
<p>To set the number of OpenMP threads spawned per MPI rank, the
<code class="docutils literal notranslate"><span class="pre">OMP_NUM_THREADS</span></code> environment variable can be used. To set the number of MPI
ranks launched, the <code class="docutils literal notranslate"><span class="pre">srun</span></code> flag <code class="docutils literal notranslate"><span class="pre">-n</span></code> can be used.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span><span class="nb">export</span><span class="w"> </span><span class="nv">OMP_NUM_THREADS</span><span class="o">=</span><span class="m">2</span>
$<span class="w"> </span>srun<span class="w"> </span>-N1<span class="w"> </span>-n2<span class="w"> </span>./hello_mpi_omp<span class="w"> </span><span class="p">|</span><span class="w"> </span>sort

WARNING:<span class="w"> </span>Requested<span class="w"> </span>total<span class="w"> </span>thread<span class="w"> </span>count<span class="w"> </span>and/or<span class="w"> </span>thread<span class="w"> </span>affinity<span class="w"> </span>may<span class="w"> </span>result<span class="w"> </span><span class="k">in</span>
oversubscription<span class="w"> </span>of<span class="w"> </span>available<span class="w"> </span>CPU<span class="w"> </span>resources!<span class="w">  </span>Performance<span class="w"> </span>may<span class="w"> </span>be<span class="w"> </span>degraded.
Explicitly<span class="w"> </span><span class="nb">set</span><span class="w"> </span><span class="nv">OMP_WAIT_POLICY</span><span class="o">=</span>PASSIVE<span class="w"> </span>or<span class="w"> </span>ACTIVE<span class="w"> </span>to<span class="w"> </span>suppress<span class="w"> </span>this<span class="w"> </span>message.
Set<span class="w"> </span><span class="nv">CRAY_OMP_CHECK_AFFINITY</span><span class="o">=</span>TRUE<span class="w"> </span>to<span class="w"> </span>print<span class="w"> </span>detailed<span class="w"> </span>thread-affinity<span class="w"> </span>messages.
WARNING:<span class="w"> </span>Requested<span class="w"> </span>total<span class="w"> </span>thread<span class="w"> </span>count<span class="w"> </span>and/or<span class="w"> </span>thread<span class="w"> </span>affinity<span class="w"> </span>may<span class="w"> </span>result<span class="w"> </span><span class="k">in</span>
oversubscription<span class="w"> </span>of<span class="w"> </span>available<span class="w"> </span>CPU<span class="w"> </span>resources!<span class="w">  </span>Performance<span class="w"> </span>may<span class="w"> </span>be<span class="w"> </span>degraded.
Explicitly<span class="w"> </span><span class="nb">set</span><span class="w"> </span><span class="nv">OMP_WAIT_POLICY</span><span class="o">=</span>PASSIVE<span class="w"> </span>or<span class="w"> </span>ACTIVE<span class="w"> </span>to<span class="w"> </span>suppress<span class="w"> </span>this<span class="w"> </span>message.
Set<span class="w"> </span><span class="nv">CRAY_OMP_CHECK_AFFINITY</span><span class="o">=</span>TRUE<span class="w"> </span>to<span class="w"> </span>print<span class="w"> </span>detailed<span class="w"> </span>thread-affinity<span class="w"> </span>messages.

MPI<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier001
MPI<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier001
MPI<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">009</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier001
MPI<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">009</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier001
</pre></div>
</div>
<p>The first thing to notice here is the <code class="docutils literal notranslate"><span class="pre">WARNING</span></code> about oversubscribing the
available CPU cores. Also, the output shows each MPI rank did spawn 2 OpenMP
threads, but both OpenMP threads ran on the same logical core (for a given
MPI rank). This was not the intended behavior; each OpenMP thread was meant to
run on its own physical CPU core.</p>
<p>The problem here arises from two default settings; 1) each MPI rank is only
allocated 1 logical core (<code class="docutils literal notranslate"><span class="pre">-c</span> <span class="pre">1</span></code>) and, 2) only 1 hardware thread per physical
CPU core is enabled (<code class="docutils literal notranslate"><span class="pre">--threads-per-core=1</span></code>).  When using
<code class="docutils literal notranslate"><span class="pre">--threads-per-core=1</span></code> and <code class="docutils literal notranslate"><span class="pre">--cpu-bind=threads</span></code> (the default setting), 1
logical core in <code class="docutils literal notranslate"><span class="pre">-c</span></code> is equivalent to 1 physical core.  So in this case, each
MPI rank only has 1 physical core (with 1 hardware thread) to run on -
including any threads the process spawns - hence the WARNING and undesired
behavior.</p>
<p><strong>Second (CORRECT) attempt</strong></p>
<p>Recall that in this scenario, because of the <code class="docutils literal notranslate"><span class="pre">--threads-per-core=1</span></code> setting,
1 logical core is equivalent to 1 physical core when using <code class="docutils literal notranslate"><span class="pre">-c</span></code>.  Therefore,
in order for each OpenMP thread to run on its own physical CPU core, each MPI
rank should be given 2 physical CPU cores (<code class="docutils literal notranslate"><span class="pre">-c</span> <span class="pre">2</span></code>).  Now the OpenMP threads
will be mapped to unique hardware threads on separate physical CPU cores.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span><span class="nb">export</span><span class="w"> </span><span class="nv">OMP_NUM_THREADS</span><span class="o">=</span><span class="m">2</span>
$<span class="w"> </span>srun<span class="w"> </span>-N1<span class="w"> </span>-n2<span class="w"> </span>-c2<span class="w"> </span>./hello_mpi_omp<span class="w"> </span><span class="p">|</span><span class="w"> </span>sort

MPI<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier001
MPI<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">002</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier001
MPI<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">009</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier001
MPI<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">010</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier001
</pre></div>
</div>
<p>Now the output shows that each OpenMP thread ran on its own physical CPU core.
More specifically (see the Frontier Compute Node diagram), OpenMP thread 000 of
MPI rank 000 ran on logical core 001 (i.e., physical CPU core 01), OpenMP
thread 001 of MPI rank 000 ran on logical core 002 (i.e., physical CPU core
02), OpenMP thread 000 of MPI rank 001 ran on logical core 009 (i.e., physical
CPU core 09), and OpenMP thread 001 of MPI rank 001 ran on logical core 010
(i.e., physical CPU core 10) - as intended.</p>
<p><strong>Third attempt - Using multiple threads per core</strong></p>
<p>To use both available hardware threads per core, the <em>job</em> must be allocated
with <code class="docutils literal notranslate"><span class="pre">--threads-per-core=2</span></code> (as opposed to only the job step - i.e., <code class="docutils literal notranslate"><span class="pre">srun</span></code>
command). That value will then be inherited by <code class="docutils literal notranslate"><span class="pre">srun</span></code> unless explcitly
overridden with <code class="docutils literal notranslate"><span class="pre">--threads-per-core=1</span></code>. Because we are using
<code class="docutils literal notranslate"><span class="pre">--threads-per-core=2</span></code>, the usage of <code class="docutils literal notranslate"><span class="pre">-c</span></code> goes back to purely meaning the
amount of <strong>logical</strong> cores (i.e., it is no longer equivalent to 1 physical core).</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>salloc<span class="w"> </span>-N1<span class="w"> </span>-A<span class="w"> </span>&lt;project_id&gt;<span class="w"> </span>-t<span class="w"> </span>&lt;time&gt;<span class="w"> </span>-p<span class="w"> </span>&lt;partition&gt;<span class="w"> </span>--threads-per-core<span class="o">=</span><span class="m">2</span>

$<span class="w"> </span><span class="nb">export</span><span class="w"> </span><span class="nv">OMP_NUM_THREADS</span><span class="o">=</span><span class="m">2</span>
$<span class="w"> </span>srun<span class="w"> </span>-N1<span class="w"> </span>-n2<span class="w"> </span>-c2<span class="w"> </span>./hello_mpi_omp<span class="w"> </span><span class="p">|</span><span class="w"> </span>sort

MPI<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier001
MPI<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">065</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier001
MPI<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">009</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier001
MPI<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">073</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier001
</pre></div>
</div>
<p>Comparing this output to the Frontier Compute Node diagram, we see that each
pair of OpenMP threads is contained within a single physical core. MPI rank 000
ran on logical cores 001 and 065 (i.e. physical CPU core 01) and MPI rank
001 ran on logical cores 009 and 073 (i.e. physical CPU core 09).</p>
</section>
</section>
<section id="gpu-mapping">
<span id="frontier-gpu-map"></span><h4>GPU Mapping<a class="headerlink" href="#gpu-mapping" title="Link to this heading"></a></h4>
<p>In this sub-section, an MPI+OpenMP+HIP Hello, World program (<a class="reference external" href="https://code.ornl.gov/olcf/hello_jobstep" target="_blank">hello_jobstep</a>) will be used to show how to make
only specific GPUs available to processes - which we will refer to as GPU
mapping. Again, Slurms <a class="reference internal" href="#frontier-interactive"><span class="std std-ref">Interactive Jobs</span></a> method was used to request
an allocation of 2 compute nodes for these examples: <code class="docutils literal notranslate"><span class="pre">salloc</span> <span class="pre">-A</span> <span class="pre">&lt;project_id&gt;</span>
<span class="pre">-t</span> <span class="pre">30</span> <span class="pre">-p</span> <span class="pre">&lt;parition&gt;</span> <span class="pre">-N</span> <span class="pre">2</span></code>. The CPU mapping part of this example is very
similar to the example used above in the Multithreading sub-section, so the
focus here will be on the GPU mapping part.</p>
<p>In general, GPU mapping can be accomplished in different ways. For example, an
application might map GPUs to MPI ranks programmatically within the code using,
say, <code class="docutils literal notranslate"><span class="pre">hipSetDevice</span></code>. In this case, there might not be a need to map GPUs using
Slurm (since it can be done in the code itself). However, many applications
expect only 1 GPU to be available to each rank. It is this latter case that the
following examples refer to.</p>
<p>Also, recall that the CPU cores in a given L3 cache region are connected to a
specific GPU (see the <a class="reference external" href="https://docs.olcf.ornl.gov/_images/Frontier_Node_Diagram.jpg" target="_blank">Frontier Node Diagram</a> and subsequent
<a class="reference internal" href="#numa-note"><span class="std std-ref">Note on NUMA domains</span></a> for more information). In the examples
below, knowledge of these details will be assumed.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>There are many different ways users might choose to perform these mappings,
so users are encouraged to clone the <code class="docutils literal notranslate"><span class="pre">hello_jobstep</span></code> program and test whether
processes and threads are mapped to the CPU cores and GPUs as intended..</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Due to the unique architecture of Frontier compute nodes and the way that
Slurm currently allocates GPUs and CPU cores to job steps, it is suggested that
all 8 GPUs on a node are allocated to the job step to ensure that optimal
bindings are possible.</p>
</div>
<section id="hello-jobstep-output">
<h5><code class="docutils literal notranslate"><span class="pre">hello_jobstep</span></code> output<a class="headerlink" href="#hello-jobstep-output" title="Link to this heading"></a></h5>
<p>Before jumping into the examples, it is helpful to understand the output from the <code class="docutils literal notranslate"><span class="pre">hello_jobstep</span></code> program:</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>ID</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">MPI</span></code></p></td>
<td><p>MPI rank ID</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">OMP</span></code></p></td>
<td><p>OpenMP thread ID</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">HWT</span></code></p></td>
<td><p>CPU hardware thread the MPI rank or OpenMP thread ran on</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">Node</span></code></p></td>
<td><p>Compute node the MPI rank or OpenMP thread ran on</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">GPU_ID</span></code></p></td>
<td><div class="line-block">
<div class="line">GPU ID the MPI rank or OpenMP thread had access to</div>
<div class="line">(This is the node-level, or global, GPU ID as shown in the Frontier node diagram)</div>
<div class="line">NOTE: This is read from <code class="docutils literal notranslate"><span class="pre">ROCR_VISIBLE_DEVICES</span></code>. If this variable is not set, the value of
<code class="docutils literal notranslate"><span class="pre">GPU_ID</span></code> will be set to <code class="docutils literal notranslate"><span class="pre">N/A</span></code> by the program</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">RT_GPU_ID</span></code></p></td>
<td><div class="line-block">
<div class="line">The runtime GPU ID</div>
<div class="line">(This is the GPU ID as seen from the HIP runtime - e.g., as reported by <code class="docutils literal notranslate"><span class="pre">hipGetDevice</span></code>)</div>
<div class="line">NOTE: The HIP runtime relabels the GPUs each rank can access starting at <cite>0</cite></div>
</div>
</td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">Bus_ID</span></code></p></td>
<td><div class="line-block">
<div class="line">The physical Bus ID associated with a GPU</div>
<div class="line">(The Bus ID can be used to e.g., confirm unique GPUs are being used)</div>
</div>
</td>
</tr>
</tbody>
</table>
</section>
<section id="mapping-1-gpu-per-task">
<h5>Mapping 1 GPU per task<a class="headerlink" href="#mapping-1-gpu-per-task" title="Link to this heading"></a></h5>
<p>In the following examples, 1 GPU will be mapped to each MPI rank (and any OpenMP threads it might spawn). The relevant <code class="docutils literal notranslate"><span class="pre">srun</span></code> options for GPU mapping used in these examples are:</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Slurm Option</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--gpus-per-task</span></code></p></td>
<td><p>Specify the number of GPUs required for the job on each task.
This option requires an explicit task count, e.g. -n</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">--gpu-bind=closest</span></code></p></td>
<td><p>Bind  each  task  to  the GPU(s) which are closest.
Here, closest refers to the GPU connected to the L3 where the MPI rank is mapped to.</p></td>
</tr>
</tbody>
</table>
<p><strong>Example 1: 8 MPI ranks - each with 7 CPU cores and 1 GPU (single-node)</strong></p>
<p>The most common use case for running on Frontier is to run with 8 MPI ranks per node, where each rank has access to 7 physical CPU cores and 1 GPU (recall it is 7 CPU cores here instead of 8 due to core specialization: see <a class="reference internal" href="#frontier-lownoise"><span class="std std-ref">low-noise mode diagram</span></a>). The MPI rank can use the 7 CPU cores to e.g., spawn OpenMP threads on (if OpenMP CPU threading is available in the application). Here is an example of such a job step on a single node:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span><span class="nv">OMP_NUM_THREADS</span><span class="o">=</span><span class="m">7</span><span class="w"> </span>srun<span class="w"> </span>-N1<span class="w"> </span>-n8<span class="w"> </span>-c7<span class="w"> </span>--gpus-per-task<span class="o">=</span><span class="m">1</span><span class="w"> </span>--gpu-bind<span class="o">=</span>closest<span class="w"> </span>./hello_jobstep<span class="w"> </span><span class="p">|</span><span class="w"> </span>sort
MPI<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier00256<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">4</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d1
MPI<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">002</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier00256<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">4</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d1
MPI<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">002</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">003</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier00256<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">4</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d1
MPI<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">003</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">004</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier00256<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">4</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d1
MPI<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">004</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">005</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier00256<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">4</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d1
MPI<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">005</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">006</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier00256<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">4</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d1
MPI<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">006</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">007</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier00256<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">4</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d1
MPI<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">009</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier00256<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">5</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d6
MPI<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">010</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier00256<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">5</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d6
MPI<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">002</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">011</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier00256<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">5</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d6
MPI<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">003</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">012</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier00256<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">5</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d6
MPI<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">004</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">013</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier00256<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">5</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d6
MPI<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">005</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">014</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier00256<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">5</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d6
MPI<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">006</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">015</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier00256<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">5</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d6
MPI<span class="w"> </span><span class="m">002</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">017</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier00256<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">2</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c9
MPI<span class="w"> </span><span class="m">002</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">018</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier00256<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">2</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c9
MPI<span class="w"> </span><span class="m">002</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">002</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">019</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier00256<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">2</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c9
MPI<span class="w"> </span><span class="m">002</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">003</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">020</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier00256<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">2</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c9
MPI<span class="w"> </span><span class="m">002</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">004</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">021</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier00256<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">2</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c9
MPI<span class="w"> </span><span class="m">002</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">005</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">022</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier00256<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">2</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c9
MPI<span class="w"> </span><span class="m">002</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">006</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">023</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier00256<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">2</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c9
MPI<span class="w"> </span><span class="m">003</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">025</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier00256<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">3</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>ce
MPI<span class="w"> </span><span class="m">003</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">026</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier00256<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">3</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>ce
MPI<span class="w"> </span><span class="m">003</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">002</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">027</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier00256<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">3</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>ce
MPI<span class="w"> </span><span class="m">003</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">003</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">028</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier00256<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">3</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>ce
MPI<span class="w"> </span><span class="m">003</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">004</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">029</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier00256<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">3</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>ce
MPI<span class="w"> </span><span class="m">003</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">005</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">030</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier00256<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">3</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>ce
MPI<span class="w"> </span><span class="m">003</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">006</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">031</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier00256<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">3</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>ce
MPI<span class="w"> </span><span class="m">004</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">033</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier00256<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">6</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d9
MPI<span class="w"> </span><span class="m">004</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">034</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier00256<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">6</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d9
MPI<span class="w"> </span><span class="m">004</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">002</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">035</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier00256<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">6</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d9
MPI<span class="w"> </span><span class="m">004</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">003</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">036</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier00256<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">6</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d9
MPI<span class="w"> </span><span class="m">004</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">004</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">037</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier00256<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">6</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d9
MPI<span class="w"> </span><span class="m">004</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">005</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">038</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier00256<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">6</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d9
MPI<span class="w"> </span><span class="m">004</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">006</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">039</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier00256<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">6</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d9
MPI<span class="w"> </span><span class="m">005</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">041</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier00256<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">7</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>de
MPI<span class="w"> </span><span class="m">005</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">042</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier00256<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">7</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>de
MPI<span class="w"> </span><span class="m">005</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">002</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">043</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier00256<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">7</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>de
MPI<span class="w"> </span><span class="m">005</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">003</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">044</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier00256<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">7</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>de
MPI<span class="w"> </span><span class="m">005</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">004</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">045</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier00256<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">7</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>de
MPI<span class="w"> </span><span class="m">005</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">005</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">046</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier00256<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">7</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>de
MPI<span class="w"> </span><span class="m">005</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">006</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">047</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier00256<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">7</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>de
MPI<span class="w"> </span><span class="m">006</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">049</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier00256<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c1
MPI<span class="w"> </span><span class="m">006</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">050</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier00256<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c1
MPI<span class="w"> </span><span class="m">006</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">002</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">051</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier00256<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c1
MPI<span class="w"> </span><span class="m">006</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">003</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">052</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier00256<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c1
MPI<span class="w"> </span><span class="m">006</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">004</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">053</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier00256<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c1
MPI<span class="w"> </span><span class="m">006</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">005</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">054</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier00256<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c1
MPI<span class="w"> </span><span class="m">006</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">006</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">055</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier00256<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c1
MPI<span class="w"> </span><span class="m">007</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">057</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier00256<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">1</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c6
MPI<span class="w"> </span><span class="m">007</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">058</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier00256<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">1</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c6
MPI<span class="w"> </span><span class="m">007</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">002</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">059</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier00256<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">1</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c6
MPI<span class="w"> </span><span class="m">007</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">003</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">060</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier00256<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">1</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c6
MPI<span class="w"> </span><span class="m">007</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">004</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">061</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier00256<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">1</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c6
MPI<span class="w"> </span><span class="m">007</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">005</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">062</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier00256<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">1</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c6
MPI<span class="w"> </span><span class="m">007</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">006</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">063</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier00256<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">1</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c6
</pre></div>
</div>
<p>As has been pointed out previously in the Frontier documentation, notice that
GPUs are NOT mapped to MPI ranks in sequential order (e.g., MPI rank 0 is
mapped to physical CPU cores 1-7 and GPU 4, MPI rank 1 is mapped to physical
CPU cores 9-15 and GPU 5), but this IS expected behavior. It is simply a
consequence of the Frontier node architectures as shown in the <a class="reference external" href="https://docs.olcf.ornl.gov/_images/Frontier_Node_Diagram.jpg" target="_blank">Frontier Node
Diagram</a> and
subsequent <a class="reference internal" href="#numa-note"><span class="std std-ref">Note on NUMA domains</span></a>.</p>
<p><strong>Example 2: 1 MPI rank with 7 CPU cores and 1 GPU (single-node)</strong></p>
<p>When new users first attempt to run their application on Frontier, they often
want to test with 1 MPI rank that has access to 7 CPU cores and 1 GPU. Although
the job step used here is very similar to Example 1, the behavior is different:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span><span class="nv">OMP_NUM_THREADS</span><span class="o">=</span><span class="m">7</span><span class="w"> </span>srun<span class="w"> </span>-N1<span class="w"> </span>-n1<span class="w"> </span>-c7<span class="w"> </span>--gpus-per-task<span class="o">=</span><span class="m">1</span><span class="w"> </span>--gpu-bind<span class="o">=</span>closest<span class="w"> </span>./hello_jobstep<span class="w"> </span><span class="p">|</span><span class="w"> </span>sort
MPI<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">049</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04086<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c1
MPI<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">050</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04086<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c1
MPI<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">002</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">051</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04086<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c1
MPI<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">003</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">052</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04086<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c1
MPI<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">004</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">053</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04086<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c1
MPI<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">005</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">054</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04086<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c1
MPI<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">006</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">055</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04086<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c1
</pre></div>
</div>
<p>Notice that our MPI rank did not get mapped to CPU cores 1-7 and GPU 4, but
instead to GPU 0 and CPU cores 49-55. The apparent reason for this can be found
in the <code class="docutils literal notranslate"><span class="pre">--gpu-bind</span></code> section in the <code class="docutils literal notranslate"><span class="pre">srun</span></code> man page: <code class="docutils literal notranslate"><span class="pre">GPU</span> <span class="pre">binding</span> <span class="pre">is</span>
<span class="pre">ignored</span> <span class="pre">if</span> <span class="pre">there</span> <span class="pre">is</span> <span class="pre">only</span> <span class="pre">one</span> <span class="pre">task.</span></code>. Here, Slurm appears to give the first GPU
it sees and maps it to the CPU cores that are closest. So although the mapping
doesnt occur as expected, the rank is still mapped to the correct GPU given
the CPU cores it ran on.</p>
<p><strong>Example 3: 16 MPI ranks - each with 7 CPU cores and 1 GPU (multi-node)</strong></p>
<p>This example simply extends Example 1 to run on 2 nodes, which simply requires
changing the number of nodes to 2 (<code class="docutils literal notranslate"><span class="pre">-N2</span></code>) and the number of MPI ranks to 16
(<code class="docutils literal notranslate"><span class="pre">-n16</span></code>).</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span><span class="nv">OMP_NUM_THREADS</span><span class="o">=</span><span class="m">7</span><span class="w"> </span>srun<span class="w"> </span>-N2<span class="w"> </span>-n16<span class="w"> </span>-c7<span class="w"> </span>--gpus-per-task<span class="o">=</span><span class="m">1</span><span class="w"> </span>--gpu-bind<span class="o">=</span>closest<span class="w"> </span>./hello_jobstep<span class="w"> </span><span class="p">|</span><span class="w"> </span>sort
MPI<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04086<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">4</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d1
MPI<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">002</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04086<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">4</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d1
MPI<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">002</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">003</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04086<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">4</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d1
MPI<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">003</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">004</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04086<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">4</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d1
MPI<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">004</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">005</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04086<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">4</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d1
MPI<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">005</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">006</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04086<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">4</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d1
MPI<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">006</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">007</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04086<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">4</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d1
MPI<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">009</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04086<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">5</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d6
MPI<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">010</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04086<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">5</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d6
MPI<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">002</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">011</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04086<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">5</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d6
MPI<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">003</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">012</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04086<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">5</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d6
MPI<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">004</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">013</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04086<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">5</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d6
MPI<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">005</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">014</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04086<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">5</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d6
MPI<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">006</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">015</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04086<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">5</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d6
MPI<span class="w"> </span><span class="m">002</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">017</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04086<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">2</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c9
MPI<span class="w"> </span><span class="m">002</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">018</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04086<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">2</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c9
MPI<span class="w"> </span><span class="m">002</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">002</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">019</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04086<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">2</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c9
MPI<span class="w"> </span><span class="m">002</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">003</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">020</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04086<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">2</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c9
MPI<span class="w"> </span><span class="m">002</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">004</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">021</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04086<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">2</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c9
MPI<span class="w"> </span><span class="m">002</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">005</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">022</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04086<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">2</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c9
MPI<span class="w"> </span><span class="m">002</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">006</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">023</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04086<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">2</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c9
MPI<span class="w"> </span><span class="m">003</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">025</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04086<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">3</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>ce
MPI<span class="w"> </span><span class="m">003</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">026</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04086<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">3</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>ce
MPI<span class="w"> </span><span class="m">003</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">002</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">027</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04086<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">3</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>ce
MPI<span class="w"> </span><span class="m">003</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">003</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">028</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04086<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">3</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>ce
MPI<span class="w"> </span><span class="m">003</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">004</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">029</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04086<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">3</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>ce
MPI<span class="w"> </span><span class="m">003</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">005</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">030</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04086<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">3</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>ce
MPI<span class="w"> </span><span class="m">003</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">006</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">031</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04086<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">3</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>ce
MPI<span class="w"> </span><span class="m">004</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">033</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04086<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">6</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d9
MPI<span class="w"> </span><span class="m">004</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">034</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04086<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">6</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d9
MPI<span class="w"> </span><span class="m">004</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">002</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">035</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04086<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">6</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d9
MPI<span class="w"> </span><span class="m">004</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">003</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">036</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04086<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">6</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d9
MPI<span class="w"> </span><span class="m">004</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">004</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">037</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04086<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">6</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d9
MPI<span class="w"> </span><span class="m">004</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">005</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">038</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04086<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">6</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d9
MPI<span class="w"> </span><span class="m">004</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">006</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">039</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04086<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">6</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d9
MPI<span class="w"> </span><span class="m">005</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">041</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04086<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">7</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>de
MPI<span class="w"> </span><span class="m">005</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">042</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04086<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">7</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>de
MPI<span class="w"> </span><span class="m">005</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">002</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">043</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04086<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">7</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>de
MPI<span class="w"> </span><span class="m">005</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">003</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">044</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04086<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">7</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>de
MPI<span class="w"> </span><span class="m">005</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">004</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">045</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04086<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">7</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>de
MPI<span class="w"> </span><span class="m">005</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">005</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">046</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04086<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">7</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>de
MPI<span class="w"> </span><span class="m">005</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">006</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">047</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04086<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">7</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>de
MPI<span class="w"> </span><span class="m">006</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">049</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04086<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c1
MPI<span class="w"> </span><span class="m">006</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">050</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04086<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c1
MPI<span class="w"> </span><span class="m">006</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">002</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">051</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04086<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c1
MPI<span class="w"> </span><span class="m">006</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">003</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">052</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04086<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c1
MPI<span class="w"> </span><span class="m">006</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">004</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">053</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04086<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c1
MPI<span class="w"> </span><span class="m">006</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">005</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">054</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04086<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c1
MPI<span class="w"> </span><span class="m">006</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">006</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">055</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04086<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c1
MPI<span class="w"> </span><span class="m">007</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">057</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04086<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">1</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c6
MPI<span class="w"> </span><span class="m">007</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">058</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04086<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">1</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c6
MPI<span class="w"> </span><span class="m">007</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">002</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">059</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04086<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">1</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c6
MPI<span class="w"> </span><span class="m">007</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">003</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">060</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04086<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">1</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c6
MPI<span class="w"> </span><span class="m">007</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">004</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">061</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04086<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">1</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c6
MPI<span class="w"> </span><span class="m">007</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">005</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">062</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04086<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">1</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c6
MPI<span class="w"> </span><span class="m">007</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">006</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">063</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04086<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">1</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c6
MPI<span class="w"> </span><span class="m">008</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04087<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">4</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d1
MPI<span class="w"> </span><span class="m">008</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">002</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04087<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">4</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d1
MPI<span class="w"> </span><span class="m">008</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">002</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">003</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04087<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">4</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d1
MPI<span class="w"> </span><span class="m">008</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">003</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">004</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04087<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">4</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d1
MPI<span class="w"> </span><span class="m">008</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">004</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">005</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04087<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">4</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d1
MPI<span class="w"> </span><span class="m">008</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">005</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">006</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04087<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">4</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d1
MPI<span class="w"> </span><span class="m">008</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">006</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">007</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04087<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">4</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d1
MPI<span class="w"> </span><span class="m">009</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">009</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04087<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">5</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d6
MPI<span class="w"> </span><span class="m">009</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">010</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04087<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">5</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d6
MPI<span class="w"> </span><span class="m">009</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">002</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">011</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04087<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">5</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d6
MPI<span class="w"> </span><span class="m">009</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">003</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">012</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04087<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">5</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d6
MPI<span class="w"> </span><span class="m">009</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">004</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">013</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04087<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">5</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d6
MPI<span class="w"> </span><span class="m">009</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">005</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">014</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04087<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">5</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d6
MPI<span class="w"> </span><span class="m">009</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">006</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">015</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04087<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">5</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d6
MPI<span class="w"> </span><span class="m">010</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">017</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04087<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">2</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c9
MPI<span class="w"> </span><span class="m">010</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">018</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04087<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">2</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c9
MPI<span class="w"> </span><span class="m">010</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">002</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">019</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04087<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">2</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c9
MPI<span class="w"> </span><span class="m">010</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">003</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">020</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04087<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">2</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c9
MPI<span class="w"> </span><span class="m">010</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">004</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">021</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04087<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">2</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c9
MPI<span class="w"> </span><span class="m">010</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">005</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">022</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04087<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">2</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c9
MPI<span class="w"> </span><span class="m">010</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">006</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">023</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04087<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">2</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c9
MPI<span class="w"> </span><span class="m">011</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">025</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04087<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">3</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>ce
MPI<span class="w"> </span><span class="m">011</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">026</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04087<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">3</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>ce
MPI<span class="w"> </span><span class="m">011</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">002</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">027</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04087<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">3</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>ce
MPI<span class="w"> </span><span class="m">011</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">003</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">028</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04087<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">3</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>ce
MPI<span class="w"> </span><span class="m">011</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">004</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">029</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04087<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">3</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>ce
MPI<span class="w"> </span><span class="m">011</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">005</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">030</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04087<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">3</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>ce
MPI<span class="w"> </span><span class="m">011</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">006</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">031</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04087<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">3</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>ce
MPI<span class="w"> </span><span class="m">012</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">033</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04087<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">6</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d9
MPI<span class="w"> </span><span class="m">012</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">034</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04087<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">6</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d9
MPI<span class="w"> </span><span class="m">012</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">002</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">035</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04087<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">6</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d9
MPI<span class="w"> </span><span class="m">012</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">003</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">036</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04087<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">6</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d9
MPI<span class="w"> </span><span class="m">012</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">004</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">037</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04087<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">6</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d9
MPI<span class="w"> </span><span class="m">012</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">005</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">038</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04087<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">6</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d9
MPI<span class="w"> </span><span class="m">012</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">006</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">039</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04087<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">6</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d9
MPI<span class="w"> </span><span class="m">013</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">041</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04087<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">7</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>de
MPI<span class="w"> </span><span class="m">013</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">042</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04087<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">7</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>de
MPI<span class="w"> </span><span class="m">013</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">002</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">043</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04087<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">7</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>de
MPI<span class="w"> </span><span class="m">013</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">003</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">044</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04087<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">7</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>de
MPI<span class="w"> </span><span class="m">013</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">004</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">045</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04087<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">7</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>de
MPI<span class="w"> </span><span class="m">013</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">005</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">046</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04087<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">7</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>de
MPI<span class="w"> </span><span class="m">013</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">006</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">047</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04087<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">7</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>de
MPI<span class="w"> </span><span class="m">014</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">049</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04087<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c1
MPI<span class="w"> </span><span class="m">014</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">050</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04087<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c1
MPI<span class="w"> </span><span class="m">014</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">002</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">051</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04087<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c1
MPI<span class="w"> </span><span class="m">014</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">003</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">052</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04087<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c1
MPI<span class="w"> </span><span class="m">014</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">004</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">053</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04087<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c1
MPI<span class="w"> </span><span class="m">014</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">005</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">054</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04087<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c1
MPI<span class="w"> </span><span class="m">014</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">006</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">055</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04087<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c1
MPI<span class="w"> </span><span class="m">015</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">057</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04087<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">1</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c6
MPI<span class="w"> </span><span class="m">015</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">058</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04087<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">1</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c6
MPI<span class="w"> </span><span class="m">015</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">002</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">059</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04087<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">1</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c6
MPI<span class="w"> </span><span class="m">015</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">003</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">060</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04087<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">1</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c6
MPI<span class="w"> </span><span class="m">015</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">004</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">061</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04087<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">1</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c6
MPI<span class="w"> </span><span class="m">015</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">005</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">062</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04087<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">1</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c6
MPI<span class="w"> </span><span class="m">015</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">006</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">063</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04087<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">1</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c6
</pre></div>
</div>
</section>
<section id="mapping-multiple-mpi-ranks-to-a-single-gpu">
<h5>Mapping multiple MPI ranks to a single GPU<a class="headerlink" href="#mapping-multiple-mpi-ranks-to-a-single-gpu" title="Link to this heading"></a></h5>
<p>In the following examples, 2 MPI ranks will be mapped to 1 GPU. For brevity,
<code class="docutils literal notranslate"><span class="pre">OMP_NUM_THREADS</span></code> will be set to <code class="docutils literal notranslate"><span class="pre">1</span></code>, so <code class="docutils literal notranslate"><span class="pre">-c1</span></code> will be used unless
otherwise specified. A new <code class="docutils literal notranslate"><span class="pre">srun</span></code> option will also be introduced to
accomplish the new mapping:</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Slurm Option</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--ntasks-per-gpu</span></code></p></td>
<td><p>Specifies the number of MPI ranks that will share access to a GPU.</p></td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>On AMDs MI250X, multi-process service (MPS) is not needed since multiple
MPI ranks per GPU is supported natively.</p>
</div>
<p><strong>Example 4: 16 MPI ranks - where 2 ranks share a GPU (round-robin, single-node)</strong></p>
<p>This example launches 16 MPI ranks (<code class="docutils literal notranslate"><span class="pre">-n16</span></code>), each with 1 physical CPU core
(<code class="docutils literal notranslate"><span class="pre">-c1</span></code>) to launch 1 OpenMP thread (<code class="docutils literal notranslate"><span class="pre">OMP_NUM_THREADS=1</span></code>) on. The MPI ranks
will be assigned to GPUs in a round-robin fashion so that each of the 8 GPUs on
the node are shared by 2 MPI ranks.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span><span class="nv">OMP_NUM_THREADS</span><span class="o">=</span><span class="m">1</span><span class="w"> </span>srun<span class="w"> </span>-N1<span class="w"> </span>-n16<span class="w"> </span>-c1<span class="w"> </span>--ntasks-per-gpu<span class="o">=</span><span class="m">2</span><span class="w"> </span>--gpu-bind<span class="o">=</span>closest<span class="w"> </span>./hello_jobstep<span class="w"> </span><span class="p">|</span><span class="w"> </span>sort
MPI<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier08413<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">4</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d1
MPI<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">009</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier08413<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">5</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d6
MPI<span class="w"> </span><span class="m">002</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">017</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier08413<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">2</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c9
MPI<span class="w"> </span><span class="m">003</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">025</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier08413<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">3</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>ce
MPI<span class="w"> </span><span class="m">004</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">033</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier08413<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">6</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d9
MPI<span class="w"> </span><span class="m">005</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">041</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier08413<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">7</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>de
MPI<span class="w"> </span><span class="m">006</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">049</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier08413<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c1
MPI<span class="w"> </span><span class="m">007</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">057</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier08413<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">1</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c6
MPI<span class="w"> </span><span class="m">008</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">002</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier08413<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">4</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d1
MPI<span class="w"> </span><span class="m">009</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">010</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier08413<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">5</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d6
MPI<span class="w"> </span><span class="m">010</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">018</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier08413<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">2</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c9
MPI<span class="w"> </span><span class="m">011</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">026</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier08413<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">3</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>ce
MPI<span class="w"> </span><span class="m">012</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">034</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier08413<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">6</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d9
MPI<span class="w"> </span><span class="m">013</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">042</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier08413<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">7</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>de
MPI<span class="w"> </span><span class="m">014</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">050</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier08413<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c1
MPI<span class="w"> </span><span class="m">015</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">058</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier08413<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">1</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c6
</pre></div>
</div>
<p>The output shows the round-robin (<code class="docutils literal notranslate"><span class="pre">cyclic</span></code>) distribution of MPI ranks to
GPUs. In fact, it is a round-robin distribution of MPI ranks <em>to L3 cache
regions</em> (the default distribution). The GPU mapping is a consequence of where
the MPI ranks are distributed; <code class="docutils literal notranslate"><span class="pre">--gpu-bind=closest</span></code> simply maps the GPU in an
L3 cache region to the MPI ranks in the same L3 region.</p>
<p><strong>Example 5: 32 MPI ranks - where 2 ranks share a GPU (round-robin, multi-node)</strong></p>
<p>This example is an extension of Example 4 to run on 2 nodes.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span><span class="nv">OMP_NUM_THREADS</span><span class="o">=</span><span class="m">1</span><span class="w"> </span>srun<span class="w"> </span>-N2<span class="w"> </span>-n32<span class="w"> </span>-c1<span class="w"> </span>--ntasks-per-gpu<span class="o">=</span><span class="m">2</span><span class="w"> </span>--gpu-bind<span class="o">=</span>closest<span class="w"> </span>./hello_jobstep<span class="w"> </span><span class="p">|</span><span class="w"> </span>sort
MPI<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04975<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">4</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d1
MPI<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">009</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04975<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">5</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d6
MPI<span class="w"> </span><span class="m">002</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">017</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04975<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">2</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c9
MPI<span class="w"> </span><span class="m">003</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">025</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04975<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">3</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>ce
MPI<span class="w"> </span><span class="m">004</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">033</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04975<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">6</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d9
MPI<span class="w"> </span><span class="m">005</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">041</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04975<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">7</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>de
MPI<span class="w"> </span><span class="m">006</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">049</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04975<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c1
MPI<span class="w"> </span><span class="m">007</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">057</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04975<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">1</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c6
MPI<span class="w"> </span><span class="m">008</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">002</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04975<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">4</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d1
MPI<span class="w"> </span><span class="m">009</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">010</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04975<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">5</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d6
MPI<span class="w"> </span><span class="m">010</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">018</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04975<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">2</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c9
MPI<span class="w"> </span><span class="m">011</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">026</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04975<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">3</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>ce
MPI<span class="w"> </span><span class="m">012</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">034</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04975<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">6</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d9
MPI<span class="w"> </span><span class="m">013</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">042</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04975<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">7</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>de
MPI<span class="w"> </span><span class="m">014</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">050</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04975<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c1
MPI<span class="w"> </span><span class="m">015</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">058</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04975<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">1</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c6
MPI<span class="w"> </span><span class="m">016</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04976<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">4</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d1
MPI<span class="w"> </span><span class="m">017</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">009</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04976<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">5</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d6
MPI<span class="w"> </span><span class="m">018</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">017</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04976<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">2</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c9
MPI<span class="w"> </span><span class="m">019</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">025</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04976<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">3</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>ce
MPI<span class="w"> </span><span class="m">020</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">033</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04976<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">6</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d9
MPI<span class="w"> </span><span class="m">021</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">041</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04976<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">7</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>de
MPI<span class="w"> </span><span class="m">022</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">049</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04976<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c1
MPI<span class="w"> </span><span class="m">023</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">057</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04976<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">1</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c6
MPI<span class="w"> </span><span class="m">024</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">002</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04976<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">4</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d1
MPI<span class="w"> </span><span class="m">025</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">010</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04976<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">5</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d6
MPI<span class="w"> </span><span class="m">026</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">018</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04976<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">2</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c9
MPI<span class="w"> </span><span class="m">027</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">026</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04976<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">3</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>ce
MPI<span class="w"> </span><span class="m">028</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">034</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04976<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">6</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d9
MPI<span class="w"> </span><span class="m">029</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">042</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04976<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">7</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>de
MPI<span class="w"> </span><span class="m">030</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">050</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04976<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c1
MPI<span class="w"> </span><span class="m">031</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">058</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04976<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">1</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c6
</pre></div>
</div>
<p><strong>Example 6: 16 MPI ranks - where 2 ranks share a GPU (packed, single-node)</strong></p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This example assumes the use of a core specialization of <code class="docutils literal notranslate"><span class="pre">-S</span> <span class="pre">0</span></code>.  Because
Frontiers default core specialization (<code class="docutils literal notranslate"><span class="pre">-S</span> <span class="pre">8</span></code>) reserves the first core in
each L3 region, the packed mode can be problematic because the 7 cores
available in each L3 region wont necessarily divide evenly. This can lead to
tasks potentially spanning multiple L3 regions with its assigned cores, which
creates problems when Slurm tries to assign GPUs to a given task.</p>
</div>
<p>This example launches 16 MPI ranks (<code class="docutils literal notranslate"><span class="pre">-n16</span></code>), each with 4 physical CPU cores
(<code class="docutils literal notranslate"><span class="pre">-c4</span></code>) to launch 1 OpenMP thread (<code class="docutils literal notranslate"><span class="pre">OMP_NUM_THREADS=1</span></code>) on. The MPI ranks
will be assigned to GPUs in a packed fashion so that each of the 8 GPUs on the
node are shared by 2 MPI ranks. Similar to Example 4, <code class="docutils literal notranslate"><span class="pre">-ntasks-per-gpu=2</span></code>
will be used, but a new <code class="docutils literal notranslate"><span class="pre">srun</span></code> flag will be used to change the default
round-robin (<code class="docutils literal notranslate"><span class="pre">cyclic</span></code>) distribution of MPI ranks across NUMA domains:</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 30.0%" />
<col style="width: 70.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Slurm Option</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--distribution=&lt;value&gt;[:&lt;value&gt;][:&lt;value&gt;]</span></code></p></td>
<td><p>Specifies the distribution of MPI ranks across compute nodes, sockets
(L3 cache regions on Frontier), and cores, respectively. The default values are
<code class="docutils literal notranslate"><span class="pre">block:cyclic:cyclic</span></code>, which is where the <code class="docutils literal notranslate"><span class="pre">cyclic</span></code> assignment comes from in the previous
examples.</p></td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In the job step for this example, <code class="docutils literal notranslate"><span class="pre">--distribution=*:block</span></code> is used, where
<code class="docutils literal notranslate"><span class="pre">*</span></code> represents the default value of <code class="docutils literal notranslate"><span class="pre">block</span></code> for the distribution of MPI
ranks across compute nodes and the distribution of MPI ranks across L3 cache
regions has been changed to <code class="docutils literal notranslate"><span class="pre">block</span></code> from its default value of <code class="docutils literal notranslate"><span class="pre">cyclic</span></code>.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Because the distribution across L3 cache regions has been changed to a
packed (<code class="docutils literal notranslate"><span class="pre">block</span></code>) configuration, caution must be taken to ensure MPI ranks
end up in the L3 cache regions where the GPUs they intend to be mapped to are
located. To accomplish this, the number of physical CPU cores assigned to an
MPI rank was increased - in this case to 4. Doing so ensures that only 2 MPI
ranks can fit into a single L3 cache region. If the value of <code class="docutils literal notranslate"><span class="pre">-c</span></code> was left at
<code class="docutils literal notranslate"><span class="pre">1</span></code>, all 8 MPI ranks would be packed into the first L3 region, where the
closest GPU would be GPU 4 - the only GPU in that L3 region.</p>
</div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span><span class="nv">OMP_NUM_THREADS</span><span class="o">=</span><span class="m">1</span><span class="w"> </span>srun<span class="w"> </span>-N1<span class="w"> </span>-n16<span class="w"> </span>-c4<span class="w"> </span>--ntasks-per-gpu<span class="o">=</span><span class="m">2</span><span class="w"> </span>--gpu-bind<span class="o">=</span>closest<span class="w"> </span>--distribution<span class="o">=</span>*:block<span class="w"> </span>./hello_jobstep<span class="w"> </span><span class="p">|</span><span class="w"> </span>sort
MPI<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">4</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d1
MPI<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">004</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">4</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d1
MPI<span class="w"> </span><span class="m">002</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">008</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">5</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d6
MPI<span class="w"> </span><span class="m">003</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">012</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">5</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d6
MPI<span class="w"> </span><span class="m">004</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">016</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">2</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c9
MPI<span class="w"> </span><span class="m">005</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">020</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">2</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c9
MPI<span class="w"> </span><span class="m">006</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">024</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">3</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>ce
MPI<span class="w"> </span><span class="m">007</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">028</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">3</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>ce
MPI<span class="w"> </span><span class="m">008</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">032</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">6</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d9
MPI<span class="w"> </span><span class="m">009</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">036</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">6</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d9
MPI<span class="w"> </span><span class="m">010</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">040</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">7</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>de
MPI<span class="w"> </span><span class="m">011</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">044</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">7</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>de
MPI<span class="w"> </span><span class="m">012</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">048</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c1
MPI<span class="w"> </span><span class="m">013</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">052</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c1
MPI<span class="w"> </span><span class="m">014</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">056</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">1</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c6
MPI<span class="w"> </span><span class="m">015</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">060</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">1</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c6
</pre></div>
</div>
<p>The overall effect of using <code class="docutils literal notranslate"><span class="pre">--distribution=*:block</span></code> and increasing the
number of physical CPU cores available to each MPI rank is to place the first
two MPI ranks in the first L3 cache region with GPU 4, the next two MPI ranks
in the second L3 cache region with GPU 5, and so on.</p>
<p><strong>Example 7: 32 MPI ranks - where 2 ranks share a GPU (packed, multi-node)</strong></p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This example assumes the use of a core specialization of <code class="docutils literal notranslate"><span class="pre">-S</span> <span class="pre">0</span></code>.  Because
Frontiers default core specialization (<code class="docutils literal notranslate"><span class="pre">-S</span> <span class="pre">8</span></code>) reserves the first core in
each L3 region, the packed mode can be problematic because the 7 cores
available in each L3 region wont necessarily divide evenly. This can lead to
tasks potentially spanning multiple L3 regions with its assigned cores, which
creates problems when Slurm tries to assign GPUs to a given task.</p>
</div>
<p>This example is an extension of Example 6 to use 2 compute nodes. With the
appropriate changes put in place in Example 6, it is a straightforward exercise
to change to using 2 nodes (<code class="docutils literal notranslate"><span class="pre">-N2</span></code>) and 32 MPI ranks (<code class="docutils literal notranslate"><span class="pre">-n32</span></code>).</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span><span class="nv">OMP_NUM_THREADS</span><span class="o">=</span><span class="m">1</span><span class="w"> </span>srun<span class="w"> </span>-N2<span class="w"> </span>-n32<span class="w"> </span>-c4<span class="w"> </span>--ntasks-per-gpu<span class="o">=</span><span class="m">2</span><span class="w"> </span>--gpu-bind<span class="o">=</span>closest<span class="w"> </span>--distribution<span class="o">=</span>*:block<span class="w"> </span>./hello_jobstep<span class="w"> </span><span class="p">|</span><span class="w"> </span>sort
MPI<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">4</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d1
MPI<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">004</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">4</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d1
MPI<span class="w"> </span><span class="m">002</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">010</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">5</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d6
MPI<span class="w"> </span><span class="m">003</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">012</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">5</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d6
MPI<span class="w"> </span><span class="m">004</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">016</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">2</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c9
MPI<span class="w"> </span><span class="m">005</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">021</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">2</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c9
MPI<span class="w"> </span><span class="m">006</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">024</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">3</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>ce
MPI<span class="w"> </span><span class="m">007</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">028</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">3</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>ce
MPI<span class="w"> </span><span class="m">008</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">032</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">6</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d9
MPI<span class="w"> </span><span class="m">009</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">037</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">6</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d9
MPI<span class="w"> </span><span class="m">010</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">041</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">7</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>de
MPI<span class="w"> </span><span class="m">011</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">044</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">7</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>de
MPI<span class="w"> </span><span class="m">012</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">049</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c1
MPI<span class="w"> </span><span class="m">013</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">052</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c1
MPI<span class="w"> </span><span class="m">014</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">056</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">1</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c6
MPI<span class="w"> </span><span class="m">015</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">060</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier002<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">1</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c6
MPI<span class="w"> </span><span class="m">016</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier004<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">4</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d1
MPI<span class="w"> </span><span class="m">017</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">004</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier004<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">4</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d1
MPI<span class="w"> </span><span class="m">018</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">008</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier004<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">5</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d6
MPI<span class="w"> </span><span class="m">019</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">013</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier004<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">5</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d6
MPI<span class="w"> </span><span class="m">020</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">016</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier004<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">2</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c9
MPI<span class="w"> </span><span class="m">021</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">020</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier004<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">2</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c9
MPI<span class="w"> </span><span class="m">022</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">024</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier004<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">3</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>ce
MPI<span class="w"> </span><span class="m">023</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">028</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier004<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">3</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>ce
MPI<span class="w"> </span><span class="m">024</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">034</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier004<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">6</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d9
MPI<span class="w"> </span><span class="m">025</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">036</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier004<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">6</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d9
MPI<span class="w"> </span><span class="m">026</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">040</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier004<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">7</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>de
MPI<span class="w"> </span><span class="m">027</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">044</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier004<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">7</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>de
MPI<span class="w"> </span><span class="m">028</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">048</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier004<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c1
MPI<span class="w"> </span><span class="m">029</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">052</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier004<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c1
MPI<span class="w"> </span><span class="m">030</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">056</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier004<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">1</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c6
MPI<span class="w"> </span><span class="m">031</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">060</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier004<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">1</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c6
</pre></div>
</div>
<p><strong>Example 8: 56 MPI ranks - where 7 ranks share a GPU (packed, single-node)</strong></p>
<p>An alternative solution to Example 6 and 7s <code class="docutils literal notranslate"><span class="pre">-S</span> <span class="pre">8</span></code> issue is to use <code class="docutils literal notranslate"><span class="pre">-c</span> <span class="pre">1</span></code>
instead.  There is no problem when running with 1 core per MPI rank (i.e., 7
ranks per GPU) because the task cant span multiple L3s.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span><span class="nv">OMP_NUM_THREADS</span><span class="o">=</span><span class="m">1</span><span class="w"> </span>srun<span class="w"> </span>-N1<span class="w"> </span>-n56<span class="w"> </span>-c1<span class="w"> </span>--ntasks-per-gpu<span class="o">=</span><span class="m">7</span><span class="w"> </span>--gpu-bind<span class="o">=</span>closest<span class="w"> </span>--distribution<span class="o">=</span>*:block<span class="w"> </span>./hello_jobstep<span class="w"> </span><span class="p">|</span><span class="w"> </span>sort
MPI<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier08413<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">4</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d1
MPI<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">002</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier08413<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">4</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d1
MPI<span class="w"> </span><span class="m">002</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">003</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier08413<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">4</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d1
MPI<span class="w"> </span><span class="m">003</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">004</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier08413<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">4</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d1
MPI<span class="w"> </span><span class="m">004</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">005</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier08413<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">4</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d1
MPI<span class="w"> </span><span class="m">005</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">006</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier08413<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">4</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d1
MPI<span class="w"> </span><span class="m">006</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">007</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier08413<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">4</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d1
MPI<span class="w"> </span><span class="m">007</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">009</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier08413<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">5</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d6
MPI<span class="w"> </span><span class="m">008</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">010</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier08413<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">5</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d6
MPI<span class="w"> </span><span class="m">009</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">011</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier08413<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">5</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d6
MPI<span class="w"> </span><span class="m">010</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">012</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier08413<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">5</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d6
MPI<span class="w"> </span><span class="m">011</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">013</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier08413<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">5</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d6
MPI<span class="w"> </span><span class="m">012</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">014</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier08413<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">5</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d6
MPI<span class="w"> </span><span class="m">013</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">015</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier08413<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">5</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d6
MPI<span class="w"> </span><span class="m">014</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">017</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier08413<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">2</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c9
MPI<span class="w"> </span><span class="m">015</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">018</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier08413<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">2</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c9
MPI<span class="w"> </span><span class="m">016</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">019</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier08413<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">2</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c9
MPI<span class="w"> </span><span class="m">017</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">020</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier08413<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">2</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c9
MPI<span class="w"> </span><span class="m">018</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">021</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier08413<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">2</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c9
MPI<span class="w"> </span><span class="m">019</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">022</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier08413<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">2</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c9
MPI<span class="w"> </span><span class="m">020</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">023</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier08413<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">2</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c9
MPI<span class="w"> </span><span class="m">021</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">025</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier08413<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">3</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>ce
MPI<span class="w"> </span><span class="m">022</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">026</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier08413<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">3</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>ce
MPI<span class="w"> </span><span class="m">023</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">027</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier08413<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">3</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>ce
MPI<span class="w"> </span><span class="m">024</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">028</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier08413<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">3</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>ce
MPI<span class="w"> </span><span class="m">025</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">029</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier08413<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">3</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>ce
MPI<span class="w"> </span><span class="m">026</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">030</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier08413<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">3</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>ce
MPI<span class="w"> </span><span class="m">027</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">031</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier08413<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">3</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>ce
MPI<span class="w"> </span><span class="m">028</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">033</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier08413<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">6</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d9
MPI<span class="w"> </span><span class="m">029</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">034</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier08413<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">6</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d9
MPI<span class="w"> </span><span class="m">030</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">035</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier08413<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">6</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d9
MPI<span class="w"> </span><span class="m">031</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">036</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier08413<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">6</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d9
MPI<span class="w"> </span><span class="m">032</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">037</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier08413<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">6</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d9
MPI<span class="w"> </span><span class="m">033</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">038</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier08413<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">6</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d9
MPI<span class="w"> </span><span class="m">034</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">039</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier08413<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">6</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d9
MPI<span class="w"> </span><span class="m">035</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">041</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier08413<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">7</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>de
MPI<span class="w"> </span><span class="m">036</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">042</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier08413<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">7</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>de
MPI<span class="w"> </span><span class="m">037</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">043</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier08413<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">7</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>de
MPI<span class="w"> </span><span class="m">038</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">044</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier08413<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">7</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>de
MPI<span class="w"> </span><span class="m">039</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">045</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier08413<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">7</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>de
MPI<span class="w"> </span><span class="m">040</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">046</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier08413<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">7</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>de
MPI<span class="w"> </span><span class="m">041</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">047</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier08413<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">7</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>de
MPI<span class="w"> </span><span class="m">042</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">049</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier08413<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c1
MPI<span class="w"> </span><span class="m">043</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">050</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier08413<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c1
MPI<span class="w"> </span><span class="m">044</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">051</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier08413<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c1
MPI<span class="w"> </span><span class="m">045</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">052</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier08413<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c1
MPI<span class="w"> </span><span class="m">046</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">053</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier08413<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c1
MPI<span class="w"> </span><span class="m">047</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">054</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier08413<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c1
MPI<span class="w"> </span><span class="m">048</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">055</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier08413<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c1
MPI<span class="w"> </span><span class="m">049</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">057</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier08413<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">1</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c6
MPI<span class="w"> </span><span class="m">050</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">058</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier08413<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">1</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c6
MPI<span class="w"> </span><span class="m">051</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">059</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier08413<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">1</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c6
MPI<span class="w"> </span><span class="m">052</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">060</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier08413<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">1</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c6
MPI<span class="w"> </span><span class="m">053</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">061</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier08413<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">1</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c6
MPI<span class="w"> </span><span class="m">054</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">062</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier08413<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">1</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c6
MPI<span class="w"> </span><span class="m">055</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">063</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier08413<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">1</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c6
</pre></div>
</div>
</section>
<section id="multiple-independent-job-steps">
<h5>Multiple Independent Job Steps<a class="headerlink" href="#multiple-independent-job-steps" title="Link to this heading"></a></h5>
<p><strong>Example 9: 8 independent and simultaneous job steps running on a single node</strong></p>
<p>This example shows how to run multiple independent, simultaneous job steps on a single compute node. Specifically, it shows how to run 8 independent <code class="docutils literal notranslate"><span class="pre">hello_jobstep</span></code> programs running on their own CPU core and GPU.</p>
<p>Submission script:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>

<span class="c1">#SBATCH -A stf016_frontier</span>
<span class="c1">#SBATCH -N 1</span>
<span class="c1">#SBATCH -t 5</span>

<span class="k">for</span><span class="w"> </span>idx<span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="o">{</span><span class="m">1</span>..8<span class="o">}</span><span class="p">;</span>

<span class="w">    </span><span class="k">do</span>
<span class="w">        </span>date

<span class="w">        </span><span class="nv">OMP_NUM_THREADS</span><span class="o">=</span><span class="m">1</span><span class="w"> </span>srun<span class="w"> </span>-u<span class="w"> </span>--gpus-per-task<span class="o">=</span><span class="m">1</span><span class="w"> </span>--gpu-bind<span class="o">=</span>closest<span class="w"> </span>-N1<span class="w"> </span>-n1<span class="w"> </span>-c1<span class="w"> </span>./hello_jobstep<span class="w"> </span><span class="p">&amp;</span>

<span class="w">        </span>sleep<span class="w"> </span><span class="m">1</span>
<span class="w">    </span><span class="k">done</span>

<span class="nb">wait</span>
</pre></div>
</div>
<p>Output:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>Fri<span class="w"> </span><span class="m">02</span><span class="w"> </span>Jun<span class="w"> </span><span class="m">2023</span><span class="w"> </span><span class="m">03</span>:33:45<span class="w"> </span>PM<span class="w"> </span>EDT
MPI<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">049</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04724<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c1
Fri<span class="w"> </span><span class="m">02</span><span class="w"> </span>Jun<span class="w"> </span><span class="m">2023</span><span class="w"> </span><span class="m">03</span>:33:46<span class="w"> </span>PM<span class="w"> </span>EDT
MPI<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">057</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04724<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">1</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c6
Fri<span class="w"> </span><span class="m">02</span><span class="w"> </span>Jun<span class="w"> </span><span class="m">2023</span><span class="w"> </span><span class="m">03</span>:33:47<span class="w"> </span>PM<span class="w"> </span>EDT
MPI<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">017</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04724<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">2</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>c9
Fri<span class="w"> </span><span class="m">02</span><span class="w"> </span>Jun<span class="w"> </span><span class="m">2023</span><span class="w"> </span><span class="m">03</span>:33:48<span class="w"> </span>PM<span class="w"> </span>EDT
MPI<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">025</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04724<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">3</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>ce
Fri<span class="w"> </span><span class="m">02</span><span class="w"> </span>Jun<span class="w"> </span><span class="m">2023</span><span class="w"> </span><span class="m">03</span>:33:49<span class="w"> </span>PM<span class="w"> </span>EDT
MPI<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">001</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04724<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">4</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d1
Fri<span class="w"> </span><span class="m">02</span><span class="w"> </span>Jun<span class="w"> </span><span class="m">2023</span><span class="w"> </span><span class="m">03</span>:33:50<span class="w"> </span>PM<span class="w"> </span>EDT
MPI<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">009</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04724<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">5</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d6
Fri<span class="w"> </span><span class="m">02</span><span class="w"> </span>Jun<span class="w"> </span><span class="m">2023</span><span class="w"> </span><span class="m">03</span>:33:51<span class="w"> </span>PM<span class="w"> </span>EDT
MPI<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">033</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04724<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">6</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>d9
Fri<span class="w"> </span><span class="m">02</span><span class="w"> </span>Jun<span class="w"> </span><span class="m">2023</span><span class="w"> </span><span class="m">03</span>:33:52<span class="w"> </span>PM<span class="w"> </span>EDT
MPI<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>OMP<span class="w"> </span><span class="m">000</span><span class="w"> </span>-<span class="w"> </span>HWT<span class="w"> </span><span class="m">041</span><span class="w"> </span>-<span class="w"> </span>Node<span class="w"> </span>frontier04724<span class="w"> </span>-<span class="w"> </span>RT_GPU_ID<span class="w"> </span><span class="m">0</span><span class="w"> </span>-<span class="w"> </span>GPU_ID<span class="w"> </span><span class="m">7</span><span class="w"> </span>-<span class="w"> </span>Bus_ID<span class="w"> </span>de
</pre></div>
</div>
<p>The output shows that each independent process ran on its own CPU core and GPU
on the same single node. To show that the ranks ran simultaneously, <code class="docutils literal notranslate"><span class="pre">date</span></code>
was called before each job step and a 20 second sleep was added to the end of
the <code class="docutils literal notranslate"><span class="pre">hello_jobstep</span></code> program. So the output also shows that the first job step
was submitted at <code class="docutils literal notranslate"><span class="pre">:45</span></code> and the subsequent job steps were all submitted
between <code class="docutils literal notranslate"><span class="pre">:46</span></code> and <code class="docutils literal notranslate"><span class="pre">:52</span></code>. But because each <code class="docutils literal notranslate"><span class="pre">hello_jobstep</span></code> sleeps for 20
seconds, the subsequent job steps must have all been running while the first
job step was still sleeping (and holding up its resources). And the same
argument can be made for the other job steps.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The <code class="docutils literal notranslate"><span class="pre">wait</span></code> command is needed so the job script (and allocation) do not immediately end after launching the job steps in the background.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">sleep</span> <span class="pre">1</span></code> is needed to give Slurm sufficient time to launch each job step.</p>
</div>
</section>
<section id="multiple-gpus-per-mpi-rank">
<h5>Multiple GPUs per MPI rank<a class="headerlink" href="#multiple-gpus-per-mpi-rank" title="Link to this heading"></a></h5>
<p>As mentioned previously, all GPUs are accessible by all MPI ranks by default,
so it is possible to <em>programatically</em> map any combination of GPUs to MPI
ranks. It should be noted however that Cray MPICH does not support GPU-aware
MPI for multiple GPUs per rank, so this binding is not suggested.</p>
</section>
</section>
<section id="nic-mapping">
<h4>NIC Mapping<a class="headerlink" href="#nic-mapping" title="Link to this heading"></a></h4>
<p>As shown in the <a class="reference external" href="https://docs.olcf.ornl.gov/_images/Frontier_Node_Diagram.jpg" target="_blank">Frontier Node Diagram</a>, each of the 4
NICs on a compute node is connected to a specific MI250X, and each MI250X is
(in turn) connected to a specific NUMA domain - so each NUMA domain is
correlated to a specific NIC. By default, processes (e.g., MPI ranks) that are
mapped to CPU cores in a specific NUMA domain are mapped (by CrayMPICH) to the
NIC that is correlated to that NUMA domain.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If a user attempts to map a process to a set of cores that span more than 1
NUMA domain using the default NIC mapping, they will see an error such as
<code class="docutils literal notranslate"><span class="pre">MPICH</span> <span class="pre">ERROR:</span> <span class="pre">Unable</span> <span class="pre">to</span> <span class="pre">use</span> <span class="pre">a</span> <span class="pre">NIC_POLICY</span> <span class="pre">of</span> <span class="pre">'NUMA'.</span> <span class="pre">Rank</span> <span class="pre">0</span> <span class="pre">is</span> <span class="pre">not</span> <span class="pre">confined</span>
<span class="pre">to</span> <span class="pre">a</span> <span class="pre">single</span> <span class="pre">NUMA</span> <span class="pre">node.</span></code>. This is expected behavior for the default NIC
policy.</p>
</div>
<p>The default behavior can be changed by using the <code class="docutils literal notranslate"><span class="pre">MPICH_OFI_NIC_POLICY</span></code>
environment variable (see <code class="docutils literal notranslate"><span class="pre">man</span> <span class="pre">mpi</span></code> for available options).</p>
</section>
</section>
<section id="ensemble-jobs">
<h3>Ensemble Jobs<a class="headerlink" href="#ensemble-jobs" title="Link to this heading"></a></h3>
<p>For many applications and use cases, the ability to launch many copies of the same binary in an independent context is needed.
This section highlights a few recommended solutions to launching ensemble runs on Frontier.</p>
<p>Before covering the tools that can be useful for this, be advised that the most reliable solution to this problem will be the use of MPI sub-communicators by your application.
For example, the LAMMPS Molecular Dynamics software supports a <code class="docutils literal notranslate"><span class="pre">partition</span></code> command, which can create many independent simulations from a single <code class="docutils literal notranslate"><span class="pre">srun</span></code> launch.</p>
<section id="single-process-ensemble-members">
<h4>Single-process ensemble members<a class="headerlink" href="#single-process-ensemble-members" title="Link to this heading"></a></h4>
<p>If you are able to fit each ensemble member onto a single MPI rank and single AMD Instinct MI250X GCD (8 GCDs per node), the most reliable solution is to use a single <code class="docutils literal notranslate"><span class="pre">srun</span></code> as follows:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>srun<span class="w"> </span>-N<span class="w"> </span><span class="nv">$SLURM_NNODES</span><span class="w"> </span>-n<span class="w"> </span><span class="k">$((</span><span class="nv">SLURM_NNODES</span><span class="o">*</span><span class="m">8</span><span class="k">))</span><span class="w"> </span>-c<span class="w"> </span><span class="m">7</span><span class="w"> </span>--gpus-per-task<span class="o">=</span><span class="m">1</span><span class="w"> </span>--gpu-bind<span class="o">=</span>closest<span class="w"> </span>./wrapper.sh
</pre></div>
</div>
<p>Where <code class="docutils literal notranslate"><span class="pre">wrapper.sh</span></code> is a shell script that launches your application.
This shell script is simply for convenience, in case you wish to vary the parameters to your application based on MPI rank.
This approach is fastest, most reliable, and easily scales to the entirety of Frontier.</p>
</section>
<section id="using-multiple-simultanoues-srun-s">
<h4>Using multiple simultanoues sruns<a class="headerlink" href="#using-multiple-simultanoues-srun-s" title="Link to this heading"></a></h4>
<p>If you are not able to fit each ensemble member onto a single MPI rank and GCD, a common approach is to launch multiple <code class="docutils literal notranslate"><span class="pre">srun</span></code> processes in the background simultaneously.
For example:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="k">for</span><span class="w"> </span>node<span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="k">$(</span>scontrol<span class="w"> </span>show<span class="w"> </span>hostnames<span class="k">)</span><span class="p">;</span><span class="w"> </span><span class="k">do</span>
<span class="w">    </span>srun<span class="w"> </span>-N<span class="w"> </span><span class="m">1</span><span class="w"> </span>-n<span class="w"> </span><span class="m">8</span><span class="w"> </span>-c<span class="w"> </span><span class="m">7</span><span class="w"> </span>--gpus-per-task<span class="o">=</span><span class="m">1</span><span class="w"> </span>--gpu-bind<span class="o">=</span>closest<span class="w"> </span>&lt;executable&gt;<span class="w"> </span><span class="p">&amp;</span>
<span class="k">done</span>
<span class="c1"># Wait for srun&#39;s to all finish</span>
<span class="nb">wait</span>
</pre></div>
</div>
<p>Each <code class="docutils literal notranslate"><span class="pre">srun</span></code> communicates to the Slurm controller node (which is shared among all users) when it is launched.
Large amounts of <code class="docutils literal notranslate"><span class="pre">srun</span></code> processes can temporarily overwhelm the Slurm controller, making commands like <code class="docutils literal notranslate"><span class="pre">sbatch</span></code> and <code class="docutils literal notranslate"><span class="pre">squeue</span></code> hang.
This approach can be fast, but is unreliable and does not scale to a significant portion of Frontier, and potentially overloads the Slurm controller.
We do not yet recommend this approach beyond 100 simultaneous <code class="docutils literal notranslate"><span class="pre">srun</span></code>s.</p>
<p>Slurm version 24.05 (installed on Frontier August 20, 2024) introduces the <code class="docutils literal notranslate"><span class="pre">--stepmgr</span></code> flag for <code class="docutils literal notranslate"><span class="pre">sbatch</span></code>, which uses the first node in the allocation to manage job steps instead of the Slurm controller.
This feature should substantially improve the ability to run many simultaneous <code class="docutils literal notranslate"><span class="pre">srun</span></code>s.
However, this flag does not currently work reliably on Frontier, and it is not recommended at this time.
This functionality can be re-created by using the Flux scheduler within Slurm, described in the next section.</p>
</section>
<section id="flux-in-a-slurm-allocation">
<h4>Flux in a Slurm allocation<a class="headerlink" href="#flux-in-a-slurm-allocation" title="Link to this heading"></a></h4>
<p><a class="reference external" href="https://hpc-tutorials.llnl.gov/flux/" target="_blank">Flux</a> is a light-weight batch scheduler that can be run inside of a Slurm allocation.
This effectively creates a local queue of jobs that you alone can submit to and manage.
Functionally, this achieves the same objective as launching multiple <code class="docutils literal notranslate"><span class="pre">srun</span></code>s in the background,
but has the added benefit that Flux can automatically start the next job on a node as each job finishes.
Using <code class="docutils literal notranslate"><span class="pre">srun</span></code>, you are forced to use <code class="docutils literal notranslate"><span class="pre">wait</span></code> to wait for all processes to finish, before launching another flight of processes.
Flux can more readily load-balance workloads across nodes inside a Slurm job allocation.</p>
<p>The following code is an example of how to launch an ensemble where each job step is run on one node using Flux:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1">#SBATCH -A &lt;proj&gt;</span>
<span class="c1">#SBATCH -t &lt;timelimit&gt;</span>
<span class="c1">#SBATCH -N 8</span>

module<span class="w"> </span>load<span class="w"> </span>rocm
module<span class="w"> </span>load<span class="w"> </span>hwloc/2.9.1-gpu<span class="w"> </span><span class="c1"># Flux requires a GPU-enabled hwloc to see the GPUs</span>
module<span class="w"> </span>load<span class="w"> </span>flux

<span class="c1"># A few Flux commands to note:</span>
<span class="c1">#   flux start -- starts the Flux server daemons</span>
<span class="c1">#   flux resource list -- lists the resources available to Flux</span>
<span class="c1">#   flux submit -- submits &amp; detaches from a Flux job. Returns a hash string identifying the submitted job</span>
<span class="c1">#   flux jobs -- synonymous to `squeue`, displays the Flux queue</span>
<span class="c1">#   flux run -- submits &amp; runs a Flux job (does not return prompt until command is complete)</span>
<span class="c1">#   flux queue drain -- similar to `wait`, blocks until Flux queue is empty</span>

<span class="c1"># Flux flags:</span>
<span class="c1">#   -N 1 -- 1 node</span>
<span class="c1">#   -n 8 -- 8 tasks</span>
<span class="c1">#   -c 7 -- 7 cores per task</span>
<span class="c1">#   --gpus-per-task=1 -- binds 1 GPU per task (DOES NOT WORK currently)</span>
<span class="c1"># We launch one Flux process per node, with all available CPUs and GPUs allocated to it</span>
<span class="c1"># Flux understands that it was launched in a Slurm allocation, and only the Flux daemon on the first node is listening to commands</span>
srun<span class="w"> </span>-N<span class="w"> </span><span class="nv">$SLURM_NNODES</span><span class="w"> </span>-n<span class="w"> </span><span class="nv">$SLURM_NNODES</span><span class="w"> </span>-c<span class="w"> </span><span class="m">56</span><span class="w"> </span>--gpus-per-node<span class="o">=</span><span class="m">8</span><span class="w"> </span>flux<span class="w"> </span>start<span class="w"> </span><span class="se">\</span>
<span class="w">    </span><span class="s2">&quot;flux resource list;</span>
<span class="s2">    for i in \$(seq 1 </span><span class="nv">$SLURM_NNODES</span><span class="s2">); do</span>
<span class="s2">        flux submit -N 1 -n 8 -c 7 -x --gpus-per-task=1 --output=output.\$i.txt bash -c &#39;hostname; env | grep VISIBLE; /usr/bin/time ./vadd&#39;;</span>
<span class="s2">    done;</span>
<span class="s2">    flux queue drain;&quot;</span>
</pre></div>
</div>
<p>This approach is slightly slower than using background <code class="docutils literal notranslate"><span class="pre">srun</span></code>s, but is much more reliable and flexible.
For example, if you have 100 nodes and 500 single-node jobs to run, you can submit all 500 job steps to the Flux scheduler and it will run them as soon as a node is available.</p>
<p>A simple performance test was perfomed using 500 nodes, assigning 1 job to each node using <code class="docutils literal notranslate"><span class="pre">flux</span> <span class="pre">submit</span></code>, as in the above example.
It took 2 minutes to submit the 500 jobs to Flux.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The Flux <code class="docutils literal notranslate"><span class="pre">--gpus-per-task=1</span></code> flag does not currently work as expected. With this flag, all 8 GPUs on a node will be seen by each rank.
Users should either explicitly set <code class="docutils literal notranslate"><span class="pre">ROCR_VISIBLE_DEVICES</span></code> for each rank to a different GPU, or provide information to the application about how to bind to a single GPU.</p>
</div>
</section>
</section>
<section id="tips-for-launching-at-scale">
<h3>Tips for Launching at Scale<a class="headerlink" href="#tips-for-launching-at-scale" title="Link to this heading"></a></h3>
<section id="sbcast-your-executable-and-libraries">
<h4>SBCAST your executable and libraries<a class="headerlink" href="#sbcast-your-executable-and-libraries" title="Link to this heading"></a></h4>
<p>Slurm contains a utility called <code class="docutils literal notranslate"><span class="pre">sbcast</span></code>. This program takes a file and broadcasts it to each nodes node-local storage (ie, <code class="docutils literal notranslate"><span class="pre">/tmp</span></code>, NVMe).
This is useful for sharing large input files, binaries and shared libraries, while reducing the overhead on shared file systems and overhead at startup.
This is highly recommended at scale if you have multiple shared libraries on Lustre/NFS file systems.</p>
<section id="sbcasting-a-single-file">
<h5>SBCASTing a single file<a class="headerlink" href="#sbcasting-a-single-file" title="Link to this heading"></a></h5>
<p>Here is a simple example of a file <code class="docutils literal notranslate"><span class="pre">sbcast</span></code> from a users scratch space on Lustre to each nodes NVMe drive:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="c1">#SBATCH -A &lt;projid&gt;</span>
<span class="c1">#SBATCH -J sbcast_to_nvme</span>
<span class="c1">#SBATCH -o %x-%j.out</span>
<span class="c1">#SBATCH -t 00:05:00</span>
<span class="c1">#SBATCH -p batch</span>
<span class="c1">#SBATCH -N 2</span>
<span class="c1">#SBATCH -C nvme</span>

date

<span class="c1"># Change directory to user scratch space (Orion)</span>
<span class="nb">cd</span><span class="w"> </span>/lustre/orion/&lt;projid&gt;/scratch/&lt;userid&gt;

<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;This is an example file&quot;</span><span class="w"> </span>&gt;<span class="w"> </span>test.txt
<span class="nb">echo</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;*****ORIGINAL FILE*****&quot;</span>
cat<span class="w"> </span>test.txt
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;***********************&quot;</span>

<span class="c1"># SBCAST file from Orion to NVMe -- NOTE: ``-C nvme`` is required to use the NVMe drive</span>
sbcast<span class="w"> </span>-pf<span class="w"> </span>test.txt<span class="w"> </span>/mnt/bb/<span class="nv">$USER</span>/test.txt
<span class="k">if</span><span class="w"> </span><span class="o">[</span><span class="w"> </span>!<span class="w"> </span><span class="s2">&quot;</span><span class="nv">$?</span><span class="s2">&quot;</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="s2">&quot;0&quot;</span><span class="w"> </span><span class="o">]</span><span class="p">;</span><span class="w"> </span><span class="k">then</span>
<span class="w">    </span><span class="c1"># CHECK EXIT CODE. When SBCAST fails, it may leave partial files on the compute nodes, and if you continue to launch srun,</span>
<span class="w">    </span><span class="c1"># your application may pick up partially complete shared library files, which would give you confusing errors.</span>
<span class="w">    </span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;SBCAST failed!&quot;</span>
<span class="w">    </span><span class="nb">exit</span><span class="w"> </span><span class="m">1</span>
<span class="k">fi</span>

<span class="nb">echo</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;*****DISPLAYING FILES ON EACH NODE IN THE ALLOCATION*****&quot;</span>
<span class="c1"># Check to see if file exists</span>
srun<span class="w"> </span>-N<span class="w"> </span><span class="si">${</span><span class="nv">SLURM_NNODES</span><span class="si">}</span><span class="w"> </span>-n<span class="w"> </span><span class="si">${</span><span class="nv">SLURM_NNODES</span><span class="si">}</span><span class="w"> </span>--ntasks-per-node<span class="o">=</span><span class="m">1</span><span class="w"> </span>bash<span class="w"> </span>-c<span class="w"> </span><span class="s2">&quot;echo \&quot;\$(hostname): \$(ls -lh /mnt/bb/</span><span class="nv">$USER</span><span class="s2">/test.txt)\&quot;&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;*********************************************************&quot;</span>

<span class="nb">echo</span>
<span class="c1"># Showing the file on the current node -- this will be the same on all other nodes in the allocation</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;*****SBCAST FILE ON CURRENT NODE******&quot;</span>
cat<span class="w"> </span>/mnt/bb/<span class="nv">$USER</span>/test.txt
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;**************************************&quot;</span>
</pre></div>
</div>
<p>and here is the output from that script:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>Fri<span class="w"> </span><span class="m">03</span><span class="w"> </span>Mar<span class="w"> </span><span class="m">2023</span><span class="w"> </span><span class="m">03</span>:43:30<span class="w"> </span>PM<span class="w"> </span>EST

*****ORIGINAL<span class="w"> </span>FILE*****
This<span class="w"> </span>is<span class="w"> </span>an<span class="w"> </span>example<span class="w"> </span>file
***********************

*****DISPLAYING<span class="w"> </span>FILES<span class="w"> </span>ON<span class="w"> </span>EACH<span class="w"> </span>NODE<span class="w"> </span>IN<span class="w"> </span>THE<span class="w"> </span>ALLOCATION*****
frontier00001:<span class="w"> </span>-rw-r--r--<span class="w"> </span><span class="m">1</span><span class="w"> </span>hagertnl<span class="w"> </span>hagertnl<span class="w"> </span><span class="m">24</span><span class="w"> </span>Mar<span class="w">  </span><span class="m">3</span><span class="w"> </span><span class="m">15</span>:43<span class="w"> </span>/mnt/bb/hagertnl/test.txt
frontier00002:<span class="w"> </span>-rw-r--r--<span class="w"> </span><span class="m">1</span><span class="w"> </span>hagertnl<span class="w"> </span>hagertnl<span class="w"> </span><span class="m">24</span><span class="w"> </span>Mar<span class="w">  </span><span class="m">3</span><span class="w"> </span><span class="m">15</span>:43<span class="w"> </span>/mnt/bb/hagertnl/test.txt
*********************************************************

*****SBCAST<span class="w"> </span>FILE<span class="w"> </span>ON<span class="w"> </span>CURRENT<span class="w"> </span>NODE******
This<span class="w"> </span>is<span class="w"> </span>an<span class="w"> </span>example<span class="w"> </span>file
**************************************
</pre></div>
</div>
</section>
<section id="sbcasting-a-binary-with-libraries-stored-on-shared-file-systems">
<h5>SBCASTing a binary with libraries stored on shared file systems<a class="headerlink" href="#sbcasting-a-binary-with-libraries-stored-on-shared-file-systems" title="Link to this heading"></a></h5>
<p><code class="docutils literal notranslate"><span class="pre">sbcast</span></code> also handles binaries and their libraries:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="c1">#SBATCH -A &lt;projid&gt;</span>
<span class="c1">#SBATCH -J sbcast_binary_to_nvme</span>
<span class="c1">#SBATCH -o %x-%j.out</span>
<span class="c1">#SBATCH -t 00:05:00</span>
<span class="c1">#SBATCH -p batch</span>
<span class="c1">#SBATCH -N 2</span>
<span class="c1">#SBATCH -C nvme</span>

date

<span class="c1"># Change directory to user scratch space (Orion)</span>
<span class="nb">cd</span><span class="w"> </span>/lustre/orion/&lt;projid&gt;/scratch/&lt;userid&gt;

<span class="c1"># For this example, I use a HIP-enabled LAMMPS binary, with dependencies to MPI, HIP, and HWLOC</span>
<span class="nv">exe</span><span class="o">=</span><span class="s2">&quot;lmp&quot;</span>

<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;*****ldd ./</span><span class="si">${</span><span class="nv">exe</span><span class="si">}</span><span class="s2">*****&quot;</span>
ldd<span class="w"> </span>./<span class="si">${</span><span class="nv">exe</span><span class="si">}</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;*************************&quot;</span>

<span class="c1"># SBCAST executable from Orion to NVMe -- NOTE: ``-C nvme`` is needed in SBATCH headers to use the NVMe drive</span>
<span class="c1"># NOTE: dlopen&#39;d files will NOT be picked up by sbcast</span>
<span class="c1"># SBCAST automatically excludes several directories: /lib,/usr/lib,/lib64,/usr/lib64,/opt</span>
<span class="c1">#   - These directories are node-local and are very fast to read from, so SBCASTing them isn&#39;t critical</span>
<span class="c1">#   - see ``$ scontrol show config | grep BcastExclude`` for current list</span>
<span class="c1">#   - OLCF-provided libraries in ``/sw`` are not on the exclusion list. ``/sw`` is an NFS shared file system</span>
<span class="c1">#   - To override, add ``--exclude=NONE`` to arguments</span>
sbcast<span class="w"> </span>--send-libs<span class="w"> </span>-pf<span class="w"> </span><span class="si">${</span><span class="nv">exe</span><span class="si">}</span><span class="w"> </span>/mnt/bb/<span class="nv">$USER</span>/<span class="si">${</span><span class="nv">exe</span><span class="si">}</span>
<span class="k">if</span><span class="w"> </span><span class="o">[</span><span class="w"> </span>!<span class="w"> </span><span class="s2">&quot;</span><span class="nv">$?</span><span class="s2">&quot;</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="s2">&quot;0&quot;</span><span class="w"> </span><span class="o">]</span><span class="p">;</span><span class="w"> </span><span class="k">then</span>
<span class="w">    </span><span class="c1"># CHECK EXIT CODE. When SBCAST fails, it may leave partial files on the compute nodes, and if you continue to launch srun,</span>
<span class="w">    </span><span class="c1"># your application may pick up partially complete shared library files, which would give you confusing errors.</span>
<span class="w">    </span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;SBCAST failed!&quot;</span>
<span class="w">    </span><span class="nb">exit</span><span class="w"> </span><span class="m">1</span>
<span class="k">fi</span>

<span class="c1"># Check to see if file exists</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;*****ls -lh /mnt/bb/</span><span class="nv">$USER</span><span class="s2">*****&quot;</span>
ls<span class="w"> </span>-lh<span class="w"> </span>/mnt/bb/<span class="nv">$USER</span>/
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;*****ls -lh /mnt/bb/</span><span class="nv">$USER</span><span class="s2">/</span><span class="si">${</span><span class="nv">exe</span><span class="si">}</span><span class="s2">_libs*****&quot;</span>
ls<span class="w"> </span>-lh<span class="w"> </span>/mnt/bb/<span class="nv">$USER</span>/<span class="si">${</span><span class="nv">exe</span><span class="si">}</span>_libs

<span class="c1"># SBCAST sends all libraries detected by `ld` (minus any excluded), and stores them in the same directory in each node&#39;s node-local storage</span>
<span class="c1"># Any libraries opened by `dlopen` are NOT sent, since they are not known by the linker at run-time.</span>

<span class="c1"># At minimum: prepend the node-local path to LD_LIBRARY_PATH to pick up the SBCAST libraries</span>
<span class="c1"># It is also recommended that you **remove** any paths that you don&#39;t need, like those that contain the libraries that you just SBCAST&#39;d</span>
<span class="c1"># Failure to remove may result in unnecessary calls to stat shared file systems</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="s2">&quot;/mnt/bb/</span><span class="nv">$USER</span><span class="s2">/</span><span class="si">${</span><span class="nv">exe</span><span class="si">}</span><span class="s2">_libs:</span><span class="si">${</span><span class="nv">LD_LIBRARY_PATH</span><span class="si">}</span><span class="s2">&quot;</span>

<span class="c1"># If you SBCAST **all** your libraries (ie, `--exclude=NONE`), you may use the following line:</span>
<span class="c1">#export LD_LIBRARY_PATH=&quot;/mnt/bb/$USER/${exe}_libs:$(pkg-config --variable=libdir libfabric)&quot;</span>
<span class="c1"># Use with caution -- certain libraries may use ``dlopen`` at runtime, and that is NOT covered by sbcast</span>
<span class="c1"># If you use this option, we recommend you contact OLCF Help Desk for the latest list of additional steps required</span>

<span class="c1"># You may notice that some libraries are still linked from /sw/frontier, even after SBCASTing.</span>
<span class="c1"># This is because the Spack-build modules use RPATH to find their dependencies.</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;*****ldd /mnt/bb/</span><span class="nv">$USER</span><span class="s2">/</span><span class="si">${</span><span class="nv">exe</span><span class="si">}</span><span class="s2">*****&quot;</span>
ldd<span class="w"> </span>/mnt/bb/<span class="nv">$USER</span>/<span class="si">${</span><span class="nv">exe</span><span class="si">}</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;*************************************&quot;</span>
</pre></div>
</div>
<p>and here is the output from that script:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>Tue<span class="w"> </span><span class="m">28</span><span class="w"> </span>Mar<span class="w"> </span><span class="m">2023</span><span class="w"> </span><span class="m">05</span>:01:41<span class="w"> </span>PM<span class="w"> </span>EDT
*****ldd<span class="w"> </span>./lmp*****
<span class="w">    </span>linux-vdso.so.1<span class="w"> </span><span class="o">(</span>0x00007fffeda02000<span class="o">)</span>
<span class="w">    </span>libgcc_s.so.1<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/lib64/libgcc_s.so.1<span class="w"> </span><span class="o">(</span>0x00007fffed5bb000<span class="o">)</span>
<span class="w">    </span>libpthread.so.0<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/lib64/libpthread.so.0<span class="w"> </span><span class="o">(</span>0x00007fffed398000<span class="o">)</span>
<span class="w">    </span>libm.so.6<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/lib64/libm.so.6<span class="w"> </span><span class="o">(</span>0x00007fffed04d000<span class="o">)</span>
<span class="w">    </span>librt.so.1<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/lib64/librt.so.1<span class="w"> </span><span class="o">(</span>0x00007fffece44000<span class="o">)</span>
<span class="w">    </span>libamdhip64.so.4<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/opt/rocm-4.5.2/lib/libamdhip64.so.4<span class="w"> </span><span class="o">(</span>0x00007fffec052000<span class="o">)</span>
<span class="w">    </span>libmpi_cray.so.12<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/opt/cray/pe/lib64/libmpi_cray.so.12<span class="w"> </span><span class="o">(</span>0x00007fffe96cc000<span class="o">)</span>
<span class="w">    </span>libmpi_gtl_hsa.so.0<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/opt/cray/pe/lib64/libmpi_gtl_hsa.so.0<span class="w"> </span><span class="o">(</span>0x00007fffe9469000<span class="o">)</span>
<span class="w">    </span>libhsa-runtime64.so.1<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/opt/rocm-5.3.0/lib/libhsa-runtime64.so.1<span class="w"> </span><span class="o">(</span>0x00007fffe8fdc000<span class="o">)</span>
<span class="w">    </span>libhwloc.so.15<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/sw/frontier/spack-envs/base/opt/linux-sles15-x86_64/gcc-7.5.0/hwloc-2.5.0-4p6jkgf5ez6wr27pytkzyptppzpugu3e/lib/libhwloc.so.15<span class="w"> </span><span class="o">(</span>0x00007fffe8d82000<span class="o">)</span>
<span class="w">    </span>libdl.so.2<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/lib64/libdl.so.2<span class="w"> </span><span class="o">(</span>0x00007fffe8b7e000<span class="o">)</span>
<span class="w">    </span>libhipfft.so<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/opt/rocm-5.3.0/lib/libhipfft.so<span class="w"> </span><span class="o">(</span>0x00007fffed9d2000<span class="o">)</span>
<span class="w">    </span>libstdc++.so.6<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libstdc++.so.6<span class="w"> </span><span class="o">(</span>0x00007fffe875b000<span class="o">)</span>
<span class="w">    </span>libc.so.6<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/lib64/libc.so.6<span class="w"> </span><span class="o">(</span>0x00007fffe8366000<span class="o">)</span>
<span class="w">    </span>/lib64/ld-linux-x86-64.so.2<span class="w"> </span><span class="o">(</span>0x00007fffed7da000<span class="o">)</span>
<span class="w">    </span>libamd_comgr.so.2<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/opt/rocm-5.3.0/lib/libamd_comgr.so.2<span class="w"> </span><span class="o">(</span>0x00007fffe06e0000<span class="o">)</span>
<span class="w">    </span>libnuma.so.1<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libnuma.so.1<span class="w"> </span><span class="o">(</span>0x00007fffe04d4000<span class="o">)</span>
<span class="w">    </span>libfabric.so.1<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/opt/cray/libfabric/1.15.2.0/lib64/libfabric.so.1<span class="w"> </span><span class="o">(</span>0x00007fffe01e2000<span class="o">)</span>
<span class="w">    </span>libatomic.so.1<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libatomic.so.1<span class="w"> </span><span class="o">(</span>0x00007fffdffd9000<span class="o">)</span>
<span class="w">    </span>libpmi.so.0<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/opt/cray/pe/lib64/libpmi.so.0<span class="w"> </span><span class="o">(</span>0x00007fffdfdd7000<span class="o">)</span>
<span class="w">    </span>libpmi2.so.0<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/opt/cray/pe/lib64/libpmi2.so.0<span class="w"> </span><span class="o">(</span>0x00007fffdfb9e000<span class="o">)</span>
<span class="w">    </span>libquadmath.so.0<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libquadmath.so.0<span class="w"> </span><span class="o">(</span>0x00007fffdf959000<span class="o">)</span>
<span class="w">    </span>libmodules.so.1<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/opt/cray/pe/lib64/cce/libmodules.so.1<span class="w"> </span><span class="o">(</span>0x00007fffed9b5000<span class="o">)</span>
<span class="w">    </span>libfi.so.1<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/opt/cray/pe/lib64/cce/libfi.so.1<span class="w"> </span><span class="o">(</span>0x00007fffdf3b4000<span class="o">)</span>
<span class="w">    </span>libcraymath.so.1<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/opt/cray/pe/lib64/cce/libcraymath.so.1<span class="w"> </span><span class="o">(</span>0x00007fffed8ce000<span class="o">)</span>
<span class="w">    </span>libf.so.1<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/opt/cray/pe/lib64/cce/libf.so.1<span class="w"> </span><span class="o">(</span>0x00007fffed83b000<span class="o">)</span>
<span class="w">    </span>libu.so.1<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/opt/cray/pe/lib64/cce/libu.so.1<span class="w"> </span><span class="o">(</span>0x00007fffdf2ab000<span class="o">)</span>
<span class="w">    </span>libcsup.so.1<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/opt/cray/pe/lib64/cce/libcsup.so.1<span class="w"> </span><span class="o">(</span>0x00007fffed832000<span class="o">)</span>
<span class="w">    </span>libamdhip64.so.5<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/opt/rocm-5.3.0/lib/libamdhip64.so.5<span class="w"> </span><span class="o">(</span>0x00007fffdda8a000<span class="o">)</span>
<span class="w">    </span>libelf.so.1<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libelf.so.1<span class="w"> </span><span class="o">(</span>0x00007fffdd871000<span class="o">)</span>
<span class="w">    </span>libdrm.so.2<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libdrm.so.2<span class="w"> </span><span class="o">(</span>0x00007fffdd65d000<span class="o">)</span>
<span class="w">    </span>libdrm_amdgpu.so.1<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libdrm_amdgpu.so.1<span class="w"> </span><span class="o">(</span>0x00007fffdd453000<span class="o">)</span>
<span class="w">    </span>libpciaccess.so.0<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/sw/frontier/spack-envs/base/opt/linux-sles15-x86_64/gcc-7.5.0/libpciaccess-0.16-6evcvl74myxfxdrddmnsvbc7sgfx6s5j/lib/libpciaccess.so.0<span class="w"> </span><span class="o">(</span>0x00007fffdd24a000<span class="o">)</span>
<span class="w">    </span>libxml2.so.2<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/sw/frontier/spack-envs/base/opt/linux-sles15-x86_64/gcc-7.5.0/libxml2-2.9.12-h7fe2yauotu3xit7rsaixaowd3noknur/lib/libxml2.so.2<span class="w"> </span><span class="o">(</span>0x00007fffdcee6000<span class="o">)</span>
<span class="w">    </span>librocfft.so.0<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/opt/rocm-5.3.0/lib/librocfft.so.0<span class="w"> </span><span class="o">(</span>0x00007fffdca1a000<span class="o">)</span>
<span class="w">    </span>libz.so.1<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/lib64/libz.so.1<span class="w"> </span><span class="o">(</span>0x00007fffdc803000<span class="o">)</span>
<span class="w">    </span>libtinfo.so.6<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/lib64/libtinfo.so.6<span class="w"> </span><span class="o">(</span>0x00007fffdc5d5000<span class="o">)</span>
<span class="w">    </span>libcxi.so.1<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libcxi.so.1<span class="w"> </span><span class="o">(</span>0x00007fffdc3b0000<span class="o">)</span>
<span class="w">    </span>libcurl.so.4<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libcurl.so.4<span class="w"> </span><span class="o">(</span>0x00007fffdc311000<span class="o">)</span>
<span class="w">    </span>libjson-c.so.3<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libjson-c.so.3<span class="w"> </span><span class="o">(</span>0x00007fffdc101000<span class="o">)</span>
<span class="w">    </span>libpals.so.0<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/opt/cray/pe/lib64/libpals.so.0<span class="w"> </span><span class="o">(</span>0x00007fffdbefc000<span class="o">)</span>
<span class="w">    </span>libgfortran.so.5<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/opt/cray/pe/gcc-libs/libgfortran.so.5<span class="w"> </span><span class="o">(</span>0x00007fffdba50000<span class="o">)</span>
<span class="w">    </span>liblzma.so.5<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/sw/frontier/spack-envs/base/opt/linux-sles15-x86_64/gcc-7.5.0/xz-5.2.5-zwra4gpckelt5umczowf3jtmeiz3yd7u/lib/liblzma.so.5<span class="w"> </span><span class="o">(</span>0x00007fffdb82a000<span class="o">)</span>
<span class="w">    </span>libiconv.so.2<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/sw/frontier/spack-envs/base/opt/linux-sles15-x86_64/gcc-7.5.0/libiconv-1.16-coz5dzhtoeq5unhjirayfn2xftnxk43l/lib/libiconv.so.2<span class="w"> </span><span class="o">(</span>0x00007fffdb52e000<span class="o">)</span>
<span class="w">    </span>librocfft-device-0.so.0<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/opt/rocm-5.3.0/lib/librocfft-device-0.so.0<span class="w"> </span><span class="o">(</span>0x00007fffa11a0000<span class="o">)</span>
<span class="w">    </span>librocfft-device-1.so.0<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/opt/rocm-5.3.0/lib/librocfft-device-1.so.0<span class="w"> </span><span class="o">(</span>0x00007fff6491b000<span class="o">)</span>
<span class="w">    </span>librocfft-device-2.so.0<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/opt/rocm-5.3.0/lib/librocfft-device-2.so.0<span class="w"> </span><span class="o">(</span>0x00007fff2a828000<span class="o">)</span>
<span class="w">    </span>librocfft-device-3.so.0<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/opt/rocm-5.3.0/lib/librocfft-device-3.so.0<span class="w"> </span><span class="o">(</span>0x00007ffef7eee000<span class="o">)</span>
<span class="w">    </span>libnghttp2.so.14<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libnghttp2.so.14<span class="w"> </span><span class="o">(</span>0x00007ffef7cc6000<span class="o">)</span>
<span class="w">    </span>libidn2.so.0<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libidn2.so.0<span class="w"> </span><span class="o">(</span>0x00007ffef7aa9000<span class="o">)</span>
<span class="w">    </span>libssh.so.4<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libssh.so.4<span class="w"> </span><span class="o">(</span>0x00007ffef783b000<span class="o">)</span>
<span class="w">    </span>libpsl.so.5<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libpsl.so.5<span class="w"> </span><span class="o">(</span>0x00007ffef7629000<span class="o">)</span>
<span class="w">    </span>libssl.so.1.1<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libssl.so.1.1<span class="w"> </span><span class="o">(</span>0x00007ffef758a000<span class="o">)</span>
<span class="w">    </span>libcrypto.so.1.1<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libcrypto.so.1.1<span class="w"> </span><span class="o">(</span>0x00007ffef724a000<span class="o">)</span>
<span class="w">    </span>libgssapi_krb5.so.2<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libgssapi_krb5.so.2<span class="w"> </span><span class="o">(</span>0x00007ffef6ff8000<span class="o">)</span>
<span class="w">    </span>libldap_r-2.4.so.2<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libldap_r-2.4.so.2<span class="w"> </span><span class="o">(</span>0x00007ffef6da4000<span class="o">)</span>
<span class="w">    </span>liblber-2.4.so.2<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/liblber-2.4.so.2<span class="w"> </span><span class="o">(</span>0x00007ffef6b95000<span class="o">)</span>
<span class="w">    </span>libzstd.so.1<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libzstd.so.1<span class="w"> </span><span class="o">(</span>0x00007ffef6865000<span class="o">)</span>
<span class="w">    </span>libbrotlidec.so.1<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libbrotlidec.so.1<span class="w"> </span><span class="o">(</span>0x00007ffef6659000<span class="o">)</span>
<span class="w">    </span>libunistring.so.2<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libunistring.so.2<span class="w"> </span><span class="o">(</span>0x00007ffef62d6000<span class="o">)</span>
<span class="w">    </span>libjitterentropy.so.3<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libjitterentropy.so.3<span class="w"> </span><span class="o">(</span>0x00007ffef60cf000<span class="o">)</span>
<span class="w">    </span>libkrb5.so.3<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libkrb5.so.3<span class="w"> </span><span class="o">(</span>0x00007ffef5df6000<span class="o">)</span>
<span class="w">    </span>libk5crypto.so.3<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libk5crypto.so.3<span class="w"> </span><span class="o">(</span>0x00007ffef5bde000<span class="o">)</span>
<span class="w">    </span>libcom_err.so.2<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/lib64/libcom_err.so.2<span class="w"> </span><span class="o">(</span>0x00007ffef59da000<span class="o">)</span>
<span class="w">    </span>libkrb5support.so.0<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libkrb5support.so.0<span class="w"> </span><span class="o">(</span>0x00007ffef57cb000<span class="o">)</span>
<span class="w">    </span>libresolv.so.2<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/lib64/libresolv.so.2<span class="w"> </span><span class="o">(</span>0x00007ffef55b3000<span class="o">)</span>
<span class="w">    </span>libsasl2.so.3<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libsasl2.so.3<span class="w"> </span><span class="o">(</span>0x00007ffef5396000<span class="o">)</span>
<span class="w">    </span>libbrotlicommon.so.1<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libbrotlicommon.so.1<span class="w"> </span><span class="o">(</span>0x00007ffef5175000<span class="o">)</span>
<span class="w">    </span>libkeyutils.so.1<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libkeyutils.so.1<span class="w"> </span><span class="o">(</span>0x00007ffef4f70000<span class="o">)</span>
<span class="w">    </span>libselinux.so.1<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/lib64/libselinux.so.1<span class="w"> </span><span class="o">(</span>0x00007ffef4d47000<span class="o">)</span>
<span class="w">    </span>libpcre.so.1<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libpcre.so.1<span class="w"> </span><span class="o">(</span>0x00007ffef4abe000<span class="o">)</span>
*************************
*****ls<span class="w"> </span>-lh<span class="w"> </span>/mnt/bb/hagertnl*****
total<span class="w"> </span>236M
-rwxr-xr-x<span class="w"> </span><span class="m">1</span><span class="w"> </span>hagertnl<span class="w"> </span>hagertnl<span class="w"> </span>236M<span class="w"> </span>Mar<span class="w"> </span><span class="m">28</span><span class="w"> </span><span class="m">17</span>:01<span class="w"> </span>lmp
drwx------<span class="w"> </span><span class="m">2</span><span class="w"> </span>hagertnl<span class="w"> </span>hagertnl<span class="w">  </span><span class="m">114</span><span class="w"> </span>Mar<span class="w"> </span><span class="m">28</span><span class="w"> </span><span class="m">17</span>:01<span class="w"> </span>lmp_libs
*****ls<span class="w"> </span>-lh<span class="w"> </span>/mnt/bb/hagertnl/lmp_libs*****
total<span class="w"> </span><span class="m">9</span>.2M
-rwxr-xr-x<span class="w"> </span><span class="m">1</span><span class="w"> </span>hagertnl<span class="w"> </span>hagertnl<span class="w"> </span><span class="m">1</span>.6M<span class="w"> </span>Oct<span class="w">  </span><span class="m">6</span><span class="w">  </span><span class="m">2021</span><span class="w"> </span>libhwloc.so.15
-rwxr-xr-x<span class="w"> </span><span class="m">1</span><span class="w"> </span>hagertnl<span class="w"> </span>hagertnl<span class="w"> </span><span class="m">1</span>.6M<span class="w"> </span>Oct<span class="w">  </span><span class="m">6</span><span class="w">  </span><span class="m">2021</span><span class="w"> </span>libiconv.so.2
-rwxr-xr-x<span class="w"> </span><span class="m">1</span><span class="w"> </span>hagertnl<span class="w"> </span>hagertnl<span class="w"> </span>783K<span class="w"> </span>Oct<span class="w">  </span><span class="m">6</span><span class="w">  </span><span class="m">2021</span><span class="w"> </span>liblzma.so.5
-rwxr-xr-x<span class="w"> </span><span class="m">1</span><span class="w"> </span>hagertnl<span class="w"> </span>hagertnl<span class="w"> </span>149K<span class="w"> </span>Oct<span class="w">  </span><span class="m">6</span><span class="w">  </span><span class="m">2021</span><span class="w"> </span>libpciaccess.so.0
-rwxr-xr-x<span class="w"> </span><span class="m">1</span><span class="w"> </span>hagertnl<span class="w"> </span>hagertnl<span class="w"> </span><span class="m">5</span>.2M<span class="w"> </span>Oct<span class="w">  </span><span class="m">6</span><span class="w">  </span><span class="m">2021</span><span class="w"> </span>libxml2.so.2
*****ldd<span class="w"> </span>/mnt/bb/hagertnl/lmp*****
<span class="w">    </span>linux-vdso.so.1<span class="w"> </span><span class="o">(</span>0x00007fffeda02000<span class="o">)</span>
<span class="w">    </span>libgcc_s.so.1<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/lib64/libgcc_s.so.1<span class="w"> </span><span class="o">(</span>0x00007fffed5bb000<span class="o">)</span>
<span class="w">    </span>libpthread.so.0<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/lib64/libpthread.so.0<span class="w"> </span><span class="o">(</span>0x00007fffed398000<span class="o">)</span>
<span class="w">    </span>libm.so.6<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/lib64/libm.so.6<span class="w"> </span><span class="o">(</span>0x00007fffed04d000<span class="o">)</span>
<span class="w">    </span>librt.so.1<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/lib64/librt.so.1<span class="w"> </span><span class="o">(</span>0x00007fffece44000<span class="o">)</span>
<span class="w">    </span>libamdhip64.so.4<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/opt/rocm-4.5.2/lib/libamdhip64.so.4<span class="w"> </span><span class="o">(</span>0x00007fffec052000<span class="o">)</span>
<span class="w">    </span>libmpi_cray.so.12<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/opt/cray/pe/lib64/libmpi_cray.so.12<span class="w"> </span><span class="o">(</span>0x00007fffe96cc000<span class="o">)</span>
<span class="w">    </span>libmpi_gtl_hsa.so.0<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/opt/cray/pe/lib64/libmpi_gtl_hsa.so.0<span class="w"> </span><span class="o">(</span>0x00007fffe9469000<span class="o">)</span>
<span class="w">    </span>libhsa-runtime64.so.1<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/opt/rocm-5.3.0/lib/libhsa-runtime64.so.1<span class="w"> </span><span class="o">(</span>0x00007fffe8fdc000<span class="o">)</span>
<span class="w">    </span>libhwloc.so.15<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/mnt/bb/hagertnl/lmp_libs/libhwloc.so.15<span class="w"> </span><span class="o">(</span>0x00007fffe8d82000<span class="o">)</span>
<span class="w">    </span>libdl.so.2<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/lib64/libdl.so.2<span class="w"> </span><span class="o">(</span>0x00007fffe8b7e000<span class="o">)</span>
<span class="w">    </span>libhipfft.so<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/opt/rocm-5.3.0/lib/libhipfft.so<span class="w"> </span><span class="o">(</span>0x00007fffed9d2000<span class="o">)</span>
<span class="w">    </span>libstdc++.so.6<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libstdc++.so.6<span class="w"> </span><span class="o">(</span>0x00007fffe875b000<span class="o">)</span>
<span class="w">    </span>libc.so.6<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/lib64/libc.so.6<span class="w"> </span><span class="o">(</span>0x00007fffe8366000<span class="o">)</span>
<span class="w">    </span>/lib64/ld-linux-x86-64.so.2<span class="w"> </span><span class="o">(</span>0x00007fffed7da000<span class="o">)</span>
<span class="w">    </span>libamd_comgr.so.2<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/opt/rocm-5.3.0/lib/libamd_comgr.so.2<span class="w"> </span><span class="o">(</span>0x00007fffe06e0000<span class="o">)</span>
<span class="w">    </span>libnuma.so.1<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libnuma.so.1<span class="w"> </span><span class="o">(</span>0x00007fffe04d4000<span class="o">)</span>
<span class="w">    </span>libfabric.so.1<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/opt/cray/libfabric/1.15.2.0/lib64/libfabric.so.1<span class="w"> </span><span class="o">(</span>0x00007fffe01e2000<span class="o">)</span>
<span class="w">    </span>libatomic.so.1<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libatomic.so.1<span class="w"> </span><span class="o">(</span>0x00007fffdffd9000<span class="o">)</span>
<span class="w">    </span>libpmi.so.0<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/opt/cray/pe/lib64/libpmi.so.0<span class="w"> </span><span class="o">(</span>0x00007fffdfdd7000<span class="o">)</span>
<span class="w">    </span>libpmi2.so.0<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/opt/cray/pe/lib64/libpmi2.so.0<span class="w"> </span><span class="o">(</span>0x00007fffdfb9e000<span class="o">)</span>
<span class="w">    </span>libquadmath.so.0<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libquadmath.so.0<span class="w"> </span><span class="o">(</span>0x00007fffdf959000<span class="o">)</span>
<span class="w">    </span>libmodules.so.1<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/opt/cray/pe/lib64/cce/libmodules.so.1<span class="w"> </span><span class="o">(</span>0x00007fffed9b5000<span class="o">)</span>
<span class="w">    </span>libfi.so.1<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/opt/cray/pe/lib64/cce/libfi.so.1<span class="w"> </span><span class="o">(</span>0x00007fffdf3b4000<span class="o">)</span>
<span class="w">    </span>libcraymath.so.1<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/opt/cray/pe/lib64/cce/libcraymath.so.1<span class="w"> </span><span class="o">(</span>0x00007fffed8ce000<span class="o">)</span>
<span class="w">    </span>libf.so.1<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/opt/cray/pe/lib64/cce/libf.so.1<span class="w"> </span><span class="o">(</span>0x00007fffed83b000<span class="o">)</span>
<span class="w">    </span>libu.so.1<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/opt/cray/pe/lib64/cce/libu.so.1<span class="w"> </span><span class="o">(</span>0x00007fffdf2ab000<span class="o">)</span>
<span class="w">    </span>libcsup.so.1<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/opt/cray/pe/lib64/cce/libcsup.so.1<span class="w"> </span><span class="o">(</span>0x00007fffed832000<span class="o">)</span>
<span class="w">    </span>libamdhip64.so.5<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/opt/rocm-5.3.0/lib/libamdhip64.so.5<span class="w"> </span><span class="o">(</span>0x00007fffdda8a000<span class="o">)</span>
<span class="w">    </span>libelf.so.1<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libelf.so.1<span class="w"> </span><span class="o">(</span>0x00007fffdd871000<span class="o">)</span>
<span class="w">    </span>libdrm.so.2<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libdrm.so.2<span class="w"> </span><span class="o">(</span>0x00007fffdd65d000<span class="o">)</span>
<span class="w">    </span>libdrm_amdgpu.so.1<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libdrm_amdgpu.so.1<span class="w"> </span><span class="o">(</span>0x00007fffdd453000<span class="o">)</span>
<span class="w">    </span>libpciaccess.so.0<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/sw/frontier/spack-envs/base/opt/linux-sles15-x86_64/gcc-7.5.0/libpciaccess-0.16-6evcvl74myxfxdrddmnsvbc7sgfx6s5j/lib/libpciaccess.so.0<span class="w"> </span><span class="o">(</span>0x00007fffdd24a000<span class="o">)</span>
<span class="w">    </span>libxml2.so.2<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/sw/frontier/spack-envs/base/opt/linux-sles15-x86_64/gcc-7.5.0/libxml2-2.9.12-h7fe2yauotu3xit7rsaixaowd3noknur/lib/libxml2.so.2<span class="w"> </span><span class="o">(</span>0x00007fffdcee6000<span class="o">)</span>
<span class="w">    </span>librocfft.so.0<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/opt/rocm-5.3.0/lib/librocfft.so.0<span class="w"> </span><span class="o">(</span>0x00007fffdca1a000<span class="o">)</span>
<span class="w">    </span>libz.so.1<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/lib64/libz.so.1<span class="w"> </span><span class="o">(</span>0x00007fffdc803000<span class="o">)</span>
<span class="w">    </span>libtinfo.so.6<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/lib64/libtinfo.so.6<span class="w"> </span><span class="o">(</span>0x00007fffdc5d5000<span class="o">)</span>
<span class="w">    </span>libcxi.so.1<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libcxi.so.1<span class="w"> </span><span class="o">(</span>0x00007fffdc3b0000<span class="o">)</span>
<span class="w">    </span>libcurl.so.4<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libcurl.so.4<span class="w"> </span><span class="o">(</span>0x00007fffdc311000<span class="o">)</span>
<span class="w">    </span>libjson-c.so.3<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libjson-c.so.3<span class="w"> </span><span class="o">(</span>0x00007fffdc101000<span class="o">)</span>
<span class="w">    </span>libpals.so.0<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/opt/cray/pe/lib64/libpals.so.0<span class="w"> </span><span class="o">(</span>0x00007fffdbefc000<span class="o">)</span>
<span class="w">    </span>libgfortran.so.5<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/opt/cray/pe/gcc-libs/libgfortran.so.5<span class="w"> </span><span class="o">(</span>0x00007fffdba50000<span class="o">)</span>
<span class="w">    </span>liblzma.so.5<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/sw/frontier/spack-envs/base/opt/linux-sles15-x86_64/gcc-7.5.0/xz-5.2.5-zwra4gpckelt5umczowf3jtmeiz3yd7u/lib/liblzma.so.5<span class="w"> </span><span class="o">(</span>0x00007fffdb82a000<span class="o">)</span>
<span class="w">    </span>libiconv.so.2<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/sw/frontier/spack-envs/base/opt/linux-sles15-x86_64/gcc-7.5.0/libiconv-1.16-coz5dzhtoeq5unhjirayfn2xftnxk43l/lib/libiconv.so.2<span class="w"> </span><span class="o">(</span>0x00007fffdb52e000<span class="o">)</span>
<span class="w">    </span>librocfft-device-0.so.0<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/opt/rocm-5.3.0/lib/librocfft-device-0.so.0<span class="w"> </span><span class="o">(</span>0x00007fffa11a0000<span class="o">)</span>
<span class="w">    </span>librocfft-device-1.so.0<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/opt/rocm-5.3.0/lib/librocfft-device-1.so.0<span class="w"> </span><span class="o">(</span>0x00007fff6491b000<span class="o">)</span>
<span class="w">    </span>librocfft-device-2.so.0<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/opt/rocm-5.3.0/lib/librocfft-device-2.so.0<span class="w"> </span><span class="o">(</span>0x00007fff2a828000<span class="o">)</span>
<span class="w">    </span>librocfft-device-3.so.0<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/opt/rocm-5.3.0/lib/librocfft-device-3.so.0<span class="w"> </span><span class="o">(</span>0x00007ffef7eee000<span class="o">)</span>
<span class="w">    </span>libnghttp2.so.14<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libnghttp2.so.14<span class="w"> </span><span class="o">(</span>0x00007ffef7cc6000<span class="o">)</span>
<span class="w">    </span>libidn2.so.0<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libidn2.so.0<span class="w"> </span><span class="o">(</span>0x00007ffef7aa9000<span class="o">)</span>
<span class="w">    </span>libssh.so.4<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libssh.so.4<span class="w"> </span><span class="o">(</span>0x00007ffef783b000<span class="o">)</span>
<span class="w">    </span>libpsl.so.5<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libpsl.so.5<span class="w"> </span><span class="o">(</span>0x00007ffef7629000<span class="o">)</span>
<span class="w">    </span>libssl.so.1.1<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libssl.so.1.1<span class="w"> </span><span class="o">(</span>0x00007ffef758a000<span class="o">)</span>
<span class="w">    </span>libcrypto.so.1.1<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libcrypto.so.1.1<span class="w"> </span><span class="o">(</span>0x00007ffef724a000<span class="o">)</span>
<span class="w">    </span>libgssapi_krb5.so.2<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libgssapi_krb5.so.2<span class="w"> </span><span class="o">(</span>0x00007ffef6ff8000<span class="o">)</span>
<span class="w">    </span>libldap_r-2.4.so.2<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libldap_r-2.4.so.2<span class="w"> </span><span class="o">(</span>0x00007ffef6da4000<span class="o">)</span>
<span class="w">    </span>liblber-2.4.so.2<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/liblber-2.4.so.2<span class="w"> </span><span class="o">(</span>0x00007ffef6b95000<span class="o">)</span>
<span class="w">    </span>libzstd.so.1<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libzstd.so.1<span class="w"> </span><span class="o">(</span>0x00007ffef6865000<span class="o">)</span>
<span class="w">    </span>libbrotlidec.so.1<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libbrotlidec.so.1<span class="w"> </span><span class="o">(</span>0x00007ffef6659000<span class="o">)</span>
<span class="w">    </span>libunistring.so.2<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libunistring.so.2<span class="w"> </span><span class="o">(</span>0x00007ffef62d6000<span class="o">)</span>
<span class="w">    </span>libjitterentropy.so.3<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libjitterentropy.so.3<span class="w"> </span><span class="o">(</span>0x00007ffef60cf000<span class="o">)</span>
<span class="w">    </span>libkrb5.so.3<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libkrb5.so.3<span class="w"> </span><span class="o">(</span>0x00007ffef5df6000<span class="o">)</span>
<span class="w">    </span>libk5crypto.so.3<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libk5crypto.so.3<span class="w"> </span><span class="o">(</span>0x00007ffef5bde000<span class="o">)</span>
<span class="w">    </span>libcom_err.so.2<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/lib64/libcom_err.so.2<span class="w"> </span><span class="o">(</span>0x00007ffef59da000<span class="o">)</span>
<span class="w">    </span>libkrb5support.so.0<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libkrb5support.so.0<span class="w"> </span><span class="o">(</span>0x00007ffef57cb000<span class="o">)</span>
<span class="w">    </span>libresolv.so.2<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/lib64/libresolv.so.2<span class="w"> </span><span class="o">(</span>0x00007ffef55b3000<span class="o">)</span>
<span class="w">    </span>libsasl2.so.3<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libsasl2.so.3<span class="w"> </span><span class="o">(</span>0x00007ffef5396000<span class="o">)</span>
<span class="w">    </span>libbrotlicommon.so.1<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libbrotlicommon.so.1<span class="w"> </span><span class="o">(</span>0x00007ffef5175000<span class="o">)</span>
<span class="w">    </span>libkeyutils.so.1<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libkeyutils.so.1<span class="w"> </span><span class="o">(</span>0x00007ffef4f70000<span class="o">)</span>
<span class="w">    </span>libselinux.so.1<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/lib64/libselinux.so.1<span class="w"> </span><span class="o">(</span>0x00007ffef4d47000<span class="o">)</span>
<span class="w">    </span>libpcre.so.1<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/usr/lib64/libpcre.so.1<span class="w"> </span><span class="o">(</span>0x00007ffef4abe000<span class="o">)</span>
*************************************
</pre></div>
</div>
<p>Notice that the libraries are sent to the <code class="docutils literal notranslate"><span class="pre">${exe}_libs</span></code> directory in the same prefix as the executable.
Once libraries are here, you cannot tell where they came from, so consider doing an <code class="docutils literal notranslate"><span class="pre">ldd</span></code> of your executable prior to <code class="docutils literal notranslate"><span class="pre">sbcast</span></code>.</p>
</section>
<section id="best-sbcasting-a-binary-with-all-libraries">
<h5>Best: SBCASTing a binary with ALL libraries<a class="headerlink" href="#best-sbcasting-a-binary-with-all-libraries" title="Link to this heading"></a></h5>
<p>As mentioned above, you can alternatively use <code class="docutils literal notranslate"><span class="pre">--exclude=NONE</span></code> on <code class="docutils literal notranslate"><span class="pre">sbcast</span></code> to send all libraries along with the binary.
Using <code class="docutils literal notranslate"><span class="pre">--exclude=NONE</span></code> requires more effort but substantially simplifies the linker configuration at run-time.
A job script for the previous example, modified for sending all libraries is shown below.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="c1">#SBATCH -A &lt;projid&gt;</span>
<span class="c1">#SBATCH -J sbcast_binary_to_nvme</span>
<span class="c1">#SBATCH -o %x-%j.out</span>
<span class="c1">#SBATCH -t 00:05:00</span>
<span class="c1">#SBATCH -p batch</span>
<span class="c1">#SBATCH -N 2</span>
<span class="c1">#SBATCH -C nvme</span>

date

<span class="c1"># Change directory to user scratch space (Orion)</span>
<span class="nb">cd</span><span class="w"> </span>/lustre/orion/&lt;projid&gt;/scratch/&lt;userid&gt;

<span class="c1"># For this example, I use a HIP-enabled LAMMPS binary, with dependencies to MPI, HIP, and HWLOC</span>
<span class="nv">exe</span><span class="o">=</span><span class="s2">&quot;lmp&quot;</span>

<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;*****ldd ./</span><span class="si">${</span><span class="nv">exe</span><span class="si">}</span><span class="s2">*****&quot;</span>
ldd<span class="w"> </span>./<span class="si">${</span><span class="nv">exe</span><span class="si">}</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;*************************&quot;</span>

<span class="c1"># SBCAST executable from Orion to NVMe -- NOTE: ``-C nvme`` is needed in SBATCH headers to use the NVMe drive</span>
<span class="c1"># NOTE: dlopen&#39;d files will NOT be picked up by sbcast</span>
sbcast<span class="w"> </span>--send-libs<span class="w"> </span>--exclude<span class="o">=</span>NONE<span class="w"> </span>-pf<span class="w"> </span><span class="si">${</span><span class="nv">exe</span><span class="si">}</span><span class="w"> </span>/mnt/bb/<span class="nv">$USER</span>/<span class="si">${</span><span class="nv">exe</span><span class="si">}</span>
<span class="k">if</span><span class="w"> </span><span class="o">[</span><span class="w"> </span>!<span class="w"> </span><span class="s2">&quot;</span><span class="nv">$?</span><span class="s2">&quot;</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="s2">&quot;0&quot;</span><span class="w"> </span><span class="o">]</span><span class="p">;</span><span class="w"> </span><span class="k">then</span>
<span class="w">    </span><span class="c1"># CHECK EXIT CODE. When SBCAST fails, it may leave partial files on the compute nodes, and if you continue to launch srun,</span>
<span class="w">    </span><span class="c1"># your application may pick up partially complete shared library files, which would give you confusing errors.</span>
<span class="w">    </span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;SBCAST failed!&quot;</span>
<span class="w">    </span><span class="nb">exit</span><span class="w"> </span><span class="m">1</span>
<span class="k">fi</span>

<span class="c1"># Check to see if file exists</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;*****ls -lh /mnt/bb/</span><span class="nv">$USER</span><span class="s2">*****&quot;</span>
ls<span class="w"> </span>-lh<span class="w"> </span>/mnt/bb/<span class="nv">$USER</span>/
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;*****ls -lh /mnt/bb/</span><span class="nv">$USER</span><span class="s2">/</span><span class="si">${</span><span class="nv">exe</span><span class="si">}</span><span class="s2">_libs*****&quot;</span>
ls<span class="w"> </span>-lh<span class="w"> </span>/mnt/bb/<span class="nv">$USER</span>/<span class="si">${</span><span class="nv">exe</span><span class="si">}</span>_libs

<span class="c1"># SBCAST sends all libraries detected by `ld` (minus any excluded), and stores them in the same directory in each node&#39;s node-local storage</span>
<span class="c1"># Any libraries opened by `dlopen` are NOT sent, since they are not known by the linker at run-time.</span>

<span class="c1"># All required libraries now reside in /mnt/bb/$USER/${exe}_libs</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="s2">&quot;/mnt/bb/</span><span class="nv">$USER</span><span class="s2">/</span><span class="si">${</span><span class="nv">exe</span><span class="si">}</span><span class="s2">_libs&quot;</span>

<span class="c1"># libfabric dlopen&#39;s several libraries:</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">LD_LIBRARY_PATH</span><span class="si">}</span><span class="s2">:</span><span class="k">$(</span>pkg-config<span class="w"> </span>--variable<span class="o">=</span>libdir<span class="w"> </span>libfabric<span class="k">)</span><span class="s2">&quot;</span>

<span class="c1"># cray-mpich dlopen&#39;s libhsa-runtime64.so and libamdhip64.so (non-versioned), so symlink on each node:</span>
srun<span class="w"> </span>-N<span class="w"> </span><span class="si">${</span><span class="nv">SLURM_NNODES</span><span class="si">}</span><span class="w"> </span>-n<span class="w"> </span><span class="si">${</span><span class="nv">SLURM_NNODES</span><span class="si">}</span><span class="w"> </span>--ntasks-per-node<span class="o">=</span><span class="m">1</span><span class="w"> </span>--label<span class="w"> </span>-D<span class="w"> </span>/mnt/bb/<span class="nv">$USER</span>/<span class="si">${</span><span class="nv">exe</span><span class="si">}</span>_libs<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>bash<span class="w"> </span>-c<span class="w"> </span><span class="s2">&quot;if [ -f libhsa-runtime64.so.1 ]; then ln -s libhsa-runtime64.so.1 libhsa-runtime64.so; fi;</span>
<span class="s2">    if [ -f libamdhip64.so.5 ]; then ln -s libamdhip64.so.5 libamdhip64.so; fi&quot;</span>

<span class="c1"># RocBLAS has over 1,000 device libraries that may be `dlopen`&#39;d by RocBLAS during a run.</span>
<span class="c1"># It&#39;s impractical to SBCAST all of these, so you can set this path instead, if you use RocBLAS:</span>
<span class="c1">#export ROCBLAS_TENSILE_LIBPATH=${ROCM_PATH}/lib/rocblas/library</span>

<span class="c1"># You may notice that some libraries are still linked from /sw/crusher, even after SBCASTing.</span>
<span class="c1"># This is because the Spack-build modules use RPATH to find their dependencies. This behavior cannot be changed.</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;*****ldd /mnt/bb/</span><span class="nv">$USER</span><span class="s2">/</span><span class="si">${</span><span class="nv">exe</span><span class="si">}</span><span class="s2">*****&quot;</span>
ldd<span class="w"> </span>/mnt/bb/<span class="nv">$USER</span>/<span class="si">${</span><span class="nv">exe</span><span class="si">}</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;*************************************&quot;</span>
</pre></div>
</div>
<p>Some libraries still resolved to paths outside of <code class="docutils literal notranslate"><span class="pre">/mnt/bb</span></code>, and the reason for that is that the executable may have several paths in <code class="docutils literal notranslate"><span class="pre">RPATH</span></code>.</p>
</section>
<section id="sbcasting-a-conda-environment">
<h5>SBCASTing a conda environment<a class="headerlink" href="#sbcasting-a-conda-environment" title="Link to this heading"></a></h5>
<p>Users running Python environments at scale can also take advantage of using <code class="docutils literal notranslate"><span class="pre">sbcast</span></code>.
For details on how to use <code class="docutils literal notranslate"><span class="pre">sbcast</span></code> to move your conda environments to the NVMe, please see our <a class="reference internal" href="../software/python/sbcast_conda.html"><span class="doc">Sbcast Conda Environments Guide</span></a>.</p>
</section>
</section>
</section>
</section>
<hr class="docutils" />
<section id="software">
<h2>Software<a class="headerlink" href="#software" title="Link to this heading"></a></h2>
<p>Visualization and analysis tasks should be done on the Andes cluster. There are a few tools provided for various visualization tasks, as described in the <a class="reference internal" href="andes_user_guide.html#andes-viz-tools"><span class="std std-ref">Visualization tools</span></a> section of the <a class="reference internal" href="andes_user_guide.html#andes-user-guide"><span class="std std-ref">Andes User Guide</span></a>.</p>
<p>For a full list of software availability and latest news at the OLCF, please reference the <a class="reference internal" href="../software/software-news.html"><span class="doc">Software</span></a> section in OLCFs User Documentation.</p>
</section>
<section id="debugging">
<h2>Debugging<a class="headerlink" href="#debugging" title="Link to this heading"></a></h2>
<section id="linaro-ddt">
<h3>Linaro DDT<a class="headerlink" href="#linaro-ddt" title="Link to this heading"></a></h3>
<p>Linaro DDT is an advanced debugging tool used for scalar, multi-threaded,
and large-scale parallel applications. In addition to traditional
debugging features (setting breakpoints, stepping through code,
examining variables), DDT also supports attaching to already-running
processes and memory debugging. In-depth details of DDT can be found in
the <a class="reference external" href="https://www.linaroforge.com/documentation/" target="_blank">Official DDT User Guide</a>, and
instructions for how to use it on OLCF systems can be found on the <a class="reference internal" href="../software/debugging/index.html"><span class="doc">Debugging Software</span></a> page. DDT is the
OLCFs recommended debugging software for large parallel applications.</p>
<p>One of the most useful features of DDT is its remote debugging feature. This allows you to connect to a debugging session on Frontier from a client running on your workstation. The local client provides much faster interaction than you would have if using the graphical client on Frontier. For guidance in setting up the remote client see the <a class="reference internal" href="../software/debugging/index.html"><span class="doc">Debugging Software</span></a> page.</p>
</section>
<section id="gdb">
<h3>GDB<a class="headerlink" href="#gdb" title="Link to this heading"></a></h3>
<p><a class="reference external" href="https://www.gnu.org/software/gdb/" target="_blank">GDB</a>, the GNU Project Debugger,
is a command-line debugger useful for traditional debugging and
investigating code crashes. GDB lets you debug programs written in Ada,
C, C++, Objective-C, Pascal (and many other languages).</p>
<p>GDB is availableon Summit under all compiler families:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">module</span> <span class="n">load</span> <span class="n">gdb</span>
</pre></div>
</div>
<p>To use GDB to debug your application run:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">gdb</span> <span class="o">./</span><span class="n">path_to_executable</span>
</pre></div>
</div>
<p>Additional information about GDB usage can befound on the <a class="reference external" href="https://www.sourceware.org/gdb/documentation/" target="_blank">GDB Documentation Page</a>.</p>
</section>
<section id="valgrind4hpc">
<h3>Valgrind4hpc<a class="headerlink" href="#valgrind4hpc" title="Link to this heading"></a></h3>
<p>Valgrind4hpc is a Valgrind-based debugging tool to aid in the detection of memory leaks
and errors in parallel applications. Valgrind4hpc aggregates any duplicate
messages across ranks to help provide an understandable picture of
program behavior. Valgrind4hpc manages starting and redirecting output from many
copies of Valgrind, as well as deduplicating and filtering Valgrind messages.
If your program can be debugged with Valgrind, it can be debugged with
Valgrind4hpc.</p>
<p>Valgrind4hpc is available on Frontier under all compiler families:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">module</span> <span class="n">load</span> <span class="n">valgrind4hpc</span>
</pre></div>
</div>
<p>Additional information about Valgrind4hpc usage can be found on the <a class="reference external" href="https://support.hpe.com/hpesc/public/docDisplay?docId=a00115110en_us&amp;page=Debug_Applications_With_valgrind4hpc_To_Find_Common_Errors.html" target="_blank">HPE Cray Programming Environment User Guide Page</a>.</p>
</section>
<section id="omnitrace">
<h3>Omnitrace<a class="headerlink" href="#omnitrace" title="Link to this heading"></a></h3>
<p>OLCF provides installations of AMDs <a class="reference external" href="https://github.com/AMDResearch/omnitrace" target="_blank">Omnitrace</a> profiling tools on Frontier.
AMD provides documentation on the usage of Omnitrace at <a class="reference external" href="https://amdresearch.github.io/omnitrace/" target="_blank">https://amdresearch.github.io/omnitrace/</a>.
This section details the installation and common pitfalls of the <code class="docutils literal notranslate"><span class="pre">omnitrace</span></code> module on Frontier.</p>
<p>Unlike <code class="docutils literal notranslate"><span class="pre">omniperf</span></code>, the <code class="docutils literal notranslate"><span class="pre">omnitrace</span></code> module only relies on a ROCm module.
A ROCm module must be loaded before being able to do view or load the <code class="docutils literal notranslate"><span class="pre">omnitrace</span></code> module.
As a rule of thumb, always load the <code class="docutils literal notranslate"><span class="pre">omnitrace</span></code> module last (especially after you load a ROCm module).
If you load a new version of ROCm, you will need to re-load <code class="docutils literal notranslate"><span class="pre">omnitrace</span></code>.</p>
<p>To use <code class="docutils literal notranslate"><span class="pre">omnitrace</span></code>, you may use the following commands</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">module</span> <span class="n">load</span> <span class="n">rocm</span>
<span class="n">module</span> <span class="n">load</span> <span class="n">omnitrace</span>
</pre></div>
</div>
</section>
</section>
<section id="profiling-applications">
<h2>Profiling Applications<a class="headerlink" href="#profiling-applications" title="Link to this heading"></a></h2>
<section id="getting-started-with-the-hpe-performance-analysis-tools-pat">
<h3>Getting Started with the HPE Performance Analysis Tools (PAT)<a class="headerlink" href="#getting-started-with-the-hpe-performance-analysis-tools-pat" title="Link to this heading"></a></h3>
<p>The Performance Analysis Tools (PAT), formerly CrayPAT, are a suite of utilities that enable users to capture and analyze performance data generated during program execution. These tools provide an integrated infrastructure for measurement, analysis, and visualization of computation, communication, I/O, and memory utilization to help users optimize programs for faster execution and more efficient computing resource usage.</p>
<p>There are three programming interfaces available: (1) <code class="docutils literal notranslate"><span class="pre">Perftools-lite</span></code>, (2) <code class="docutils literal notranslate"><span class="pre">Perftools</span></code>, and (3) <code class="docutils literal notranslate"><span class="pre">Perftools-preload</span></code>.</p>
<p>Below are two examples that generate an instrumented executable using <code class="docutils literal notranslate"><span class="pre">Perftools</span></code>, which is an advanced interface that provides full-featured data collection and analysis capability, including full traces with timeline displays.</p>
<p>The first example generates an instrumented executable using a <code class="docutils literal notranslate"><span class="pre">PrgEnv-amd</span></code> build:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>module<span class="w"> </span>load<span class="w"> </span>PrgEnv-amd
module<span class="w"> </span>load<span class="w"> </span>craype-accel-amd-gfx90a
module<span class="w"> </span>load<span class="w"> </span>rocm
module<span class="w"> </span>load<span class="w"> </span>perftools

<span class="nb">export</span><span class="w"> </span><span class="nv">PATH</span><span class="o">=</span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">PATH</span><span class="si">}</span><span class="s2">:</span><span class="si">${</span><span class="nv">ROCM_PATH</span><span class="si">}</span><span class="s2">/llvm/bin&quot;</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">CXX</span><span class="o">=</span><span class="s1">&#39;CC -x hip&#39;</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">CXXFLAGS</span><span class="o">=</span><span class="s1">&#39;-ggdb -O3 -std=c++17 Wall&#39;</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">LD</span><span class="o">=</span><span class="s1">&#39;CC&#39;</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">LDFLAGS</span><span class="o">=</span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">CXXFLAGS</span><span class="si">}</span><span class="s2"> -L</span><span class="si">${</span><span class="nv">ROCM_PATH</span><span class="si">}</span><span class="s2">/lib&quot;</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">LIBS</span><span class="o">=</span><span class="s1">&#39;-lamdhip64&#39;</span>

make<span class="w"> </span>clean
make

pat_build<span class="w"> </span>-g<span class="w"> </span>hip,io,mpi<span class="w"> </span>-w<span class="w"> </span>-f<span class="w"> </span>&lt;executable&gt;
</pre></div>
</div>
<p>The second example generates an instrumened executable using a <code class="docutils literal notranslate"><span class="pre">hipcc</span></code> build:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>module<span class="w"> </span>load<span class="w"> </span>perftools
module<span class="w"> </span>load<span class="w"> </span>craype-accel-amd-gfx90a
module<span class="w"> </span>load<span class="w"> </span>rocm

<span class="nb">export</span><span class="w"> </span><span class="nv">CXX</span><span class="o">=</span><span class="s1">&#39;hipcc&#39;</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">CXXFLAGS</span><span class="o">=</span><span class="s2">&quot;</span><span class="k">$(</span>pat_opts<span class="w"> </span>include<span class="w"> </span>hipcc<span class="k">)</span><span class="s2"> \</span>
<span class="s2">  </span><span class="k">$(</span>pat_opts<span class="w"> </span>pre_compile<span class="w"> </span>hipcc<span class="k">)</span><span class="s2"> -g -O3 -std=c++17 -Wall \</span>
<span class="s2">  --offload-arch=gfx90a -I</span><span class="si">${</span><span class="nv">CRAY_MPICH_DIR</span><span class="si">}</span><span class="s2">/include \</span>
<span class="s2">  </span><span class="k">$(</span>pat_opts<span class="w"> </span>post_compile<span class="w"> </span>hipcc<span class="k">)</span><span class="s2">&quot;</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">LD</span><span class="o">=</span><span class="s1">&#39;hipcc&#39;</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">LDFLAGS</span><span class="o">=</span><span class="s2">&quot;</span><span class="k">$(</span>pat_opts<span class="w"> </span>pre_link<span class="w"> </span>hipcc<span class="k">)</span><span class="s2"> </span><span class="si">${</span><span class="nv">CXXFLAGS</span><span class="si">}</span><span class="s2"> \</span>
<span class="s2">  -L</span><span class="si">${</span><span class="nv">CRAY_MPICH_DIR</span><span class="si">}</span><span class="s2">/lib </span><span class="si">${</span><span class="nv">PE_MPICH_GTL_DIR_amd_gfx908</span><span class="si">}</span><span class="s2">&quot;</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">LIBS</span><span class="o">=</span><span class="s2">&quot;-lmpi </span><span class="si">${</span><span class="nv">PE_MPICH_GTL_LIBS_amd_gfx908</span><span class="si">}</span><span class="s2"> \</span>
<span class="s2">  </span><span class="k">$(</span>pat_opts<span class="w"> </span>post_link<span class="w"> </span>hipcc<span class="k">)</span><span class="s2">&quot;</span>

make<span class="w"> </span>clean
make

pat_build<span class="w"> </span>-g<span class="w"> </span>hip,io,mpi<span class="w"> </span>-w<span class="w"> </span>-f<span class="w"> </span>&lt;executable&gt;
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">pat_build</span></code> command in the above examples generates an instrumented executable with <code class="docutils literal notranslate"><span class="pre">+pat</span></code> appended to the executable name (e.g., <code class="docutils literal notranslate"><span class="pre">hello_jobstep+pat</span></code>).</p>
<p>When run, the instrumented executable will trace HIP, I/O, MPI, and all user functions and generate a folder of results (e.g., <code class="docutils literal notranslate"><span class="pre">hello_jobstep+pat+39545-2t</span></code>).</p>
<p>To analyze these results, use the <code class="docutils literal notranslate"><span class="pre">pat_report</span></code> command, e.g.:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pat_report<span class="w"> </span>hello_jobstep+pat+39545-2t
</pre></div>
</div>
<p>The resulting report includes profiles of functions, profiles of maximum function times, details on load imbalance, details on program energy and power usages, details on memory high water mark, and more.</p>
<p>More detailed information on the HPE Performance Analysis Tools can be found in the <a class="reference external" href="https://support.hpe.com/hpesc/public/docDisplay?docLocale=en_US&amp;docId=a00123563en_us" target="_blank">HPE Performance Analysis Tools User Guide</a>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When using <code class="docutils literal notranslate"><span class="pre">perftools-lite-gpu</span></code>, there is a known issue causing <code class="docutils literal notranslate"><span class="pre">ld.lld</span></code> not to be found. A workaround this issue can be found <a class="reference external" href="https://docs.olcf.ornl.gov/systems/crusher_quick_start_guide.html#olcfdev-513-error-with-perftools-lite-gpu" target="_blank">here</a>.</p>
</div>
</section>
<section id="getting-started-with-hpctoolkit">
<h3>Getting Started with HPCToolkit<a class="headerlink" href="#getting-started-with-hpctoolkit" title="Link to this heading"></a></h3>
<p>HPCToolkit is an integrated suite of tools for measurement and analysis of program performance on computers ranging from multicore desktop systems to the nations largest supercomputers. HPCToolkit provides accurate measurements of a programs work, resource consumption, and inefficiency, correlates these metrics with the programs source code, works with multilingual, fully optimized binaries, has very low measurement overhead, and scales to large parallel systems. HPCToolkits measurements provide support for analyzing a program execution cost, inefficiency, and scaling characteristics both within and across nodes of a parallel system.</p>
<p>Programming models supported by HPCToolkit include MPI, OpenMP, OpenACC, CUDA, OpenCL, DPC++, HIP, RAJA, Kokkos, and others.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>On Frontier, currently HPCToolkit is provided as part of the User Managed Software (UMS) program.
To see currently available builds, first perform the <code class="docutils literal notranslate"><span class="pre">module</span> <span class="pre">load</span> <span class="pre">ums</span> <span class="pre">ums023</span></code> command.
A full list of available HPCToolkit versions can also be seen with the <code class="docutils literal notranslate"><span class="pre">module</span> <span class="pre">spider</span> <span class="pre">hpctoolkit</span></code> command.</p>
</div>
<p>Below is an example that generates a profile and loads the results in their GUI-based viewer.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>module<span class="w"> </span>load<span class="w"> </span>ums<span class="w"> </span>ums023<span class="w"> </span>hpctoolkit

<span class="c1"># 1. Profile and trace an application using CPU time and GPU performance counters</span>
srun<span class="w"> </span>&lt;srun_options&gt;<span class="w"> </span>hpcrun<span class="w"> </span>-o<span class="w"> </span>&lt;measurement_dir&gt;<span class="w"> </span>-t<span class="w"> </span>-e<span class="w"> </span>CPUTIME<span class="w"> </span>-e<span class="w"> </span><span class="nv">gpu</span><span class="o">=</span>amd<span class="w"> </span>&lt;application&gt;

<span class="c1"># 2. Analyze the binary of executables and its dependent libraries</span>
hpcstruct<span class="w"> </span>&lt;measurement_dir&gt;

<span class="c1"># 3. Combine measurements with program structure information and generate a database</span>
hpcprof<span class="w"> </span>-o<span class="w"> </span>&lt;database_dir&gt;<span class="w"> </span>&lt;measurement_dir&gt;

<span class="c1"># 4. Understand performance issues by analyzing profiles and traces with the GUI</span>
hpcviewer<span class="w"> </span>&lt;database_dir&gt;
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>At the moment, <code class="docutils literal notranslate"><span class="pre">hpcviewer</span></code> requires SSH X11 forwarding to work on Frontier login nodes, which might be prohibitive depending on the users network connection. Alternatively, the user can <a class="reference external" href="http://hpctoolkit.org/download.html" target="_blank">download the HPCViewer client from the HPCToolkit website</a>, install it on their local laptop/workstation system, and then transfer the database directory to the local system for local viewing and analysis.
A remote client interface to allow streaming the performance database directly from Frontier is in active development.</p>
</div>
<p>More detailed information on HPCToolkit can be found in the <a class="reference external" href="http://hpctoolkit.org/manual/HPCToolkit-users-manual.pdf" target="_blank">HPCToolkit Users Manual</a>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>HPCToolkit does not require a recompile to profile the code. It is recommended to use the <code class="docutils literal notranslate"><span class="pre">-g</span></code> optimization flag for attribution to source lines.
There is experimental support for profiling Python applications with HPCToolkit. Please submit an OLCF ticket to get in touch with the HPCToolkit team since this might require a special build of HPCToolkit on a case-by-case basis.</p>
</div>
</section>
<section id="getting-started-with-the-rocm-profiler">
<h3>Getting Started with the ROCm Profiler<a class="headerlink" href="#getting-started-with-the-rocm-profiler" title="Link to this heading"></a></h3>
<p><code class="docutils literal notranslate"><span class="pre">rocprof</span></code> gathers metrics on kernels run on AMD GPU architectures. The profiler works for HIP kernels, as well as offloaded kernels from OpenMP target offloading, OpenCL, and abstraction layers such as Kokkos.
For a simple view of kernels being run, <code class="docutils literal notranslate"><span class="pre">rocprof</span> <span class="pre">--stats</span> <span class="pre">--timestamp</span> <span class="pre">on</span></code> is a great place to start.
With the <code class="docutils literal notranslate"><span class="pre">--stats</span></code> option enabled, <code class="docutils literal notranslate"><span class="pre">rocprof</span></code> will generate a file that is named <code class="docutils literal notranslate"><span class="pre">results.stats.csv</span></code> by default, but named <code class="docutils literal notranslate"><span class="pre">&lt;output&gt;.stats.csv</span></code> if the <code class="docutils literal notranslate"><span class="pre">-o</span></code> flag is supplied.
This file will list all kernels being run, the number of times they are run, the total duration and the average duration (in nanoseconds) of the kernel, and the GPU usage percentage.
More detailed infromation on <code class="docutils literal notranslate"><span class="pre">rocprof</span></code> profiling modes can be found at <a class="reference external" href="https://rocmdocs.amd.com/en/latest/ROCm_Tools/ROCm-Tools.html" target="_blank">ROCm Profiler</a> documentation.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you are using <code class="docutils literal notranslate"><span class="pre">sbcast</span></code>, you need to explicitly <code class="docutils literal notranslate"><span class="pre">sbcast</span></code> the AQL profiling library found in <code class="docutils literal notranslate"><span class="pre">${ROCM_PATH}/hsa-amd-aqlprofile/lib/libhsa-amd-aqlprofile64.so</span></code>.
A symbolic link to this library can also be found in <code class="docutils literal notranslate"><span class="pre">${ROCM_PATH}/lib</span></code>.
Alternatively, you may leave <code class="docutils literal notranslate"><span class="pre">${ROCM_PATH}/lib</span></code> in your <code class="docutils literal notranslate"><span class="pre">LD_LIBRARY_PATH</span></code>.</p>
</div>
</section>
<section id="roofline-profiling-with-the-rocm-profiler">
<h3>Roofline Profiling with the ROCm Profiler<a class="headerlink" href="#roofline-profiling-with-the-rocm-profiler" title="Link to this heading"></a></h3>
<p>The <a class="reference external" href="https://docs.nersc.gov/tools/performance/roofline/" target="_blank">Roofline</a> performance model is an increasingly popular way to demonstrate and understand application performance.
This section documents how to construct a simple roofline model for a single kernel using <code class="docutils literal notranslate"><span class="pre">rocprof</span></code>.
This roofline model is designed to be comparable to rooflines constructed by NVIDIAs <a class="reference external" href="https://developer.nvidia.com/blog/accelerating-hpc-applications-with-nsight-compute-roofline-analysis/" target="_blank">NSight Compute</a>.
A roofline model plots the achieved performance (in floating-point operations per second, FLOPS/s) as a function of arithmetic (or operational) intensity (in FLOPS per Byte).
The model detailed here calculates the bytes moved as they move to and from the GPUs HBM.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Integer instructions and cache levels are currently not documented here.</p>
</div>
<p>To get started, you will need to make an input file for <code class="docutils literal notranslate"><span class="pre">rocprof</span></code>, to be passed in through <code class="docutils literal notranslate"><span class="pre">rocprof</span> <span class="pre">-i</span> <span class="pre">&lt;input_file&gt;</span> <span class="pre">--timestamp</span> <span class="pre">on</span> <span class="pre">-o</span> <span class="pre">my_output.csv</span> <span class="pre">&lt;my_exe&gt;</span></code>.
Below is an example, and contains the information needed to roofline profile GPU 0, as seen by each rank:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pmc</span> <span class="p">:</span> <span class="n">TCC_EA_RDREQ_32B_sum</span> <span class="n">TCC_EA_RDREQ_sum</span> <span class="n">TCC_EA_WRREQ_sum</span> <span class="n">TCC_EA_WRREQ_64B_sum</span> <span class="n">SQ_INSTS_VALU_ADD_F16</span> <span class="n">SQ_INSTS_VALU_MUL_F16</span> <span class="n">SQ_INSTS_VALU_FMA_F16</span> <span class="n">SQ_INSTS_VALU_TRANS_F16</span> <span class="n">SQ_INSTS_VALU_ADD_F32</span> <span class="n">SQ_INSTS_VALU_MUL_F32</span> <span class="n">SQ_INSTS_VALU_FMA_F32</span> <span class="n">SQ_INSTS_VALU_TRANS_F32</span>
<span class="n">pmc</span> <span class="p">:</span> <span class="n">SQ_INSTS_VALU_ADD_F64</span> <span class="n">SQ_INSTS_VALU_MUL_F64</span> <span class="n">SQ_INSTS_VALU_FMA_F64</span> <span class="n">SQ_INSTS_VALU_TRANS_F64</span> <span class="n">SQ_INSTS_VALU_MFMA_MOPS_F16</span> <span class="n">SQ_INSTS_VALU_MFMA_MOPS_BF16</span> <span class="n">SQ_INSTS_VALU_MFMA_MOPS_F32</span> <span class="n">SQ_INSTS_VALU_MFMA_MOPS_F64</span>
<span class="n">gpu</span><span class="p">:</span> <span class="mi">0</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In an application with more than one kernel, you should strongly consider filtering by kernel name by adding a line like: <code class="docutils literal notranslate"><span class="pre">kernel:</span> <span class="pre">&lt;kernel_name&gt;</span></code> to the <code class="docutils literal notranslate"><span class="pre">rocprof</span></code> input file.</p>
</div>
<p>This provides the minimum set of metrics used to construct a roofline model, in the minimum number of passes.
Each line that begins with <code class="docutils literal notranslate"><span class="pre">pmc</span></code> indicates that the application will be re-run, and the metrics in that line will be collected.
<code class="docutils literal notranslate"><span class="pre">rocprof</span></code> can collect up to 8 counters from each block (<code class="docutils literal notranslate"><span class="pre">SQ</span></code>, <code class="docutils literal notranslate"><span class="pre">TCC</span></code>) in each application re-run.
To gather metrics across multiple MPI ranks, you will need to use a command that redirects the output of rocprof to a unique file for each task.
For example:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>srun<span class="w"> </span>-N<span class="w"> </span><span class="m">2</span><span class="w"> </span>-n<span class="w"> </span><span class="m">16</span><span class="w"> </span>--ntasks-per-node<span class="o">=</span><span class="m">8</span><span class="w"> </span>--gpus-per-node<span class="o">=</span><span class="m">8</span><span class="w"> </span>--gpu-bind<span class="o">=</span>closest<span class="w"> </span>bash<span class="w"> </span>-c<span class="w"> </span><span class="s1">&#39;rocprof -o ${SLURM_JOBID}_${SLURM_PROCID}.csv -i &lt;input_file&gt; --timestamp on &lt;exe&gt;&#39;</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The <code class="docutils literal notranslate"><span class="pre">gpu:</span></code> filter in the <code class="docutils literal notranslate"><span class="pre">rocprof</span></code> input file identifies GPUs by the number the MPI rank would see them as. In the <code class="docutils literal notranslate"><span class="pre">srun</span></code> example above,
each MPI rank only has 1 GPU, so each rank sees its GPU as GPU 0.</p>
</div>
<section id="theoretical-roofline">
<h4>Theoretical Roofline<a class="headerlink" href="#theoretical-roofline" title="Link to this heading"></a></h4>
<p>The theoretical (not attainable) peak roofline constructs a theoretical maximum performance for each operational intensity.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="docutils literal notranslate"><span class="pre">theoretical</span></code> peak is determined by the hardware specifications and is not attainable in practice. <code class="docutils literal notranslate"><span class="pre">attainable</span></code> peak is the performance as measured by
in-situ microbenchmarks designed to best utilize the hardware. <code class="docutils literal notranslate"><span class="pre">achieved</span></code> performance is what the profiled application actually achieves.</p>
</div>
<p>The theoretical roofline can be constructed as:</p>
<div class="math notranslate nohighlight">
\[FLOPS_{peak} = minimum(ArithmeticIntensity * BW_{HBM}, TheoreticalFLOPS)\]</div>
<p>On Frontier, the memory bandwidth for HBM is 1.6 TB/s, and the theoretical peak floating-point FLOPS/s when using vector registers is calculated by:</p>
<div class="math notranslate nohighlight">
\[TheoreticalFLOPS = 128 FLOP/cycle/CU * 110 CU * 1700000000 cycles/second = 23.9 TFLOP/s\]</div>
<p>However, when using MFMA instructions, the theoretical peak floating-point FLOPS/s is calculated by:</p>
<div class="math notranslate nohighlight">
\[TheoreticalFLOPS = 256 FLOP/cycle/CU * 110 CU * 1700000000 cycles/second = 47.9 TFLOP/s\]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Attainable peak rooflines are constructed using microbenchmarks, and are not currently discussed here.
Attainable rooflines consider the limitations of cooling and power consumption and are more representative of what an application can achieve.</p>
</div>
</section>
<section id="achieved-flops-s">
<h4>Achieved FLOPS/s<a class="headerlink" href="#achieved-flops-s" title="Link to this heading"></a></h4>
<p>We calculate the achieved performance at the desired level (here, double-precision floating point, FP64), by summing each metric count and weighting the FMA metric by 2, since a fused multiply-add is considered 2 floating point operations.
Also note that these <code class="docutils literal notranslate"><span class="pre">SQ_INSTS_VALU_&lt;ADD,MUL,TRANS&gt;_F64</span></code> metrics are reported as per-simd, so we mutliply by the wavefront size as well.
Similarly, the <code class="docutils literal notranslate"><span class="pre">SQ_INSTS_VALU_MFMA_MOPS_*_F64</span></code> instructions should be multiplied by 512.
We use this equation to calculate the number of double-precision FLOPS:</p>
<div class="math notranslate nohighlight">
\[\begin{split}FP64\_FLOPS =   64  *&amp;(SQ\_INSTS\_VALU\_ADD\_F64         \\\\
                     &amp;+ SQ\_INSTS\_VALU\_MUL\_F64       \\\\
                     &amp;+ SQ\_INSTS\_VALU\_TRANS\_F64     \\\\
                     &amp;+ 2 * SQ\_INSTS\_VALU\_FMA\_F64)  \\\\
              + 512 *&amp;(SQ\_INSTS\_VALU\_MFMA\_MOPS\_F64)\end{split}\]</div>
<p>When <code class="docutils literal notranslate"><span class="pre">SQ_INSTS_VALU_MFMA_MOPS_*_F64</span></code> instructions are used, then 47.8 TF/s is considered the theoretical maximum FLOPS/s.
If only <code class="docutils literal notranslate"><span class="pre">SQ_INSTS_VALU_&lt;ADD,MUL,TRANS&gt;_F64</span></code> are used, then 23.9 TF/s is the theoretical maximum FLOPS/s.
Then, we divide the number of FLOPS by the elapsed time of the kernel to find FLOPS per second.
This is found from subtracting the <code class="docutils literal notranslate"><span class="pre">rocprof</span></code> metrics <code class="docutils literal notranslate"><span class="pre">EndNs</span></code> by <code class="docutils literal notranslate"><span class="pre">BeginNs</span></code>, provided by <code class="docutils literal notranslate"><span class="pre">--timestamp</span> <span class="pre">on</span></code>, then converting from nanoseconds to seconds by dividing by 1,000,000,000 (power(10,9)).</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For ROCm/5.2.0 and earlier, there is a known issue causing the timings provided by <code class="docutils literal notranslate"><span class="pre">--timestamp</span> <span class="pre">on</span></code> to be inaccurate.</p>
</div>
<section id="calculating-for-all-precisions">
<h5>Calculating for all precisions<a class="headerlink" href="#calculating-for-all-precisions" title="Link to this heading"></a></h5>
<p>The above formula can be adapted to compute the total FLOPS across all floating-point precisions (<code class="docutils literal notranslate"><span class="pre">INT</span></code> excluded).</p>
<div class="math notranslate nohighlight">
\[\begin{split}TOTAL\_FLOPS =   64  *&amp;(SQ\_INSTS\_VALU\_ADD\_F16         \\\\
                     &amp;+ SQ\_INSTS\_VALU\_MUL\_F16       \\\\
                     &amp;+ SQ\_INSTS\_VALU\_TRANS\_F16     \\\\
                     &amp;+ 2 * SQ\_INSTS\_VALU\_FMA\_F16)  \\\\
              + 64  *&amp;(SQ\_INSTS\_VALU\_ADD\_F32         \\\\
                     &amp;+ SQ\_INSTS\_VALU\_MUL\_F32       \\\\
                     &amp;+ SQ\_INSTS\_VALU\_TRANS\_F32     \\\\
                     &amp;+ 2 * SQ\_INSTS\_VALU\_FMA\_F32)  \\\\
              + 64  *&amp;(SQ\_INSTS\_VALU\_ADD\_F64         \\\\
                     &amp;+ SQ\_INSTS\_VALU\_MUL\_F64       \\\\
                     &amp;+ SQ\_INSTS\_VALU\_TRANS\_F64     \\\\
                     &amp;+ 2 * SQ\_INSTS\_VALU\_FMA\_F64)  \\\\
              + 512 &amp;*(SQ\_INSTS\_VALU\_MFMA\_MOPS\_F16) \\\\
              + 512 &amp;*(SQ\_INSTS\_VALU\_MFMA\_MOPS\_BF16) \\\\
              + 512 *&amp;(SQ\_INSTS\_VALU\_MFMA\_MOPS\_F32) \\\\
              + 512 *&amp;(SQ\_INSTS\_VALU\_MFMA\_MOPS\_F64) \\\\\end{split}\]</div>
</section>
</section>
<section id="arithmetic-intensity">
<h4>Arithmetic Intensity<a class="headerlink" href="#arithmetic-intensity" title="Link to this heading"></a></h4>
<p>Arithmetic intensity calculates the ratio of FLOPS to bytes moved between HBM and L2 cache.
We calculated FLOPS above (<code class="docutils literal notranslate"><span class="pre">FP64_FLOPS</span></code>).
We can calculate the number of bytes moved using the <code class="docutils literal notranslate"><span class="pre">rocprof</span></code> metrics <code class="docutils literal notranslate"><span class="pre">TCC_EA_WRREQ_64B</span></code>, <code class="docutils literal notranslate"><span class="pre">TCC_EA_WRREQ_sum</span></code>, <code class="docutils literal notranslate"><span class="pre">TCC_EA_RDREQ_32B</span></code>, and <code class="docutils literal notranslate"><span class="pre">TCC_EA_RDREQ_sum</span></code>.
<code class="docutils literal notranslate"><span class="pre">TCC</span></code> refers to the L2 cache, and <code class="docutils literal notranslate"><span class="pre">EA</span></code> is the interface between L2 and HBM.
<code class="docutils literal notranslate"><span class="pre">WRREQ</span></code> and <code class="docutils literal notranslate"><span class="pre">RDREQ</span></code> are write-requests and read-requests, respectively.
Each of these requests is either 32 bytes or 64 bytes.
So we calculate the number of bytes traveling over the EA interface as:</p>
<div class="math notranslate nohighlight">
\[BytesMoved = BytesWritten + BytesRead\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[BytesWritten = 64 * TCC\_EA\_WRREQ\_64B\_sum + 32 * (TCC\_EA\_WRREQ\_sum - TCC\_EA\_WRREQ\_64B\_sum)\]</div>
<div class="math notranslate nohighlight">
\[BytesRead = 32 * TCC\_EA\_RDREQ\_32B\_sum + 64 * (TCC\_EA\_RDREQ\_sum - TCC\_EA\_RDREQ\_32B\_sum)\]</div>
</section>
</section>
<section id="omniperf">
<h3>Omniperf<a class="headerlink" href="#omniperf" title="Link to this heading"></a></h3>
<p>OLCF provides installations of AMDs <a class="reference external" href="https://github.com/AMDResearch/omniperf" target="_blank">Omniperf</a> profiling tools on Frontier.
AMD provides documentation on the usage of Omniperf at <a class="reference external" href="https://amdresearch.github.io/omniperf/" target="_blank">https://amdresearch.github.io/omniperf/</a>.
This section details the installation and common pitfalls of the <code class="docutils literal notranslate"><span class="pre">omniperf</span></code> module on Frontier.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">omniperf</span></code> module relies on two other modules  a <code class="docutils literal notranslate"><span class="pre">rocm</span></code> module and optionally a <code class="docutils literal notranslate"><span class="pre">cray-python</span></code> module.
A ROCm module must be loaded before being able to do view or load the <code class="docutils literal notranslate"><span class="pre">omniperf</span></code> module.
As for <code class="docutils literal notranslate"><span class="pre">cray-python</span></code>, <code class="docutils literal notranslate"><span class="pre">omniperf</span></code> is a Python script and has several dependencies that cannot be met by the systems default Python, and are not met by the default <code class="docutils literal notranslate"><span class="pre">cray-python</span></code> installation.
As such, you must either (1) load the <code class="docutils literal notranslate"><span class="pre">cray-python</span></code> module or (2) satisfy the Python dependencies in your own Python environment (ie, in a Conda environment).</p>
<p>As a rule of thumb, always load the <code class="docutils literal notranslate"><span class="pre">omniperf</span></code> module last (especially after you load a ROCm module).
If you load a new version of ROCm, you will need to re-load <code class="docutils literal notranslate"><span class="pre">omniperf</span></code>.</p>
<section id="using-cray-python">
<h4>Using <code class="docutils literal notranslate"><span class="pre">cray-python</span></code><a class="headerlink" href="#using-cray-python" title="Link to this heading"></a></h4>
<p>To use <code class="docutils literal notranslate"><span class="pre">omniperf</span></code> with <code class="docutils literal notranslate"><span class="pre">cray-python</span></code>, you may use the following commands:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">module</span> <span class="n">load</span> <span class="n">rocm</span>
<span class="n">module</span> <span class="n">load</span> <span class="n">cray</span><span class="o">-</span><span class="n">python</span>
<span class="n">module</span> <span class="n">load</span> <span class="n">omniperf</span>
</pre></div>
</div>
<p>No more work is needed on your part  <code class="docutils literal notranslate"><span class="pre">omniperf</span></code> points to a directory that contains pre-built libraries for the <code class="docutils literal notranslate"><span class="pre">cray-python</span></code> version you are running.
It is <strong>critically</strong> important that if you load a different version of ROCm or <code class="docutils literal notranslate"><span class="pre">cray-python</span></code> that you re-load <code class="docutils literal notranslate"><span class="pre">omniperf</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Omniperf requires relatively new versions of many dependencies.
Installing dependencies may break some currently installed packages that require older versions of the dependencies.
It is recommended that you use the newest <code class="docutils literal notranslate"><span class="pre">cray-python</span></code> modules available.</p>
</div>
</section>
<section id="using-your-own-python">
<h4>Using your own Python<a class="headerlink" href="#using-your-own-python" title="Link to this heading"></a></h4>
<p>To use <code class="docutils literal notranslate"><span class="pre">omniperf</span></code> with your own Python installation, you must first install the dependencies of Omniperf in your Pythons environment.
To do so, use the <code class="docutils literal notranslate"><span class="pre">requirements.txt</span></code> file in the <a class="reference external" href="https://github.com/AMDResearch/omniperf" target="_blank">Omniperf GitHub Repo</a>.
You may install the dependencies using a command like:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">python3</span> <span class="o">-</span><span class="n">m</span> <span class="n">pip</span> <span class="n">install</span> <span class="o">-</span><span class="n">r</span> <span class="n">requirements</span><span class="o">.</span><span class="n">txt</span>
</pre></div>
</div>
<p>Once you have installed the dependencies, you may load <code class="docutils literal notranslate"><span class="pre">omniperf</span></code> using commands like:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Your Python environment should be active by this point</span>
<span class="n">module</span> <span class="n">load</span> <span class="n">rocm</span>
<span class="n">module</span> <span class="n">load</span> <span class="n">omniperf</span>
</pre></div>
</div>
<p>Again, it is <strong>critically</strong> important that if you load a different version of ROCm that you re-load <code class="docutils literal notranslate"><span class="pre">omniperf</span></code>.</p>
<hr class="docutils" />
</section>
</section>
</section>
<section id="tips-and-tricks">
<span id="id15"></span><h2>Tips and Tricks<a class="headerlink" href="#tips-and-tricks" title="Link to this heading"></a></h2>
<p>This section details tips and tricks and information of interest to users when porting from Summit to Frontier.</p>
<section id="using-reduced-precision-fp16-and-bf16-datatypes">
<span id="using-reduced-precision"></span><h3>Using reduced precision (FP16 and BF16 datatypes)<a class="headerlink" href="#using-reduced-precision-fp16-and-bf16-datatypes" title="Link to this heading"></a></h3>
<p>Users leveraging BF16 and FP16 datatypes for applications such as ML/AI training and low-precision matrix multiplication should be aware that the AMD MI250X GPU has different denormal handling than the V100 GPUs on Summit. On the MI250X, the V_DOT2 and the matrix instructions for FP16 and BF16 flush input and output denormal values to zero.FP32 and FP64 MFMA instructions do not flush input and output denormal values to zero.</p>
<p>When training deep learning models using FP16 precision, some models may fail to converge with FP16 denorms flushed to zero. This occurs in operations encountering denormal values, and so is more likely to occur in FP16 because of a small dynamic range. BF16 numbers have a larger dynamic range than FP16 numbers and are less likely to encounter denormal values.</p>
<p>AMD has provided a solution in ROCm 5.0 which modifies the behavior of Tensorflow, PyTorch, and rocBLAS. This modification starts with FP16 input values, casting the intermediate FP16 values to BF16, and then casting back to FP16 output after the accumulate FP32 operations. In this way, the input and output types are unchanged. The behavior is enabled by default in machine learning frameworks. This behavior requires user action in rocBLAS, via a special enum type. For more information, see the rocBLAS link below.</p>
<p>If you encounter significant differences when running using reduced precision, explore replacing non-converging models in FP16 with BF16, because of the greater dynamic range in BF16. We recommend using BF16 for ML models in general. If you have further questions or encounter issues, contact <a class="reference external" href="mailto:help&#37;&#52;&#48;olcf&#46;ornl&#46;gov" target="_blank">help<span>&#64;</span>olcf<span>&#46;</span>ornl<span>&#46;</span>gov</a>.</p>
<dl class="simple">
<dt>Additional information on MI250X reduced precision can be found at:</dt><dd><ul class="simple">
<li><p>The MI250X ISA specification details the flush to zero denorm behavior at: <a class="reference external" href="https://developer.amd.com/wp-content/resources/CDNA2_Shader_ISA_18November2021.pdf" target="_blank">https://developer.amd.com/wp-content/resources/CDNA2_Shader_ISA_18November2021.pdf</a> (See page 41 and 46)</p></li>
<li><p>AMD rocBLAS library reference guide details this behavior at: <a class="reference external" href="https://rocblas.readthedocs.io/en/master/API_Reference_Guide.html#mi200-gfx90a-considerations" target="_blank">https://rocblas.readthedocs.io/en/master/API_Reference_Guide.html#mi200-gfx90a-considerations</a></p></li>
</ul>
</dd>
</dl>
</section>
<section id="enabling-gpu-page-migration">
<span id="id16"></span><h3>Enabling GPU Page Migration<a class="headerlink" href="#enabling-gpu-page-migration" title="Link to this heading"></a></h3>
<p>The AMD MI250X and operating system on Frontier supports unified virtual addressing across the entire host and device memory, and automatic page migration between CPU and GPU memory. Migratable, universally addressable memory is sometimes called managed or unified memory, but neither of these terms fully describes how memory may behave on Frontier. In the following section well discuss how the heterogenous memory space on a Frontier node is surfaced within your application.</p>
<p>The accessibility of memory from GPU kernels and whether pages may migrate depends three factors: how the memory was allocated; the XNACK operating mode of the GPU; whether the kernel was compiled to support page migration. The latter two factors are intrinsically linked, as the MI250X GPU operating mode restricts the types of kernels which may run.</p>
<p>XNACK (pronounced X-knack) refers to the AMD GPUs ability to retry memory accesses that fail due to a page fault. The XNACK mode of an MI250X can be changed by setting the environment variable <code class="docutils literal notranslate"><span class="pre">HSA_XNACK</span></code> before starting a process that uses the GPU. Valid values are 0 (disabled) and 1 (enabled), and all processes connected to a GPU must use the same XNACK setting. The default MI250X on Frontier is <code class="docutils literal notranslate"><span class="pre">HSA_XNACK=0</span></code>.</p>
<p>If <code class="docutils literal notranslate"><span class="pre">HSA_XNACK=0</span></code>, page faults in GPU kernels are not handled and will terminate the kernel. Therefore all memory locations accessed by the GPU must either be resident in the GPU HBM or mapped by the HIP runtime. Memory regions may be migrated between the host DDR4 and GPU HBM using explicit HIP library functions such as <code class="docutils literal notranslate"><span class="pre">hipMemAdvise</span></code> and <code class="docutils literal notranslate"><span class="pre">hipPrefetchAsync</span></code>, but memory will not be automatically migrated based on access patterns alone.</p>
<p>If <code class="docutils literal notranslate"><span class="pre">HSA_XNACK=1</span></code>, page faults in GPU kernels will trigger a page table lookup. If the memory location can be made accessible to the GPU, either by being migrated to GPU HBM or being mapped for remote access, the appropriate action will occur and the access will be replayed. Page migration  will happen between CPU DDR4 and GPU HBM according to page touch. The exceptions are if the programmer uses a HIP library call such as <code class="docutils literal notranslate"><span class="pre">hipPrefetchAsync</span></code> to request migration, or if a preferred location is set via <code class="docutils literal notranslate"><span class="pre">hipMemAdvise</span></code>, or if GPU HBM becomes full and the page must forcibly be evicted back to CPU DDR4 to make room for other data.</p>
<section id="migration-of-memory-by-allocator-and-xnack-mode">
<span id="migration-of-memory-allocator-xnack"></span><h4>Migration of Memory by Allocator and XNACK Mode<a class="headerlink" href="#migration-of-memory-by-allocator-and-xnack-mode" title="Link to this heading"></a></h4>
<p>Most applications that use managed or unified memory on other platforms will want to enable XNACK to take advantage of automatic page migration on Frontier. The following table shows how common allocators currently behave with XNACK enabled. The behavior of a specific memory region may vary from the default if the programmer uses certain API calls.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The page migration behavior summarized by the following tables represents the current, observable behavior. Said behavior will likely change in the near future.</p>
<blockquote>
<div></div></blockquote>
</div>
<p><code class="docutils literal notranslate"><span class="pre">HSA_XNACK=1</span></code> <strong>Automatic Page Migration Enabled</strong></p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Allocator</p></th>
<th class="head"><p>Initial Physical Location</p></th>
<th class="head"><p>CPU Access after GPU First Touch</p></th>
<th class="head"><p>Default Behavior for GPU Access</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>System Allocator (malloc,new,allocate, etc)</p></td>
<td><p>CPU DDR4</p></td>
<td><p>Migrate to CPU DDR4 on touch</p></td>
<td><p>Migrate to GPU HBM on touch</p></td>
</tr>
<tr class="row-odd"><td><p>hipMallocManaged</p></td>
<td><p>CPU DDR4</p></td>
<td><p>Migrate to CPU DDR4 on touch</p></td>
<td><p>Migrate to GPU HBM on touch</p></td>
</tr>
<tr class="row-even"><td><p>hipHostMalloc</p></td>
<td><p>CPU DDR4</p></td>
<td><p>Local read/write</p></td>
<td><p>Zero copy read/write over Infinity Fabric</p></td>
</tr>
<tr class="row-odd"><td><p>hipMalloc</p></td>
<td><p>GPU HBM</p></td>
<td><p>Zero copy read/write over Inifinity Fabric</p></td>
<td><p>Local read/write</p></td>
</tr>
</tbody>
</table>
<p>Disabling XNACK will not necessarily result in an application failure, as most types of memory can still be accessed by the AMD Optimized 3rd Gen EPYC CPU and AMD MI250X GPU. In most cases, however, the access will occur in a zero-copy fashion over the Infinity Fabric. The exception is memory allocated through standard system allocators such as <code class="docutils literal notranslate"><span class="pre">malloc</span></code>, which cannot be accessed directly from GPU kernels without previously being registered via a HIP runtime call such as <code class="docutils literal notranslate"><span class="pre">hipHostRegister</span></code>. Access to malloced and unregistered memory from GPU kernels will result in fatal unhandled page faults. The table below shows how common allocators behave with XNACK disabled.</p>
<p><code class="docutils literal notranslate"><span class="pre">HSA_XNACK=0</span></code> <strong>Automatic Page Migration Disabled</strong></p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Allocator</p></th>
<th class="head"><p>Initial Physical Location</p></th>
<th class="head"><p>Default Behavior for CPU Access</p></th>
<th class="head"><p>Default Behavior for GPU Access</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>System Allocator (malloc,new,allocate, etc)</p></td>
<td><p>CPU DDR4</p></td>
<td><p>Local read/write</p></td>
<td><p>Fatal Unhandled Page Fault</p></td>
</tr>
<tr class="row-odd"><td><p>hipMallocManaged</p></td>
<td><p>CPU DDR4</p></td>
<td><p>Local read/write</p></td>
<td><p>Zero copy read/write over Infinity Fabric</p></td>
</tr>
<tr class="row-even"><td><p>hipHostMalloc</p></td>
<td><p>CPU DDR4</p></td>
<td><p>Local read/write</p></td>
<td><p>Zero copy read/write over Infinity Fabric</p></td>
</tr>
<tr class="row-odd"><td><p>hipMalloc</p></td>
<td><p>GPU HBM</p></td>
<td><p>Zero copy read/write over Inifinity Fabric</p></td>
<td><p>Local read/write</p></td>
</tr>
</tbody>
</table>
</section>
<section id="compiling-hip-kernels-for-specific-xnack-modes">
<span id="compiling-hip-kernels-for-xnack-modes"></span><h4>Compiling HIP kernels for specific XNACK modes<a class="headerlink" href="#compiling-hip-kernels-for-specific-xnack-modes" title="Link to this heading"></a></h4>
<p>Although XNACK is a capability of the MI250X GPU, it does require that kernels be able to recover from page faults. Both the ROCm and CCE HIP compilers will default to generating code that runs correctly with both XNACK enabled and disabled. Some applications may benefit from using the following compilation options to target specific XNACK modes.</p>
<div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">hipcc</span> <span class="pre">--offload-arch=gfx90a</span></code> or <code class="docutils literal notranslate"><span class="pre">CC</span> <span class="pre">--offload-arch=gfx90a</span> <span class="pre">-x</span> <span class="pre">hip</span></code></div>
<div class="line-block">
<div class="line">Kernels are compiled to a single xnack any binary, which will run correctly with both XNACK enabled and XNACK disabled.</div>
</div>
</div>
<div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">hipcc</span> <span class="pre">--offload-arch=gfx90a:xnack+</span></code> or <code class="docutils literal notranslate"><span class="pre">CC</span> <span class="pre">--offload-arch=gfx90a:xnack+</span> <span class="pre">-x</span> <span class="pre">hip</span></code></div>
<div class="line-block">
<div class="line">Kernels are compiled in xnack plus mode and will <em>only</em> be able to run on GPUs with <code class="docutils literal notranslate"><span class="pre">HSA_XNACK=1</span></code> to enable XNACK. Performance may be better than xnack any, but attempts to run with XNACK disabled will fail.</div>
</div>
</div>
<div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">hipcc</span> <span class="pre">--offload-arch=gfx90a:xnack-</span></code> or <code class="docutils literal notranslate"><span class="pre">CC</span> <span class="pre">--offload-arch=gfx90a:xnack-</span> <span class="pre">-x</span> <span class="pre">hip</span></code></div>
<div class="line-block">
<div class="line">Kernels are compiled in xnack minus mode and will <em>only</em> be able to run on GPUs with <code class="docutils literal notranslate"><span class="pre">HSA_XNACK=0</span></code> and XNACK disabled. Performance may be better than xnack any, but attempts to run with XNACK enabled will fail.</div>
</div>
</div>
<div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">hipcc</span> <span class="pre">--offload-arch=gfx90a:xnack-</span> <span class="pre">--offload-arch=gfx90a:xnack+</span> <span class="pre">-x</span> <span class="pre">hip</span></code> or <code class="docutils literal notranslate"><span class="pre">CC</span> <span class="pre">--offload-arch=gfx90a:xnack-</span> <span class="pre">--offload-arch=gfx90a:xnack+</span> <span class="pre">-x</span> <span class="pre">hip</span></code></div>
<div class="line-block">
<div class="line">Two versions of each kernel will be generated, one that runs with XNACK disabled and one that runs if XNACK is enabled. This is different from xnack any in that two versions of each kernel are compiled and HIP picks the appropriate one at runtime, rather than there being a single version compatible with both. A fat binary compiled in this way will have the same performance of xnack+ with <code class="docutils literal notranslate"><span class="pre">HSA_XNACK=1</span></code> and as xnack- with <code class="docutils literal notranslate"><span class="pre">HSA_XNACK=0</span></code>, but the final executable will be larger since it contains two copies of every kernel.</div>
</div>
</div>
<p>If the HIP runtime cannot find a kernel image that matches the XNACK mode of the device, it will fail with <code class="docutils literal notranslate"><span class="pre">hipErrorNoBinaryForGpu</span></code>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ HSA_XNACK=0 srun -n 1 -N 1 -t 1 ./xnack_plus.exe
&quot;hipErrorNoBinaryForGpu: Unable to find code object for all current devices!&quot;
srun: error: frontier002: task 0: Aborted
srun: launch/slurm: _step_signal: Terminating StepId=74100.0
</pre></div>
</div>
<p>One way to diagnose <code class="docutils literal notranslate"><span class="pre">hipErrorNoBinaryForGpu</span></code> messages is to set the environment variable <code class="docutils literal notranslate"><span class="pre">AMD_LOG_LEVEL</span></code> to 1 or greater:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ AMD_LOG_LEVEL=1 HSA_XNACK=0 srun -n 1 -N 1 -t 1 ./xnack_plus.exe
:1:rocdevice.cpp            :1573: 43966598070 us: HSA_AMD_AGENT_INFO_SVM_DIRECT_HOST_ACCESS query failed.
:1:rocdevice.cpp            :1573: 43966598762 us: HSA_AMD_AGENT_INFO_SVM_DIRECT_HOST_ACCESS query failed.
:1:rocdevice.cpp            :1573: 43966599392 us: HSA_AMD_AGENT_INFO_SVM_DIRECT_HOST_ACCESS query failed.
:1:rocdevice.cpp            :1573: 43966599970 us: HSA_AMD_AGENT_INFO_SVM_DIRECT_HOST_ACCESS query failed.
:1:rocdevice.cpp            :1573: 43966600550 us: HSA_AMD_AGENT_INFO_SVM_DIRECT_HOST_ACCESS query failed.
:1:rocdevice.cpp            :1573: 43966601109 us: HSA_AMD_AGENT_INFO_SVM_DIRECT_HOST_ACCESS query failed.
:1:rocdevice.cpp            :1573: 43966601673 us: HSA_AMD_AGENT_INFO_SVM_DIRECT_HOST_ACCESS query failed.
:1:rocdevice.cpp            :1573: 43966602248 us: HSA_AMD_AGENT_INFO_SVM_DIRECT_HOST_ACCESS query failed.
:1:hip_code_object.cpp      :460 : 43966602806 us: hipErrorNoBinaryForGpu: Unable to find code object for all current devices!
:1:hip_code_object.cpp      :461 : 43966602810 us:   Devices:
:1:hip_code_object.cpp      :464 : 43966602811 us:     amdgcn-amd-amdhsa--gfx90a:sramecc+:xnack- - [Not Found]
:1:hip_code_object.cpp      :464 : 43966602811 us:     amdgcn-amd-amdhsa--gfx90a:sramecc+:xnack- - [Not Found]
:1:hip_code_object.cpp      :464 : 43966602812 us:     amdgcn-amd-amdhsa--gfx90a:sramecc+:xnack- - [Not Found]
:1:hip_code_object.cpp      :464 : 43966602813 us:     amdgcn-amd-amdhsa--gfx90a:sramecc+:xnack- - [Not Found]
:1:hip_code_object.cpp      :464 : 43966602813 us:     amdgcn-amd-amdhsa--gfx90a:sramecc+:xnack- - [Not Found]
:1:hip_code_object.cpp      :464 : 43966602814 us:     amdgcn-amd-amdhsa--gfx90a:sramecc+:xnack- - [Not Found]
:1:hip_code_object.cpp      :464 : 43966602814 us:     amdgcn-amd-amdhsa--gfx90a:sramecc+:xnack- - [Not Found]
:1:hip_code_object.cpp      :464 : 43966602815 us:     amdgcn-amd-amdhsa--gfx90a:sramecc+:xnack- - [Not Found]
:1:hip_code_object.cpp      :468 : 43966602816 us:   Bundled Code Objects:
:1:hip_code_object.cpp      :485 : 43966602817 us:     host-x86_64-unknown-linux - [Unsupported]
:1:hip_code_object.cpp      :483 : 43966602818 us:     hipv4-amdgcn-amd-amdhsa--gfx90a:xnack+ - [code object v4 is amdgcn-amd-amdhsa--gfx90a:xnack+]
&quot;hipErrorNoBinaryForGpu: Unable to find code object for all current devices!&quot;
srun: error: frontier129: task 0: Aborted
srun: launch/slurm: _step_signal: Terminating StepId=74102.0
</pre></div>
</div>
<p>The above log messages indicate the type of image required by each device, given its current mode (<code class="docutils literal notranslate"><span class="pre">amdgcn-amd-amdhsa--gfx90a:sramecc+:xnack-</span></code>) and the images found in the binary (<code class="docutils literal notranslate"><span class="pre">hipv4-amdgcn-amd-amdhsa--gfx90a:xnack+</span></code>).</p>
<hr class="docutils" />
</section>
</section>
<section id="floating-point-fp-atomic-operations-and-coarse-fine-grained-memory-allocations">
<span id="fp-atomic-ops-coarse-fine-allocations"></span><h3>Floating-Point (FP) Atomic Operations and Coarse/Fine Grained Memory Allocations<a class="headerlink" href="#floating-point-fp-atomic-operations-and-coarse-fine-grained-memory-allocations" title="Link to this heading"></a></h3>
<p>The Frontier system, equipped with CDNA2-based architecture MI250X cards, offers a coherent host interface that enables advanced memory and unique cache coherency capabilities.
The AMD driver leverages the Heterogeneous Memory Management (HMM) support in the Linux kernel to perform seamless page migrations to/from CPU/GPUs.
This new capability comes with a memory model that needs to be understood completely to avoid unexpected behavior in real applications. For more details, please visit the previous section.</p>
<p>AMD GPUs can allocate two different types of memory locations: 1) Coarse grained and 2) Fine grained.</p>
<p><strong>Coarse grained</strong> memory is only guaranteed to be coherent outside of GPU kernels that modify it, enabling higher performance memory operations. Changes applied to coarse-grained memory by a GPU kernel are  only visible to the rest of the system (CPU or other GPUs) when the kernel has completed. A GPU kernel is only guaranteed to see changes applied to coarse grained memory by the rest of the system (CPU or other GPUs) if those changes were made before the kernel launched.</p>
<p><strong>Fine grained</strong> memory allows CPUs and GPUs to synchronize (via atomics) and coherently communicate with each other while the GPU kernel is running, allowing more advanced programming patterns. The additional visibility impacts the performance of fine grained allocated memory.</p>
<p>The fast hardware-based Floating point (FP) atomic operations available on MI250X are assumed to be working on coarse grained memory regions; when these instructions are applied to a fine-grained memory region, they will silently produce a no-op. To avoid returning incorrect results, the compiler never emits hardware-based FP atomics instructions by default, even when applied to coarse grained memory regions. Currently, users can use the <cite>-munsafe-fp-atomics</cite> flag to force the compiler to emit hardware-based FP atomics.
Using hardware-based FP atomics translates in a substantial performance improvement over the default choice.</p>
<p>Users applying floating point atomic operations (e.g., atomicAdd) on memory regions allocated via regular hipMalloc() can safely apply the <cite>-munsafe-fp-atomics</cite> flags to their codes to get the best possible performance and leverage hardware supported floating point atomics.
Atomic operations supported in hardware on non-FP datatypes  (e.g., INT32) will work correctly regardless of the nature of the memory region used.</p>
<p>In ROCm-5.1 and earlier versions, the flag <cite>-munsafe-fp-atomics</cite> is interpreted as a suggestion by the compiler, whereas from ROCm-5.2 the flag will always enforce the use of fast hardware-based FP atomics.</p>
<p>The following tables summarize the result granularity of various combinations of allocators, flags and arguments.</p>
<p>For <code class="docutils literal notranslate"><span class="pre">hipHostMalloc()</span></code>, the following table shows the nature of the memory returned based on the flag passed as argument.</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>API</p></th>
<th class="head"><p>Flag</p></th>
<th class="head"><p>Results</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>hipHostMalloc()</p></td>
<td><p>hipHostMallocDefault</p></td>
<td><p>Fine grained</p></td>
</tr>
<tr class="row-odd"><td><p>hipHostMalloc()</p></td>
<td><p>hipHostMallocNonCoherent</p></td>
<td><p>Coarse grained</p></td>
</tr>
</tbody>
</table>
<p>The following table shows the nature of the memory returned based on the flag passed as argument to <code class="docutils literal notranslate"><span class="pre">hipExtMallocWithFlags()</span></code>.</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>API</p></th>
<th class="head"><p>Flag</p></th>
<th class="head"><p>Result</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>hipExtMallocWithFlags()</p></td>
<td><p>hipDeviceMallocDefault</p></td>
<td><p>Coarse grained</p></td>
</tr>
<tr class="row-odd"><td><p>hipExtMallocWithFlags()</p></td>
<td><p>hipDeviceMallocFinegrained</p></td>
<td><p>Fine grained</p></td>
</tr>
</tbody>
</table>
<p>Finally, the following table summarizes the nature of the memory returned based on the flag passed as argument to <code class="docutils literal notranslate"><span class="pre">hipMallocManaged()</span></code> and the use of CPU regular <code class="docutils literal notranslate"><span class="pre">malloc()</span></code> routine with the possible use of <code class="docutils literal notranslate"><span class="pre">hipMemAdvise()</span></code>.</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>API</p></th>
<th class="head"><p>MemAdvice</p></th>
<th class="head"><p>Result</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>hipMallocManaged()</p></td>
<td></td>
<td><p>Fine grained</p></td>
</tr>
<tr class="row-odd"><td><p>hipMallocManaged()</p></td>
<td><p>hipMemAdvise (hipMemAdviseSetCoarseGrain)</p></td>
<td><p>Coarse grained</p></td>
</tr>
<tr class="row-even"><td><p>malloc()</p></td>
<td></td>
<td><p>Fine grained</p></td>
</tr>
<tr class="row-odd"><td><p>malloc()</p></td>
<td><p>hipMemAdvise (hipMemAdviseSetCoarseGrain)</p></td>
<td><p>Coarse grained</p></td>
</tr>
</tbody>
</table>
</section>
<section id="performance-considerations-for-lds-fp-atomicadd">
<span id="performance-lds-atomicadd"></span><h3>Performance considerations for LDS FP atomicAdd()<a class="headerlink" href="#performance-considerations-for-lds-fp-atomicadd" title="Link to this heading"></a></h3>
<p>Hardware FP atomic operations performed in LDS memory are usually always faster than an equivalent CAS loop, in particular when contention on LDS memory locations is high.
Because of a hardware design choice, FP32 LDS atomicAdd() operations can be slower than equivalent FP64 LDS atomicAdd(), in particular when contention on memory locations is low (e.g. random access pattern).
The aforementioned behavior is only true for FP atomicAdd() operations. Hardware atomic operations for CAS/Min/Max on FP32 are usually faster than the FP64 counterparts.
In cases when contention is very low, a FP32 CAS loop implementing an atomicAdd() operation could be faster than an hardware FP32 LDS atomicAdd().
Applications using single precision FP atomicAdd() are encouraged to experiment with the use of double precision to evaluate the trade-off between high atomicAdd() performance vs. potential lower occupancy due to higher LDS usage.</p>
</section>
<section id="library-considerations-with-atomic-operations">
<h3>Library considerations with atomic operations<a class="headerlink" href="#library-considerations-with-atomic-operations" title="Link to this heading"></a></h3>
<p>Some functionality provided by the rocBLAS and hipBLAS libraries use atomic operations to improve performance by default. This can cause results to not be bit-wise reproducible.
Level 2 functions that may use atomic operations include: gemv, hemv, and symv, which introduced atomic operations in ROCm 5.5. All of the Level 3 functions, along with Level 2 trsv, may use atomic operations where dependent
on gemm. Atomic operations are used for problem sizes where they are shown to improve performance.
If it is necessary to have bit-wise reproducible results from these libraries, it is recommended to turn the atomic operations off by setting the mode via the rocBLAS or hipBLAS handle:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="p">...</span>
<span class="n">rocblas_create_handle</span><span class="p">(</span><span class="n">handle</span><span class="p">);</span>
<span class="n">rocblas_set_atomics_mode</span><span class="p">(</span><span class="n">handle</span><span class="p">,</span><span class="w"> </span><span class="n">rocblas_atomics_not_allowed</span><span class="p">);</span>

<span class="n">hipblasCreate</span><span class="p">(</span><span class="o">&amp;</span><span class="n">handle</span><span class="p">);</span>
<span class="n">hipblasSetAtomicsMode</span><span class="p">(</span><span class="n">handle</span><span class="p">,</span><span class="w"> </span><span class="n">HIPBLAS_ATOMICS_NOT_ALLOWED</span><span class="p">);</span>
</pre></div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="system-updates">
<h2>System Updates<a class="headerlink" href="#system-updates" title="Link to this heading"></a></h2>
<section id="id17">
<h3>2024-09-03<a class="headerlink" href="#id17" title="Link to this heading"></a></h3>
<p>On Tuesday, September 3, 2024, Frontiers system software was upgraded to Slingshot 2.2.0. Please report any issues to <a class="reference external" href="mailto:help&#37;&#52;&#48;olcf&#46;ornl&#46;gov" target="_blank">help<span>&#64;</span>olcf<span>&#46;</span>ornl<span>&#46;</span>gov</a>.</p>
</section>
<section id="id18">
<h3>2024-08-20<a class="headerlink" href="#id18" title="Link to this heading"></a></h3>
<p>On Tuesday, August 20, 2024, Frontiers system software will be upgraded.</p>
<p>The following system changes will take place:</p>
<ul class="simple">
<li><p>Upgrade to AMD GPU 6.8.5 device driver (ROCm 6.2.0 release).</p></li>
<li><p>Upgrade to Slingshot Host Software 2.2.0. This changes the libfabric version from 1.15.2.0 to 1.20.1.0 and changes the location of the shared libraries from <cite>/opt/cray/libfabric/1.15.2.0</cite> to <cite>/usr/lib64</cite>.</p></li>
<li><p>Upgrade to Cray OS 3.0 (SLES-15 SP5).</p></li>
<li><p>Upgrade Slurm to version 24.05.</p></li>
<li><p>HPE/Cray Programming Environment (CPE) 24.03 AND 24.07 are now available via the <code class="docutils literal notranslate"><span class="pre">cpe/24.03</span></code> and <code class="docutils literal notranslate"><span class="pre">cpe/24.07</span></code> modulefiles.</p></li>
<li><p>ROCm 6.1.3 and 6.2.0 are now available via the <code class="docutils literal notranslate"><span class="pre">rocm</span></code> modulefiles.</p></li>
<li><p>CPE 23.12 and ROCm 5.7.1 remain as default.</p></li>
</ul>
<p>Please report any issues to <a class="reference external" href="mailto:help&#37;&#52;&#48;olcf&#46;ornl&#46;gov" target="_blank">help<span>&#64;</span>olcf<span>&#46;</span>ornl<span>&#46;</span>gov</a>.
The <a class="reference external" href="https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#known-issues" target="_blank">Frontier Known Issues</a> have been updated with the latest available information.</p>
</section>
<section id="id19">
<h3>2024-07-16<a class="headerlink" href="#id19" title="Link to this heading"></a></h3>
<p>On Tuesday, July 16, 2024, Frontiers system software will be upgraded. The following changes will take place:</p>
<ul class="simple">
<li><p>ROCm 5.7.1 and HPE/Cray PE 23.12 will become default.</p></li>
<li><p>The system will be upgraded to the AMD GPU 6.7.0 device driver (ROCm 6.1.0 release).</p></li>
</ul>
<p>Please note major changes in the AMD and GNU programming environments detailed in Known Issues <a class="reference external" href="https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#olcfdev-1799-new-rocm-module-layout-for-prgenv-amd" target="_blank">OLCFDEV-1799</a> and <a class="reference external" href="https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#olcfdev-1801-new-prgenv-gnu-module-implementation" target="_blank">OLCFDEV_1801</a>:</p>
<ul class="simple">
<li><p>If using <code class="docutils literal notranslate"><span class="pre">PrgEnv-gnu</span></code>, the <code class="docutils literal notranslate"><span class="pre">gcc</span></code> module has been renamed to <code class="docutils literal notranslate"><span class="pre">gcc-native</span></code> beginning in HPE/Cray PE 23.12. <code class="docutils literal notranslate"><span class="pre">gcc</span></code> modules still exist in older HPE/Cray PE versions.</p></li>
<li><p>If using <code class="docutils literal notranslate"><span class="pre">PrgEnv-amd</span></code>, you must load a <code class="docutils literal notranslate"><span class="pre">rocm</span></code> module in addition to the <code class="docutils literal notranslate"><span class="pre">amd</span></code> module. <code class="docutils literal notranslate"><span class="pre">amd</span></code> no longer provides the full ROCm toolkit, only the host compiler. The versions of these modules must match.</p></li>
<li><p>If using <code class="docutils literal notranslate"><span class="pre">amd-mixed</span></code>, please use a <code class="docutils literal notranslate"><span class="pre">rocm</span></code> module instead. <code class="docutils literal notranslate"><span class="pre">amd-mixed</span></code> no longer provides the full ROCm toolkit, only the host compiler.</p></li>
</ul>
<p>Users are encouraged to try the versions that will become default and report any issues to <a class="reference external" href="mailto:help&#37;&#52;&#48;olcf&#46;ornl&#46;gov" target="_blank">help<span>&#64;</span>olcf<span>&#46;</span>ornl<span>&#46;</span>gov</a>.</p>
</section>
<section id="id20">
<h3>2024-04-17<a class="headerlink" href="#id20" title="Link to this heading"></a></h3>
<p>On Wednesday, April 17, 2024, the <code class="docutils literal notranslate"><span class="pre">lfs-wrapper/0.0.1</span></code> modulefile became default. If you encounter any issues or have questions, please contact <a class="reference external" href="mailto:help&#37;&#52;&#48;olcf&#46;ornl&#46;gov" target="_blank">help<span>&#64;</span>olcf<span>&#46;</span>ornl<span>&#46;</span>gov</a>.</p>
</section>
<section id="id21">
<h3>2024-03-19<a class="headerlink" href="#id21" title="Link to this heading"></a></h3>
<p>On Tuesday, March 19, 2024, Frontiers system software was upgraded to Slingshot 2.1.1 and Slingshot Host Software 2.1.2. If you encounter any issues or have questions, please contact <a class="reference external" href="mailto:help&#37;&#52;&#48;olcf&#46;ornl&#46;gov" target="_blank">help<span>&#64;</span>olcf<span>&#46;</span>ornl<span>&#46;</span>gov</a>.</p>
</section>
<section id="id22">
<h3>2024-01-23<a class="headerlink" href="#id22" title="Link to this heading"></a></h3>
<p>On Tuesday, January 23, 2024, Frontiers system software was upgraded. The following changes took place:</p>
<ul class="simple">
<li><p>ROCm 6.0.0 is now available via the <code class="docutils literal notranslate"><span class="pre">rocm/6.0.0</span></code> modulefile.</p></li>
<li><p>HPE/Cray Programming Environment (PE) 23.12 is now available via the <code class="docutils literal notranslate"><span class="pre">cpe/23.12</span></code> modulefile.</p></li>
<li><p>ROCm 5.3.0 and HPE/Cray PE 22.12 remain as default.</p></li>
<li><p>The system was upgraded to AMD GPU 6.3.6 device driver (ROCm 6.0.0 release).</p></li>
</ul>
<p>Please note that target default versions will be updated to PE 23.12 and ROCm 5.7.1 in the near future. Users are encouraged to try both and report any issues to <a class="reference external" href="mailto:help&#37;&#52;&#48;olcf&#46;ornl&#46;gov" target="_blank">help<span>&#64;</span>olcf<span>&#46;</span>ornl<span>&#46;</span>gov</a>.</p>
</section>
<section id="id23">
<h3>2023-12-05<a class="headerlink" href="#id23" title="Link to this heading"></a></h3>
<p>On Tuesday, December 5, 2023, Frontiers system software was upgraded. The following changes took place:</p>
<ul class="simple">
<li><p>ROCm 5.7.1 is now available via the <code class="docutils literal notranslate"><span class="pre">rocm/5.7.1</span></code> modulefile.</p></li>
<li><p>Flux 0.56.0 is now available via the <code class="docutils literal notranslate"><span class="pre">flux/0.56.0</span></code> modulefile.</p></li>
</ul>
</section>
<section id="id24">
<h3>2023-10-03<a class="headerlink" href="#id24" title="Link to this heading"></a></h3>
<p>On Tuesday, October 3, 2023, Frontiers system software was upgraded. The following changes took place:</p>
<ul class="simple">
<li><p>The system was to the AMD GPU 6.1.5 device driver (ROCm 5.6.1 release).</p></li>
<li><p>Slurm was upgraded to version 23.02.5</p></li>
</ul>
</section>
<section id="id25">
<h3>2023-09-19<a class="headerlink" href="#id25" title="Link to this heading"></a></h3>
<p>On Tuesday, September 19, 2023, Frontiers system software was upgraded. The following changes took place:</p>
<ul class="simple">
<li><p>The system was upgraded to Slingshot Host Software 2.1.0.</p></li>
<li><p>ROCm 5.6.0 and 5.7.0 are now available via the <code class="docutils literal notranslate"><span class="pre">rocm/5.6.0</span></code> and <code class="docutils literal notranslate"><span class="pre">rocm/5.7.0</span></code> modulefiles, respectively.</p></li>
<li><p>HPE/Cray Programming Environment (PE) 23.09 is now available via the <code class="docutils literal notranslate"><span class="pre">cpe/23.09</span></code> modulefile.</p></li>
<li><p>ROCm 5.3.0 and HPE/Cray PE 22.12 remain as default.</p></li>
</ul>
</section>
<section id="id26">
<h3>2023-07-18<a class="headerlink" href="#id26" title="Link to this heading"></a></h3>
<p>On Tuesday, July 18, 2023, Frontier was upgraded to a new version of the system software stack. During the upgrade, the following changes took place:</p>
<ul class="simple">
<li><p>The system was upgraded to Cray OS 2.5, Slingshot Host Software 2.0.2-112, and the AMD GPU 6.0.5 device driver (ROCm 5.5.1 release).</p></li>
<li><p>ROCm 5.5.1 is now available via the <code class="docutils literal notranslate"><span class="pre">rocm/5.5.1</span></code> modulefile.</p></li>
<li><p>HPE/Cray Programming Environments (PE) 23.05 is now available via the <code class="docutils literal notranslate"><span class="pre">cpe/23.05</span></code> modulefile.</p></li>
<li><p>HPE/Cray PE 23.05 introduces support for ROCm 5.5.1. However, due to issues identified during testing, ROCm 5.3.0 and HPE/Cray PE 22.12 remain as default.</p></li>
</ul>
</section>
<section id="id27">
<h3>2023-05-09<a class="headerlink" href="#id27" title="Link to this heading"></a></h3>
<p>On Tuesday, May 9, 2023, the <cite>darshan-runtime</cite> modulefile was added to <cite>DefApps</cite> and is now loaded by default on Frontier. This module will allow users to profile the I/O of their applications with minimal impact. The logs are available to users on the Orion file system in <cite>/lustre/orion/darshan/&lt;system&gt;/&lt;yyyy&gt;/&lt;mm&gt;/&lt;dd&gt;</cite>. Unloading <cite>darshan-runtime</cite> is recommended for users profiling their applications with other profilers to prevent conflicts.</p>
</section>
</section>
<hr class="docutils" />
<section id="known-issues">
<h2>Known Issues<a class="headerlink" href="#known-issues" title="Link to this heading"></a></h2>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="2024_olcf_system_changes.html" class="btn btn-neutral float-left" title="2024 Notable System Changes" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="summit_user_guide.html" class="btn btn-neutral float-right" title="Summit User Guide" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, OLCF.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>  



</body>
</html>